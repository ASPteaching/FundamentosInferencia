<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Capítulo 14 Métodos de computación intensiva: El Bootstrap | Fundamentos de Inferencia Estadistica</title>
  <meta name="description" content="Capítulo 14 Métodos de computación intensiva: El Bootstrap | Fundamentos de Inferencia Estadistica" />
  <meta name="generator" content="bookdown 0.45 and GitBook 2.6.7" />

  <meta property="og:title" content="Capítulo 14 Métodos de computación intensiva: El Bootstrap | Fundamentos de Inferencia Estadistica" />
  <meta property="og:type" content="book" />
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Capítulo 14 Métodos de computación intensiva: El Bootstrap | Fundamentos de Inferencia Estadistica" />
  
  
  

<meta name="author" content="Alex Sanchez Pla y Santiago Pérez Hoyos" />


<meta name="date" content="2026-01-22" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="estadística-no-paramétrica.html"/>
<link rel="next" href="bibliografia.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="blocks.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<div style="margin:0 0 10px 15px;">
  <a href="FundamentosInferenciaEstadistica.pdf" target="_blank"
     title="Descargar PDF"
     style="display:flex;align-items:center;gap:6px;text-decoration:none;">
    <img src="images/aPDF.png" alt="PDF" width="16" height="20">
    <span style="font-weight:bold;">Descargar versión PDF</span>
  </a>
</div>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Presentación</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#objetivo"><i class="fa fa-check"></i>Objetivo</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#prerequisitos-y-organizaci%C3%B3n-del-material"><i class="fa fa-check"></i>Prerequisitos y organización del material</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="agradecimiento-y-fuentes-utilizadas.html"><a href="agradecimiento-y-fuentes-utilizadas.html"><i class="fa fa-check"></i>Agradecimiento y fuentes utilizadas</a>
<ul>
<li class="chapter" data-level="" data-path="agradecimiento-y-fuentes-utilizadas.html"><a href="agradecimiento-y-fuentes-utilizadas.html#el-proyecto-statmedia"><i class="fa fa-check"></i>El proyecto Statmedia</a></li>
<li class="chapter" data-level="" data-path="agradecimiento-y-fuentes-utilizadas.html"><a href="agradecimiento-y-fuentes-utilizadas.html#otros-materiales-utilizados"><i class="fa fa-check"></i>Otros materiales utilizados</a></li>
<li class="chapter" data-level="" data-path="agradecimiento-y-fuentes-utilizadas.html"><a href="agradecimiento-y-fuentes-utilizadas.html#materiales-complementarios"><i class="fa fa-check"></i>Materiales complementarios</a>
<ul>
<li class="chapter" data-level="" data-path="agradecimiento-y-fuentes-utilizadas.html"><a href="agradecimiento-y-fuentes-utilizadas.html#complementos-matem%C3%A1ticos"><i class="fa fa-check"></i>Complementos matemáticos</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="1" data-path="probabilidad-y-experimentos-aleatorios.html"><a href="probabilidad-y-experimentos-aleatorios.html"><i class="fa fa-check"></i><b>1</b> Probabilidad y Experimentos aleatorios</a>
<ul>
<li class="chapter" data-level="1.1" data-path="probabilidad-y-experimentos-aleatorios.html"><a href="probabilidad-y-experimentos-aleatorios.html#introducci%C3%B3n"><i class="fa fa-check"></i><b>1.1</b> Introducción</a>
<ul>
<li class="chapter" data-level="1.1.1" data-path="probabilidad-y-experimentos-aleatorios.html"><a href="probabilidad-y-experimentos-aleatorios.html#fen%C3%B3menos-deterministas-y-fen%C3%B3menos-aleatorios"><i class="fa fa-check"></i><b>1.1.1</b> Fenómenos deterministas y fenómenos aleatorios</a></li>
<li class="chapter" data-level="1.1.2" data-path="probabilidad-y-experimentos-aleatorios.html"><a href="probabilidad-y-experimentos-aleatorios.html#sucesos"><i class="fa fa-check"></i><b>1.1.2</b> Sucesos</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="probabilidad-y-experimentos-aleatorios.html"><a href="probabilidad-y-experimentos-aleatorios.html#funci%C3%B3n-de-probabilidad"><i class="fa fa-check"></i><b>1.2</b> Función de probabilidad</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="probabilidad-y-experimentos-aleatorios.html"><a href="probabilidad-y-experimentos-aleatorios.html#diferentes-funciones-de-probabilidad-para-una-misma-experiencia-aleatoria"><i class="fa fa-check"></i><b>1.2.1</b> ¿Diferentes funciones de probabilidad para una misma experiencia aleatoria?</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="probabilidad-y-experimentos-aleatorios.html"><a href="probabilidad-y-experimentos-aleatorios.html#c%C3%B3mo-se-calculan-las-probabilidades"><i class="fa fa-check"></i><b>1.3</b> ¿Cómo se calculan las probabilidades?</a></li>
<li class="chapter" data-level="1.4" data-path="probabilidad-y-experimentos-aleatorios.html"><a href="probabilidad-y-experimentos-aleatorios.html#sucesos-elementales-y-sucesos-observables"><i class="fa fa-check"></i><b>1.4</b> Sucesos elementales y sucesos observables</a></li>
<li class="chapter" data-level="1.5" data-path="probabilidad-y-experimentos-aleatorios.html"><a href="probabilidad-y-experimentos-aleatorios.html#propiedades-inmediatas-de-la-probabilidad"><i class="fa fa-check"></i><b>1.5</b> Propiedades inmediatas de la probabilidad</a>
<ul>
<li class="chapter" data-level="1.5.1" data-path="probabilidad-y-experimentos-aleatorios.html"><a href="probabilidad-y-experimentos-aleatorios.html#succeso-imposible"><i class="fa fa-check"></i><b>1.5.1</b> Succeso imposible</a></li>
<li class="chapter" data-level="1.5.2" data-path="probabilidad-y-experimentos-aleatorios.html"><a href="probabilidad-y-experimentos-aleatorios.html#suceso-implicado"><i class="fa fa-check"></i><b>1.5.2</b> Suceso implicado</a></li>
<li class="chapter" data-level="1.5.3" data-path="probabilidad-y-experimentos-aleatorios.html"><a href="probabilidad-y-experimentos-aleatorios.html#complementario-de-un-suceso"><i class="fa fa-check"></i><b>1.5.3</b> Complementario de un suceso</a></li>
<li class="chapter" data-level="1.5.4" data-path="probabilidad-y-experimentos-aleatorios.html"><a href="probabilidad-y-experimentos-aleatorios.html#ocurrencia-de-algun-suceso"><i class="fa fa-check"></i><b>1.5.4</b> Ocurrencia de algun suceso</a></li>
<li class="chapter" data-level="1.5.5" data-path="probabilidad-y-experimentos-aleatorios.html"><a href="probabilidad-y-experimentos-aleatorios.html#probabilidad-de-que-ocurra-algun-suceso"><i class="fa fa-check"></i><b>1.5.5</b> Probabilidad de que ocurra algun suceso</a></li>
<li class="chapter" data-level="1.5.6" data-path="probabilidad-y-experimentos-aleatorios.html"><a href="probabilidad-y-experimentos-aleatorios.html#probabilidad-de-que-ocurran-dos-o-m%C3%A1s-sucesos-a-la-vez"><i class="fa fa-check"></i><b>1.5.6</b> Probabilidad de que ocurran dos (o más) sucesos a la vez</a></li>
</ul></li>
<li class="chapter" data-level="1.6" data-path="probabilidad-y-experimentos-aleatorios.html"><a href="probabilidad-y-experimentos-aleatorios.html#espacios-de-probabilidad"><i class="fa fa-check"></i><b>1.6</b> Espacios de probabilidad</a></li>
<li class="chapter" data-level="1.7" data-path="probabilidad-y-experimentos-aleatorios.html"><a href="probabilidad-y-experimentos-aleatorios.html#probabilidad-condicionada"><i class="fa fa-check"></i><b>1.7</b> Probabilidad condicionada</a>
<ul>
<li class="chapter" data-level="1.7.1" data-path="probabilidad-y-experimentos-aleatorios.html"><a href="probabilidad-y-experimentos-aleatorios.html#la-probabilidad-condicionada-es-una-medida-de-probabilidad"><i class="fa fa-check"></i><b>1.7.1</b> La probabilidad condicionada es una medida de probabilidad</a></li>
<li class="chapter" data-level="1.7.2" data-path="probabilidad-y-experimentos-aleatorios.html"><a href="probabilidad-y-experimentos-aleatorios.html#sucesos-dependientes-y-sucesos-independientes"><i class="fa fa-check"></i><b>1.7.2</b> Sucesos dependientes y sucesos independientes</a></li>
<li class="chapter" data-level="1.7.3" data-path="probabilidad-y-experimentos-aleatorios.html"><a href="probabilidad-y-experimentos-aleatorios.html#incompatibilidad-e-independencia"><i class="fa fa-check"></i><b>1.7.3</b> Incompatibilidad e independencia</a></li>
</ul></li>
<li class="chapter" data-level="1.8" data-path="probabilidad-y-experimentos-aleatorios.html"><a href="probabilidad-y-experimentos-aleatorios.html#dos-teoremas-importantes"><i class="fa fa-check"></i><b>1.8</b> Dos Teoremas importantes</a>
<ul>
<li class="chapter" data-level="1.8.1" data-path="probabilidad-y-experimentos-aleatorios.html"><a href="probabilidad-y-experimentos-aleatorios.html#teorema-de-las-probabilidades-totales"><i class="fa fa-check"></i><b>1.8.1</b> Teorema de las probabilidades totales</a></li>
<li class="chapter" data-level="1.8.2" data-path="probabilidad-y-experimentos-aleatorios.html"><a href="probabilidad-y-experimentos-aleatorios.html#teorema-de-bayes"><i class="fa fa-check"></i><b>1.8.2</b> Teorema de Bayes</a></li>
</ul></li>
<li class="chapter" data-level="1.9" data-path="probabilidad-y-experimentos-aleatorios.html"><a href="probabilidad-y-experimentos-aleatorios.html#introducci%C3%B3n-a-los-experimentos-m%C3%BAltiples"><i class="fa fa-check"></i><b>1.9</b> Introducción a los experimentos múltiples</a></li>
<li class="chapter" data-level="1.10" data-path="probabilidad-y-experimentos-aleatorios.html"><a href="probabilidad-y-experimentos-aleatorios.html#combinatoria"><i class="fa fa-check"></i><b>1.10</b> Combinatoria</a>
<ul>
<li class="chapter" data-level="1.10.1" data-path="probabilidad-y-experimentos-aleatorios.html"><a href="probabilidad-y-experimentos-aleatorios.html#permutaciones"><i class="fa fa-check"></i><b>1.10.1</b> Permutaciones</a></li>
<li class="chapter" data-level="1.10.2" data-path="probabilidad-y-experimentos-aleatorios.html"><a href="probabilidad-y-experimentos-aleatorios.html#variaciones"><i class="fa fa-check"></i><b>1.10.2</b> Variaciones</a></li>
<li class="chapter" data-level="1.10.3" data-path="probabilidad-y-experimentos-aleatorios.html"><a href="probabilidad-y-experimentos-aleatorios.html#variaciones-con-repetici%C3%B3n"><i class="fa fa-check"></i><b>1.10.3</b> Variaciones con repetición</a></li>
<li class="chapter" data-level="1.10.4" data-path="probabilidad-y-experimentos-aleatorios.html"><a href="probabilidad-y-experimentos-aleatorios.html#combinaciones"><i class="fa fa-check"></i><b>1.10.4</b> Combinaciones</a></li>
<li class="chapter" data-level="1.10.5" data-path="probabilidad-y-experimentos-aleatorios.html"><a href="probabilidad-y-experimentos-aleatorios.html#permutaciones-con-repetici%C3%B3n"><i class="fa fa-check"></i><b>1.10.5</b> Permutaciones con repetición</a></li>
</ul></li>
<li class="chapter" data-level="1.11" data-path="probabilidad-y-experimentos-aleatorios.html"><a href="probabilidad-y-experimentos-aleatorios.html#frecuencia-relativa-y-probabilidad"><i class="fa fa-check"></i><b>1.11</b> Frecuencia relativa y probabilidad</a>
<ul>
<li class="chapter" data-level="1.11.1" data-path="probabilidad-y-experimentos-aleatorios.html"><a href="probabilidad-y-experimentos-aleatorios.html#ilustraci%C3%B3n-por-simulaci%C3%B3n"><i class="fa fa-check"></i><b>1.11.1</b> Ilustración por simulación</a></li>
</ul></li>
<li class="chapter" data-level="1.12" data-path="probabilidad-y-experimentos-aleatorios.html"><a href="probabilidad-y-experimentos-aleatorios.html#caso-de-estudio-eficacia-de-una-prueba-diagn%C3%B3stica"><i class="fa fa-check"></i><b>1.12</b> Caso de Estudio: Eficacia de una prueba diagnóstica</a>
<ul>
<li class="chapter" data-level="1.12.1" data-path="probabilidad-y-experimentos-aleatorios.html"><a href="probabilidad-y-experimentos-aleatorios.html#aplicaci%C3%B3n-del-teorema-de-bayes"><i class="fa fa-check"></i><b>1.12.1</b> Aplicación del Teorema de Bayes</a></li>
<li class="chapter" data-level="1.12.2" data-path="probabilidad-y-experimentos-aleatorios.html"><a href="probabilidad-y-experimentos-aleatorios.html#ejemplo-num%C3%A9rico"><i class="fa fa-check"></i><b>1.12.2</b> Ejemplo numérico</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="variables-aleatorias-y-distribuciones-de-probabilidad.html"><a href="variables-aleatorias-y-distribuciones-de-probabilidad.html"><i class="fa fa-check"></i><b>2</b> Variables aleatorias y Distribuciones de probabilidad</a>
<ul>
<li class="chapter" data-level="2.1" data-path="variables-aleatorias-y-distribuciones-de-probabilidad.html"><a href="variables-aleatorias-y-distribuciones-de-probabilidad.html#el-espacio-muestral-y-sus-elementos"><i class="fa fa-check"></i><b>2.1</b> El espacio muestral y sus elementos</a></li>
<li class="chapter" data-level="2.2" data-path="variables-aleatorias-y-distribuciones-de-probabilidad.html"><a href="variables-aleatorias-y-distribuciones-de-probabilidad.html#representaci%C3%B3n-num%C3%A9rica-de-los-sucesos-elementales.-variables-aleatorias"><i class="fa fa-check"></i><b>2.2</b> Representación numérica de los sucesos elementales. Variables aleatorias</a></li>
<li class="chapter" data-level="2.3" data-path="variables-aleatorias-y-distribuciones-de-probabilidad.html"><a href="variables-aleatorias-y-distribuciones-de-probabilidad.html#caracterizaci%C3%B3n-de-una-variable-aleatoria-a-trav%C3%A9s-de-la-probabilidad.-funci%C3%B3n-de-distribuci%C3%B3n"><i class="fa fa-check"></i><b>2.3</b> Caracterización de una variable aleatoria a través de la probabilidad. Función de distribución</a></li>
<li class="chapter" data-level="2.4" data-path="variables-aleatorias-y-distribuciones-de-probabilidad.html"><a href="variables-aleatorias-y-distribuciones-de-probabilidad.html#propiedades-de-la-funci%C3%B3n-de-distribuci%C3%B3n"><i class="fa fa-check"></i><b>2.4</b> Propiedades de la función de distribución</a></li>
<li class="chapter" data-level="2.5" data-path="variables-aleatorias-y-distribuciones-de-probabilidad.html"><a href="variables-aleatorias-y-distribuciones-de-probabilidad.html#clasificaci%C3%B3n-de-las-variables-aleatorias"><i class="fa fa-check"></i><b>2.5</b> Clasificación de las variables aleatorias</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="variables-aleatorias-y-distribuciones-de-probabilidad.html"><a href="variables-aleatorias-y-distribuciones-de-probabilidad.html#variables-aleatorias-discretas"><i class="fa fa-check"></i><b>2.5.1</b> Variables aleatorias discretas</a></li>
<li class="chapter" data-level="2.5.2" data-path="variables-aleatorias-y-distribuciones-de-probabilidad.html"><a href="variables-aleatorias-y-distribuciones-de-probabilidad.html#variables-aleatorias-continuas"><i class="fa fa-check"></i><b>2.5.2</b> Variables aleatorias continuas</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="variables-aleatorias-y-distribuciones-de-probabilidad.html"><a href="variables-aleatorias-y-distribuciones-de-probabilidad.html#variable-aleatoria-discretas"><i class="fa fa-check"></i><b>2.6</b> Variable aleatoria discretas</a>
<ul>
<li class="chapter" data-level="2.6.1" data-path="variables-aleatorias-y-distribuciones-de-probabilidad.html"><a href="variables-aleatorias-y-distribuciones-de-probabilidad.html#caracterizaci%C3%B3n-de-las-v.a.-discretas"><i class="fa fa-check"></i><b>2.6.1</b> Caracterización de las v.a. discretas</a></li>
<li class="chapter" data-level="2.6.2" data-path="variables-aleatorias-y-distribuciones-de-probabilidad.html"><a href="variables-aleatorias-y-distribuciones-de-probabilidad.html#propiedades-de-la-funci%C3%B3n-de-densidad-discreta"><i class="fa fa-check"></i><b>2.6.2</b> Propiedades de la función de densidad discreta</a></li>
<li class="chapter" data-level="2.6.3" data-path="variables-aleatorias-y-distribuciones-de-probabilidad.html"><a href="variables-aleatorias-y-distribuciones-de-probabilidad.html#relaciones-entre-la-funci%C3%B3n-de-distribuci%C3%B3n-y-la-funci%C3%B3n-de-densidad-discreta.-probabilidad-de-intervalos."><i class="fa fa-check"></i><b>2.6.3</b> Relaciones entre la función de distribución y la función de densidad discreta. <br> Probabilidad de intervalos.</a></li>
</ul></li>
<li class="chapter" data-level="2.7" data-path="variables-aleatorias-y-distribuciones-de-probabilidad.html"><a href="variables-aleatorias-y-distribuciones-de-probabilidad.html#variables-aleatorias-continuas-1"><i class="fa fa-check"></i><b>2.7</b> Variables aleatorias continuas</a>
<ul>
<li class="chapter" data-level="2.7.1" data-path="variables-aleatorias-y-distribuciones-de-probabilidad.html"><a href="variables-aleatorias-y-distribuciones-de-probabilidad.html#funci%C3%B3n-de-densidad-continua"><i class="fa fa-check"></i><b>2.7.1</b> Función de densidad continua</a></li>
<li class="chapter" data-level="2.7.2" data-path="variables-aleatorias-y-distribuciones-de-probabilidad.html"><a href="variables-aleatorias-y-distribuciones-de-probabilidad.html#relaciones-entre-la-funci%C3%B3n-de-distribuci%C3%B3n-y-la-funci%C3%B3n-de-densidad."><i class="fa fa-check"></i><b>2.7.2</b> Relaciones entre la función de distribución y la función de densidad.</a></li>
</ul></li>
<li class="chapter" data-level="2.8" data-path="variables-aleatorias-y-distribuciones-de-probabilidad.html"><a href="variables-aleatorias-y-distribuciones-de-probabilidad.html#caracterizaci%C3%B3n-de-una-variable-aleatoria-a-trav%C3%A9s-de-par%C3%A1metros"><i class="fa fa-check"></i><b>2.8</b> Caracterización de una variable aleatoria a través de parámetros</a></li>
<li class="chapter" data-level="2.9" data-path="variables-aleatorias-y-distribuciones-de-probabilidad.html"><a href="variables-aleatorias-y-distribuciones-de-probabilidad.html#esperanza-de-una-variable-aleatoria-discreta"><i class="fa fa-check"></i><b>2.9</b> Esperanza de una variable aleatoria discreta</a></li>
<li class="chapter" data-level="2.10" data-path="variables-aleatorias-y-distribuciones-de-probabilidad.html"><a href="variables-aleatorias-y-distribuciones-de-probabilidad.html#esperanza-de-una-variable-aleatoria-continua"><i class="fa fa-check"></i><b>2.10</b> Esperanza de una variable aleatoria continua</a></li>
<li class="chapter" data-level="2.11" data-path="variables-aleatorias-y-distribuciones-de-probabilidad.html"><a href="variables-aleatorias-y-distribuciones-de-probabilidad.html#propiedades-de-la-esperanza-matem%C3%A1tica"><i class="fa fa-check"></i><b>2.11</b> Propiedades de la esperanza matemática</a>
<ul>
<li class="chapter" data-level="2.11.1" data-path="variables-aleatorias-y-distribuciones-de-probabilidad.html"><a href="variables-aleatorias-y-distribuciones-de-probabilidad.html#linealidad-de-la-esperanza-matem%C3%A1tica"><i class="fa fa-check"></i><b>2.11.1</b> Linealidad de la esperanza matemática</a></li>
<li class="chapter" data-level="2.11.2" data-path="variables-aleatorias-y-distribuciones-de-probabilidad.html"><a href="variables-aleatorias-y-distribuciones-de-probabilidad.html#esperanza-del-producto"><i class="fa fa-check"></i><b>2.11.2</b> Esperanza del producto</a></li>
</ul></li>
<li class="chapter" data-level="2.12" data-path="variables-aleatorias-y-distribuciones-de-probabilidad.html"><a href="variables-aleatorias-y-distribuciones-de-probabilidad.html#varianza-de-una-variable-aleatoria"><i class="fa fa-check"></i><b>2.12</b> Varianza de una variable aleatoria</a>
<ul>
<li class="chapter" data-level="2.12.1" data-path="variables-aleatorias-y-distribuciones-de-probabilidad.html"><a href="variables-aleatorias-y-distribuciones-de-probabilidad.html#propiedades-de-la-varianza"><i class="fa fa-check"></i><b>2.12.1</b> Propiedades de la varianza</a></li>
</ul></li>
<li class="chapter" data-level="2.13" data-path="variables-aleatorias-y-distribuciones-de-probabilidad.html"><a href="variables-aleatorias-y-distribuciones-de-probabilidad.html#momentos-de-orden-k-de-una-variable-aleatoria"><i class="fa fa-check"></i><b>2.13</b> Momentos (de orden <span class="math inline">\(k\)</span>) de una variable aleatoria</a></li>
<li class="chapter" data-level="2.14" data-path="variables-aleatorias-y-distribuciones-de-probabilidad.html"><a href="variables-aleatorias-y-distribuciones-de-probabilidad.html#definici%C3%B3n-formal-de-variable-aleatoria"><i class="fa fa-check"></i><b>2.14</b> Definición formal de variable aleatoria</a></li>
<li class="chapter" data-level="2.15" data-path="variables-aleatorias-y-distribuciones-de-probabilidad.html"><a href="variables-aleatorias-y-distribuciones-de-probabilidad.html#caso-pr%C3%A1ctico-lanzamiento-de-dos-dados"><i class="fa fa-check"></i><b>2.15</b> Caso práctico: Lanzamiento de dos dados</a>
<ul>
<li class="chapter" data-level="2.15.1" data-path="variables-aleatorias-y-distribuciones-de-probabilidad.html"><a href="variables-aleatorias-y-distribuciones-de-probabilidad.html#espacio-muestral"><i class="fa fa-check"></i><b>2.15.1</b> Espacio muestral</a></li>
<li class="chapter" data-level="2.15.2" data-path="variables-aleatorias-y-distribuciones-de-probabilidad.html"><a href="variables-aleatorias-y-distribuciones-de-probabilidad.html#representaci%C3%B3n-num%C3%A9rica"><i class="fa fa-check"></i><b>2.15.2</b> Representación numérica</a></li>
<li class="chapter" data-level="2.15.3" data-path="variables-aleatorias-y-distribuciones-de-probabilidad.html"><a href="variables-aleatorias-y-distribuciones-de-probabilidad.html#algunas-probabilidades"><i class="fa fa-check"></i><b>2.15.3</b> Algunas probabilidades</a></li>
<li class="chapter" data-level="2.15.4" data-path="variables-aleatorias-y-distribuciones-de-probabilidad.html"><a href="variables-aleatorias-y-distribuciones-de-probabilidad.html#funci%C3%B3n-de-distribuci%C3%B3n"><i class="fa fa-check"></i><b>2.15.4</b> Función de distribución</a></li>
<li class="chapter" data-level="2.15.5" data-path="variables-aleatorias-y-distribuciones-de-probabilidad.html"><a href="variables-aleatorias-y-distribuciones-de-probabilidad.html#clasificaci%C3%B3n-de-las-variables"><i class="fa fa-check"></i><b>2.15.5</b> Clasificación de las variables</a></li>
<li class="chapter" data-level="2.15.6" data-path="variables-aleatorias-y-distribuciones-de-probabilidad.html"><a href="variables-aleatorias-y-distribuciones-de-probabilidad.html#funci%C3%B3n-de-densidad-discreta"><i class="fa fa-check"></i><b>2.15.6</b> Función de densidad discreta</a></li>
<li class="chapter" data-level="2.15.7" data-path="variables-aleatorias-y-distribuciones-de-probabilidad.html"><a href="variables-aleatorias-y-distribuciones-de-probabilidad.html#probabilidad-de-intervalos-1"><i class="fa fa-check"></i><b>2.15.7</b> Probabilidad de intervalos</a></li>
<li class="chapter" data-level="2.15.8" data-path="variables-aleatorias-y-distribuciones-de-probabilidad.html"><a href="variables-aleatorias-y-distribuciones-de-probabilidad.html#esperanza"><i class="fa fa-check"></i><b>2.15.8</b> Esperanza</a></li>
<li class="chapter" data-level="2.15.9" data-path="variables-aleatorias-y-distribuciones-de-probabilidad.html"><a href="variables-aleatorias-y-distribuciones-de-probabilidad.html#esperanza-de-un-juego"><i class="fa fa-check"></i><b>2.15.9</b> Esperanza de un juego</a></li>
<li class="chapter" data-level="2.15.10" data-path="variables-aleatorias-y-distribuciones-de-probabilidad.html"><a href="variables-aleatorias-y-distribuciones-de-probabilidad.html#esperanza-con-recorrido-infinito"><i class="fa fa-check"></i><b>2.15.10</b> Esperanza con recorrido infinito</a></li>
<li class="chapter" data-level="2.15.11" data-path="variables-aleatorias-y-distribuciones-de-probabilidad.html"><a href="variables-aleatorias-y-distribuciones-de-probabilidad.html#esperanza-infinita"><i class="fa fa-check"></i><b>2.15.11</b> Esperanza infinita</a></li>
<li class="chapter" data-level="2.15.12" data-path="variables-aleatorias-y-distribuciones-de-probabilidad.html"><a href="variables-aleatorias-y-distribuciones-de-probabilidad.html#varianza"><i class="fa fa-check"></i><b>2.15.12</b> Varianza</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="distribuciones-notables.html"><a href="distribuciones-notables.html"><i class="fa fa-check"></i><b>3</b> Distribuciones Notables</a>
<ul>
<li class="chapter" data-level="3.1" data-path="distribuciones-notables.html"><a href="distribuciones-notables.html#distribuciones-discretas"><i class="fa fa-check"></i><b>3.1</b> Distribuciones discretas</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="distribuciones-notables.html"><a href="distribuciones-notables.html#la-distribuci%C3%B3n-de-bernouilli"><i class="fa fa-check"></i><b>3.1.1</b> La distribución de Bernouilli</a></li>
<li class="chapter" data-level="3.1.2" data-path="distribuciones-notables.html"><a href="distribuciones-notables.html#la-distribuci%C3%B3n-binomial"><i class="fa fa-check"></i><b>3.1.2</b> La distribución Binomial</a></li>
<li class="chapter" data-level="3.1.3" data-path="distribuciones-notables.html"><a href="distribuciones-notables.html#la-distribuci%C3%B3n-de-poisson"><i class="fa fa-check"></i><b>3.1.3</b> La distribución de Poisson</a></li>
<li class="chapter" data-level="3.1.4" data-path="distribuciones-notables.html"><a href="distribuciones-notables.html#la-distribuci%C3%B3n-uniforme-discreta"><i class="fa fa-check"></i><b>3.1.4</b> La distribución Uniforme discreta</a></li>
<li class="chapter" data-level="3.1.5" data-path="distribuciones-notables.html"><a href="distribuciones-notables.html#la-distribuci%C3%B3n-hipergeom%C3%A9trica"><i class="fa fa-check"></i><b>3.1.5</b> La distribución Hipergeométrica</a></li>
<li class="chapter" data-level="3.1.6" data-path="distribuciones-notables.html"><a href="distribuciones-notables.html#la-distribuci%C3%B3n-geom%C3%A9trica-o-de-pascal"><i class="fa fa-check"></i><b>3.1.6</b> La distribución Geométrica o de Pascal</a></li>
<li class="chapter" data-level="3.1.7" data-path="distribuciones-notables.html"><a href="distribuciones-notables.html#la-distribuci%C3%B3n-binomial-negativa"><i class="fa fa-check"></i><b>3.1.7</b> La distribución Binomial negativa</a></li>
<li class="chapter" data-level="3.1.8" data-path="distribuciones-notables.html"><a href="distribuciones-notables.html#tabla-resumen-de-las-distribuciones-discretas-principales"><i class="fa fa-check"></i><b>3.1.8</b> Tabla resumen de las distribuciones discretas principales</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="distribuciones-notables.html"><a href="distribuciones-notables.html#distribuciones-continuas"><i class="fa fa-check"></i><b>3.2</b> Distribuciones Continuas</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="distribuciones-notables.html"><a href="distribuciones-notables.html#la-distribuci%C3%B3n-uniforme"><i class="fa fa-check"></i><b>3.2.1</b> La distribución Uniforme</a></li>
<li class="chapter" data-level="3.2.2" data-path="distribuciones-notables.html"><a href="distribuciones-notables.html#la-distribuci%C3%B3n-exponencial"><i class="fa fa-check"></i><b>3.2.2</b> La distribución Exponencial</a></li>
<li class="chapter" data-level="3.2.3" data-path="distribuciones-notables.html"><a href="distribuciones-notables.html#la-distribuci%C3%B3n-normal"><i class="fa fa-check"></i><b>3.2.3</b> La distribución Normal</a></li>
<li class="chapter" data-level="3.2.4" data-path="distribuciones-notables.html"><a href="distribuciones-notables.html#la-distribuci%C3%B3n-gamma"><i class="fa fa-check"></i><b>3.2.4</b> La distribución Gamma</a></li>
<li class="chapter" data-level="3.2.5" data-path="distribuciones-notables.html"><a href="distribuciones-notables.html#la-distribuci%C3%B3n-de-cauchy"><i class="fa fa-check"></i><b>3.2.5</b> La distribución de Cauchy</a></li>
<li class="chapter" data-level="3.2.6" data-path="distribuciones-notables.html"><a href="distribuciones-notables.html#la-distribuci%C3%B3n-de-weibull"><i class="fa fa-check"></i><b>3.2.6</b> La distribución de Weibull</a></li>
<li class="chapter" data-level="3.2.7" data-path="distribuciones-notables.html"><a href="distribuciones-notables.html#tabla-resumen-de-las-principales-distribuciones-continuas"><i class="fa fa-check"></i><b>3.2.7</b> Tabla resumen de las principales distribuciones continuas</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="distribuciones-notables.html"><a href="distribuciones-notables.html#distribuciones-con-r-y-python"><i class="fa fa-check"></i><b>3.3</b> Distribuciones con R (y Python)</a></li>
<li class="chapter" data-level="3.4" data-path="distribuciones-notables.html"><a href="distribuciones-notables.html#la-familia-exponencial-de-distribuciones"><i class="fa fa-check"></i><b>3.4</b> La familia exponencial de distribuciones</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="distribuciones-notables.html"><a href="distribuciones-notables.html#ejemplos-de-distribuciones-de-esta-familia"><i class="fa fa-check"></i><b>3.4.1</b> Ejemplos de distribuciones de esta familia</a></li>
<li class="chapter" data-level="3.4.2" data-path="distribuciones-notables.html"><a href="distribuciones-notables.html#distribuci%C3%B3n-binomial"><i class="fa fa-check"></i><b>3.4.2</b> Distribución Binomial</a></li>
<li class="chapter" data-level="3.4.3" data-path="distribuciones-notables.html"><a href="distribuciones-notables.html#importancia-y-utilidad-de-la-familia-exponencial"><i class="fa fa-check"></i><b>3.4.3</b> Importancia y utilidad de la familia exponencial</a></li>
<li class="chapter" data-level="3.4.4" data-path="distribuciones-notables.html"><a href="distribuciones-notables.html#los-modelos-lineales-generalizados-glms"><i class="fa fa-check"></i><b>3.4.4</b> Los modelos lineales generalizados (GLMs)</a></li>
<li class="chapter" data-level="3.4.5" data-path="distribuciones-notables.html"><a href="distribuciones-notables.html#estimaci%C3%B3n-en-la-familia-exponencial"><i class="fa fa-check"></i><b>3.4.5</b> Estimación en la familia exponencial</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="distribuciones-de-probabilidad-multidimensionales.html"><a href="distribuciones-de-probabilidad-multidimensionales.html"><i class="fa fa-check"></i><b>4</b> Distribuciones de probabilidad multidimensionales</a>
<ul>
<li class="chapter" data-level="4.1" data-path="distribuciones-de-probabilidad-multidimensionales.html"><a href="distribuciones-de-probabilidad-multidimensionales.html#distribuciones-conjuntas-de-probabilidades"><i class="fa fa-check"></i><b>4.1</b> Distribuciones conjuntas de probabilidades</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="distribuciones-de-probabilidad-multidimensionales.html"><a href="distribuciones-de-probabilidad-multidimensionales.html#variable-aleatoria-bivariante"><i class="fa fa-check"></i><b>4.1.1</b> Variable aleatoria bivariante</a></li>
<li class="chapter" data-level="4.1.2" data-path="distribuciones-de-probabilidad-multidimensionales.html"><a href="distribuciones-de-probabilidad-multidimensionales.html#funci%C3%B3n-de-distribuci%C3%B3n-bivariante"><i class="fa fa-check"></i><b>4.1.2</b> Función de distribución bivariante</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="distribuciones-de-probabilidad-multidimensionales.html"><a href="distribuciones-de-probabilidad-multidimensionales.html#variable-aleatorias-bivariantes-discretas"><i class="fa fa-check"></i><b>4.2</b> Variable aleatorias bivariantes discretas</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="distribuciones-de-probabilidad-multidimensionales.html"><a href="distribuciones-de-probabilidad-multidimensionales.html#funci%C3%B3n-de-masa-de-probabilidad-discreta-fmp"><i class="fa fa-check"></i><b>4.2.1</b> Función de masa de probabilidad discreta (fmp)</a></li>
<li class="chapter" data-level="4.2.2" data-path="distribuciones-de-probabilidad-multidimensionales.html"><a href="distribuciones-de-probabilidad-multidimensionales.html#propiedades-de-la-fmp-bivariante"><i class="fa fa-check"></i><b>4.2.2</b> Propiedades de la fmp bivariante</a></li>
<li class="chapter" data-level="4.2.3" data-path="distribuciones-de-probabilidad-multidimensionales.html"><a href="distribuciones-de-probabilidad-multidimensionales.html#ejemplo-de-distribuci%C3%B3n-bivariante-discreta"><i class="fa fa-check"></i><b>4.2.3</b> Ejemplo de distribución bivariante discreta</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="distribuciones-de-probabilidad-multidimensionales.html"><a href="distribuciones-de-probabilidad-multidimensionales.html#la-distribuci%C3%B3n-multinomial"><i class="fa fa-check"></i><b>4.3</b> La distribución multinomial</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="distribuciones-de-probabilidad-multidimensionales.html"><a href="distribuciones-de-probabilidad-multidimensionales.html#generaci%C3%B3n-de-las-observaciones"><i class="fa fa-check"></i><b>4.3.1</b> Generación de las observaciones</a></li>
<li class="chapter" data-level="4.3.2" data-path="distribuciones-de-probabilidad-multidimensionales.html"><a href="distribuciones-de-probabilidad-multidimensionales.html#funcion-de-masa-de-probabilidad-de-la-distribuci%C3%B3n-multinomial"><i class="fa fa-check"></i><b>4.3.2</b> Funcion de masa de probabilidad de la distribución multinomial</a></li>
<li class="chapter" data-level="4.3.3" data-path="distribuciones-de-probabilidad-multidimensionales.html"><a href="distribuciones-de-probabilidad-multidimensionales.html#relaci%C3%B3n-con-la-distribuci%C3%B3n-binomial"><i class="fa fa-check"></i><b>4.3.3</b> Relación con la distribución binomial</a></li>
<li class="chapter" data-level="4.3.4" data-path="distribuciones-de-probabilidad-multidimensionales.html"><a href="distribuciones-de-probabilidad-multidimensionales.html#un-caso-particular-la-distribuci%C3%B3n-trinomial"><i class="fa fa-check"></i><b>4.3.4</b> Un caso particular: La distribución trinomial</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="distribuciones-de-probabilidad-multidimensionales.html"><a href="distribuciones-de-probabilidad-multidimensionales.html#distribuciones-marginales"><i class="fa fa-check"></i><b>4.4</b> Distribuciones marginales</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="distribuciones-de-probabilidad-multidimensionales.html"><a href="distribuciones-de-probabilidad-multidimensionales.html#las-marginales-est%C3%A1n-en-los-m%C3%A1rgenes"><i class="fa fa-check"></i><b>4.4.1</b> Las marginales están en los márgenes</a></li>
<li class="chapter" data-level="4.4.2" data-path="distribuciones-de-probabilidad-multidimensionales.html"><a href="distribuciones-de-probabilidad-multidimensionales.html#densidades-marginales-discretas"><i class="fa fa-check"></i><b>4.4.2</b> Densidades marginales discretas</a></li>
<li class="chapter" data-level="4.4.3" data-path="distribuciones-de-probabilidad-multidimensionales.html"><a href="distribuciones-de-probabilidad-multidimensionales.html#trinomial-m5-0.6-0.2-distribuciones-marginales"><i class="fa fa-check"></i><b>4.4.3</b> Trinomial M(5; 0.6, 0.2): Distribuciones marginales</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="distribuciones-de-probabilidad-multidimensionales.html"><a href="distribuciones-de-probabilidad-multidimensionales.html#distribuciones-condicionales"><i class="fa fa-check"></i><b>4.5</b> Distribuciones condicionales</a>
<ul>
<li class="chapter" data-level="4.5.1" data-path="distribuciones-de-probabilidad-multidimensionales.html"><a href="distribuciones-de-probabilidad-multidimensionales.html#densidad-condicional"><i class="fa fa-check"></i><b>4.5.1</b> Densidad condicional</a></li>
<li class="chapter" data-level="4.5.2" data-path="distribuciones-de-probabilidad-multidimensionales.html"><a href="distribuciones-de-probabilidad-multidimensionales.html#trinomial-m5-0.6-0.2-distribuci%C3%B3n-condicional"><i class="fa fa-check"></i><b>4.5.2</b> Trinomial M(5; 0.6, 0.2): Distribución condicional</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="distribuciones-de-probabilidad-multidimensionales.html"><a href="distribuciones-de-probabilidad-multidimensionales.html#vectores-aleatorios-absolutamente-continuos"><i class="fa fa-check"></i><b>4.6</b> Vectores aleatorios absolutamente continuos</a>
<ul>
<li class="chapter" data-level="4.6.1" data-path="distribuciones-de-probabilidad-multidimensionales.html"><a href="distribuciones-de-probabilidad-multidimensionales.html#propiedades-de-la-funci%C3%B3n-de-densidad-conjunta"><i class="fa fa-check"></i><b>4.6.1</b> Propiedades de la función de densidad conjunta</a></li>
<li class="chapter" data-level="4.6.2" data-path="distribuciones-de-probabilidad-multidimensionales.html"><a href="distribuciones-de-probabilidad-multidimensionales.html#densidades-marginales-en-el-caso-continuo"><i class="fa fa-check"></i><b>4.6.2</b> Densidades marginales en el caso continuo</a></li>
<li class="chapter" data-level="4.6.3" data-path="distribuciones-de-probabilidad-multidimensionales.html"><a href="distribuciones-de-probabilidad-multidimensionales.html#densidad-condicional-en-el-caso-continuo"><i class="fa fa-check"></i><b>4.6.3</b> Densidad condicional en el caso continuo</a></li>
<li class="chapter" data-level="4.6.4" data-path="distribuciones-de-probabilidad-multidimensionales.html"><a href="distribuciones-de-probabilidad-multidimensionales.html#la-distribuci%C3%B3n-normal-bivariante"><i class="fa fa-check"></i><b>4.6.4</b> La Distribución Normal Bivariante</a></li>
<li class="chapter" data-level="4.6.5" data-path="distribuciones-de-probabilidad-multidimensionales.html"><a href="distribuciones-de-probabilidad-multidimensionales.html#distribuciones-condicionales-1"><i class="fa fa-check"></i><b>4.6.5</b> Distribuciones Condicionales</a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="distribuciones-de-probabilidad-multidimensionales.html"><a href="distribuciones-de-probabilidad-multidimensionales.html#independencia-de-variables-aleatorias"><i class="fa fa-check"></i><b>4.7</b> Independencia de variables aleatorias</a>
<ul>
<li class="chapter" data-level="4.7.1" data-path="distribuciones-de-probabilidad-multidimensionales.html"><a href="distribuciones-de-probabilidad-multidimensionales.html#primera-caracterizaci%C3%B3n-de-la-independencia"><i class="fa fa-check"></i><b>4.7.1</b> Primera caracterización de la independencia</a></li>
<li class="chapter" data-level="4.7.2" data-path="distribuciones-de-probabilidad-multidimensionales.html"><a href="distribuciones-de-probabilidad-multidimensionales.html#propiedades-de-las-variables-independientes"><i class="fa fa-check"></i><b>4.7.2</b> Propiedades de las variables independientes</a></li>
</ul></li>
<li class="chapter" data-level="4.8" data-path="distribuciones-de-probabilidad-multidimensionales.html"><a href="distribuciones-de-probabilidad-multidimensionales.html#momentos-de-vectores-aleatorios"><i class="fa fa-check"></i><b>4.8</b> Momentos de vectores aleatorios</a>
<ul>
<li class="chapter" data-level="4.8.1" data-path="distribuciones-de-probabilidad-multidimensionales.html"><a href="distribuciones-de-probabilidad-multidimensionales.html#esperanza-de-un-vector-aleatorio-o-vector-de-medias"><i class="fa fa-check"></i><b>4.8.1</b> Esperanza de un vector aleatorio o vector de medias</a></li>
<li class="chapter" data-level="4.8.2" data-path="distribuciones-de-probabilidad-multidimensionales.html"><a href="distribuciones-de-probabilidad-multidimensionales.html#covarianza-entre-dos-variables-aleatorias"><i class="fa fa-check"></i><b>4.8.2</b> Covarianza entre dos variables aleatorias</a></li>
<li class="chapter" data-level="4.8.3" data-path="distribuciones-de-probabilidad-multidimensionales.html"><a href="distribuciones-de-probabilidad-multidimensionales.html#covarianza-y-correlaci%C3%B3n"><i class="fa fa-check"></i><b>4.8.3</b> Covarianza y correlación</a></li>
<li class="chapter" data-level="4.8.4" data-path="distribuciones-de-probabilidad-multidimensionales.html"><a href="distribuciones-de-probabilidad-multidimensionales.html#matriz-de-varianzas-covarianzas"><i class="fa fa-check"></i><b>4.8.4</b> Matriz de varianzas-covarianzas</a></li>
<li class="chapter" data-level="4.8.5" data-path="distribuciones-de-probabilidad-multidimensionales.html"><a href="distribuciones-de-probabilidad-multidimensionales.html#matriz-de-correlaciones"><i class="fa fa-check"></i><b>4.8.5</b> Matriz de correlaciones</a></li>
<li class="chapter" data-level="4.8.6" data-path="distribuciones-de-probabilidad-multidimensionales.html"><a href="distribuciones-de-probabilidad-multidimensionales.html#segunda-caracterizaci%C3%B3n-de-la-independencia"><i class="fa fa-check"></i><b>4.8.6</b> Segunda caracterización de la independencia</a></li>
<li class="chapter" data-level="4.8.7" data-path="distribuciones-de-probabilidad-multidimensionales.html"><a href="distribuciones-de-probabilidad-multidimensionales.html#relaci%C3%B3n-entre-incorrelaci%C3%B3n-e-independencia"><i class="fa fa-check"></i><b>4.8.7</b> Relación entre incorrelación e independencia</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="grandes-muestras.html"><a href="grandes-muestras.html"><i class="fa fa-check"></i><b>5</b> Grandes muestras</a>
<ul>
<li class="chapter" data-level="5.1" data-path="grandes-muestras.html"><a href="grandes-muestras.html#introducci%C3%B3n-aproximaciones-asint%C3%B3ticas"><i class="fa fa-check"></i><b>5.1</b> Introducción: Aproximaciones asintóticas</a></li>
<li class="chapter" data-level="5.2" data-path="grandes-muestras.html"><a href="grandes-muestras.html#ley-de-los-grandes-n%C3%BAmeros-ley-d%C3%A9bil"><i class="fa fa-check"></i><b>5.2</b> Ley de los Grandes Números (Ley débil)</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="grandes-muestras.html"><a href="grandes-muestras.html#ejemplo-3"><i class="fa fa-check"></i><b>5.2.1</b> Ejemplo</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="grandes-muestras.html"><a href="grandes-muestras.html#el-teorema-central-del-l%C3%ADmite"><i class="fa fa-check"></i><b>5.3</b> El teorema central del límite</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="grandes-muestras.html"><a href="grandes-muestras.html#sumas-de-variables-aleatorias"><i class="fa fa-check"></i><b>5.3.1</b> Sumas de variables aleatorias</a></li>
<li class="chapter" data-level="5.3.2" data-path="grandes-muestras.html"><a href="grandes-muestras.html#definici%C3%B3n-de-convergencia-en-ley"><i class="fa fa-check"></i><b>5.3.2</b> Definición de convergencia en ley</a></li>
<li class="chapter" data-level="5.3.3" data-path="grandes-muestras.html"><a href="grandes-muestras.html#enunciado-del-teorema-central-del-l%C3%ADmite"><i class="fa fa-check"></i><b>5.3.3</b> Enunciado del teorema central del límite</a></li>
<li class="chapter" data-level="5.3.4" data-path="grandes-muestras.html"><a href="grandes-muestras.html#algunos-ejemplos-de-aplicaci%C3%B3n-del-tcl"><i class="fa fa-check"></i><b>5.3.4</b> Algunos ejemplos de aplicación del TCL</a></li>
<li class="chapter" data-level="5.3.5" data-path="grandes-muestras.html"><a href="grandes-muestras.html#casos-particulares-m%C3%A1s-notables"><i class="fa fa-check"></i><b>5.3.5</b> Casos particulares más notables</a></li>
<li class="chapter" data-level="5.3.6" data-path="grandes-muestras.html"><a href="grandes-muestras.html#interpretaci%C3%B3n-del-teorema-central-del-l%C3%ADmite"><i class="fa fa-check"></i><b>5.3.6</b> Interpretación del teorema central del límite</a></li>
<li class="chapter" data-level="5.3.7" data-path="grandes-muestras.html"><a href="grandes-muestras.html#acerca-de-las-variables-aproximadamente-normales"><i class="fa fa-check"></i><b>5.3.7</b> Acerca de las variables aproximadamente normales</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="introducción-a-la-inferencia-estadística.html"><a href="introducción-a-la-inferencia-estadística.html"><i class="fa fa-check"></i><b>6</b> Introducción a la inferencia estadística</a>
<ul>
<li class="chapter" data-level="6.1" data-path="introducción-a-la-inferencia-estadística.html"><a href="introducción-a-la-inferencia-estadística.html#inferencia-estad%C3%ADstica"><i class="fa fa-check"></i><b>6.1</b> Inferencia estadística</a></li>
<li class="chapter" data-level="6.2" data-path="introducción-a-la-inferencia-estadística.html"><a href="introducción-a-la-inferencia-estadística.html#problemas-de-inferencia-estad%C3%ADstica"><i class="fa fa-check"></i><b>6.2</b> Problemas de inferencia estadística</a></li>
<li class="chapter" data-level="6.3" data-path="introducción-a-la-inferencia-estadística.html"><a href="introducción-a-la-inferencia-estadística.html#distribuci%C3%B3n-de-la-poblaci%C3%B3n"><i class="fa fa-check"></i><b>6.3</b> Distribución de la población</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="introducción-a-la-inferencia-estadística.html"><a href="introducción-a-la-inferencia-estadística.html#ejemplo-4"><i class="fa fa-check"></i><b>6.3.1</b> Ejemplo</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="introducción-a-la-inferencia-estadística.html"><a href="introducción-a-la-inferencia-estadística.html#muestra-aleatoria-simple"><i class="fa fa-check"></i><b>6.4</b> Muestra aleatoria simple</a>
<ul>
<li class="chapter" data-level="6.4.1" data-path="introducción-a-la-inferencia-estadística.html"><a href="introducción-a-la-inferencia-estadística.html#definici%C3%B3n-muestra-aleatoria-simple"><i class="fa fa-check"></i><b>6.4.1</b> Definición (Muestra aleatoria simple)</a></li>
<li class="chapter" data-level="6.4.2" data-path="introducción-a-la-inferencia-estadística.html"><a href="introducción-a-la-inferencia-estadística.html#distribuci%C3%B3n-de-la-muestra"><i class="fa fa-check"></i><b>6.4.2</b> Distribución de la muestra</a></li>
<li class="chapter" data-level="6.4.3" data-path="introducción-a-la-inferencia-estadística.html"><a href="introducción-a-la-inferencia-estadística.html#casos-particulares"><i class="fa fa-check"></i><b>6.4.3</b> Casos particulares</a></li>
<li class="chapter" data-level="6.4.4" data-path="introducción-a-la-inferencia-estadística.html"><a href="introducción-a-la-inferencia-estadística.html#ejemplo-5"><i class="fa fa-check"></i><b>6.4.4</b> Ejemplo</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="introducción-a-la-inferencia-estadística.html"><a href="introducción-a-la-inferencia-estadística.html#estad%C3%ADsticos"><i class="fa fa-check"></i><b>6.5</b> Estadísticos</a>
<ul>
<li class="chapter" data-level="6.5.1" data-path="introducción-a-la-inferencia-estadística.html"><a href="introducción-a-la-inferencia-estadística.html#definici%C3%B3n"><i class="fa fa-check"></i><b>6.5.1</b> Definición</a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="introducción-a-la-inferencia-estadística.html"><a href="introducción-a-la-inferencia-estadística.html#distribuci%C3%B3n-en-el-muestreo-de-un-estad%C3%ADstico"><i class="fa fa-check"></i><b>6.6</b> Distribución en el muestreo de un estadístico</a>
<ul>
<li class="chapter" data-level="6.6.1" data-path="introducción-a-la-inferencia-estadística.html"><a href="introducción-a-la-inferencia-estadística.html#ejemplo-6"><i class="fa fa-check"></i><b>6.6.1</b> Ejemplo</a></li>
<li class="chapter" data-level="6.6.2" data-path="introducción-a-la-inferencia-estadística.html"><a href="introducción-a-la-inferencia-estadística.html#ejemplo-7"><i class="fa fa-check"></i><b>6.6.2</b> Ejemplo</a></li>
<li class="chapter" data-level="6.6.3" data-path="introducción-a-la-inferencia-estadística.html"><a href="introducción-a-la-inferencia-estadística.html#ejemplo-8"><i class="fa fa-check"></i><b>6.6.3</b> Ejemplo</a></li>
</ul></li>
<li class="chapter" data-level="6.7" data-path="introducción-a-la-inferencia-estadística.html"><a href="introducción-a-la-inferencia-estadística.html#la-distribuci%C3%B3n-emp%C3%ADrica"><i class="fa fa-check"></i><b>6.7</b> La distribución empírica</a>
<ul>
<li class="chapter" data-level="6.7.1" data-path="introducción-a-la-inferencia-estadística.html"><a href="introducción-a-la-inferencia-estadística.html#ejemplo-9"><i class="fa fa-check"></i><b>6.7.1</b> Ejemplo</a></li>
</ul></li>
<li class="chapter" data-level="6.8" data-path="introducción-a-la-inferencia-estadística.html"><a href="introducción-a-la-inferencia-estadística.html#momentos-muestrales"><i class="fa fa-check"></i><b>6.8</b> Momentos muestrales</a>
<ul>
<li class="chapter" data-level="6.8.1" data-path="introducción-a-la-inferencia-estadística.html"><a href="introducción-a-la-inferencia-estadística.html#observaciones"><i class="fa fa-check"></i><b>6.8.1</b> Observaciones</a></li>
</ul></li>
<li class="chapter" data-level="6.9" data-path="introducción-a-la-inferencia-estadística.html"><a href="introducción-a-la-inferencia-estadística.html#distribuci%C3%B3n-en-el-muestreo-de-los-momentos-muestrales"><i class="fa fa-check"></i><b>6.9</b> Distribución en el muestreo de los momentos muestrales</a></li>
<li class="chapter" data-level="6.10" data-path="introducción-a-la-inferencia-estadística.html"><a href="introducción-a-la-inferencia-estadística.html#muestreo-en-poblaciones-normales"><i class="fa fa-check"></i><b>6.10</b> Muestreo en poblaciones normales</a>
<ul>
<li class="chapter" data-level="6.10.1" data-path="introducción-a-la-inferencia-estadística.html"><a href="introducción-a-la-inferencia-estadística.html#la-distribuci%C3%B3n-chi-cuadrado-y-la-varianza-muestral"><i class="fa fa-check"></i><b>6.10.1</b> La distribución chi-cuadrado y la varianza muestral</a></li>
<li class="chapter" data-level="6.10.2" data-path="introducción-a-la-inferencia-estadística.html"><a href="introducción-a-la-inferencia-estadística.html#la-distribuci%C3%B3n-t-de-student-y-el-estad%C3%ADstico-t"><i class="fa fa-check"></i><b>6.10.2</b> La distribución t de Student y el estadístico <span class="math inline">\(T\)</span></a></li>
<li class="chapter" data-level="6.10.3" data-path="introducción-a-la-inferencia-estadística.html"><a href="introducción-a-la-inferencia-estadística.html#la-distribuci%C3%B3n-f-de-fisher-y-la-raz%C3%B3n-de-varianzas."><i class="fa fa-check"></i><b>6.10.3</b> La distribución F de Fisher y la razón de varianzas.</a></li>
</ul></li>
<li class="chapter" data-level="6.11" data-path="introducción-a-la-inferencia-estadística.html"><a href="introducción-a-la-inferencia-estadística.html#ap%C3%A9ndice-t%C3%A9cnico-opcional"><i class="fa fa-check"></i><b>6.11</b> Apéndice técnico (opcional)</a>
<ul>
<li class="chapter" data-level="6.11.1" data-path="introducción-a-la-inferencia-estadística.html"><a href="introducción-a-la-inferencia-estadística.html#funci%C3%B3n-generadora-de-momentos-de-la-media-muestral"><i class="fa fa-check"></i><b>6.11.1</b> Función generadora de momentos de la media muestral</a></li>
<li class="chapter" data-level="6.11.2" data-path="introducción-a-la-inferencia-estadística.html"><a href="introducción-a-la-inferencia-estadística.html#momentos-centrales-y-relaci%C3%B3n-con-la-varianza-muestral"><i class="fa fa-check"></i><b>6.11.2</b> Momentos centrales y relación con la varianza muestral</a></li>
<li class="chapter" data-level="6.11.3" data-path="introducción-a-la-inferencia-estadística.html"><a href="introducción-a-la-inferencia-estadística.html#propiedades-asint%C3%B3ticas-de-los-momentos-muestrales"><i class="fa fa-check"></i><b>6.11.3</b> Propiedades asintóticas de los momentos muestrales</a></li>
<li class="chapter" data-level="6.11.4" data-path="introducción-a-la-inferencia-estadística.html"><a href="introducción-a-la-inferencia-estadística.html#recordatorio-propiedades-%C3%BAtiles-de-la-distribuci%C3%B3n-gamma"><i class="fa fa-check"></i><b>6.11.4</b> Recordatorio: propiedades útiles de la distribución Gamma</a></li>
<li class="chapter" data-level="6.11.5" data-path="introducción-a-la-inferencia-estadística.html"><a href="introducción-a-la-inferencia-estadística.html#derivaci%C3%B3n-estructurada-de-chi2-t-y-f"><i class="fa fa-check"></i><b>6.11.5</b> Derivación estructurada de <span class="math inline">\(\chi^2\)</span>, <span class="math inline">\(t\)</span> y <span class="math inline">\(F\)</span></a></li>
<li class="chapter" data-level="6.11.6" data-path="introducción-a-la-inferencia-estadística.html"><a href="introducción-a-la-inferencia-estadística.html#asint%C3%B3tica-%C3%BAtil-para-inferencia"><i class="fa fa-check"></i><b>6.11.6</b> Asintótica útil para inferencia</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="estimación-puntual.html"><a href="estimación-puntual.html"><i class="fa fa-check"></i><b>7</b> Estimación puntual</a>
<ul>
<li class="chapter" data-level="7.1" data-path="estimación-puntual.html"><a href="estimación-puntual.html#el-problema-de-la-estimaci%C3%B3n-puntual"><i class="fa fa-check"></i><b>7.1</b> El problema de la estimación puntual</a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="estimación-puntual.html"><a href="estimación-puntual.html#criterios-de-optimalidad-de-estimadores.-el-riesgo"><i class="fa fa-check"></i><b>7.1.1</b> Criterios de optimalidad de estimadores. El Riesgo</a></li>
<li class="chapter" data-level="7.1.2" data-path="estimación-puntual.html"><a href="estimación-puntual.html#el-error-cuadr%C3%A1tico-medio"><i class="fa fa-check"></i><b>7.1.2</b> El error cuadrático medio</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="estimación-puntual.html"><a href="estimación-puntual.html#estudio-de-las-propiedades-deseables-de-los-estimadores"><i class="fa fa-check"></i><b>7.2</b> Estudio de las propiedades deseables de los estimadores</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="estimación-puntual.html"><a href="estimación-puntual.html#el-sesgo"><i class="fa fa-check"></i><b>7.2.1</b> El sesgo</a></li>
<li class="chapter" data-level="7.2.2" data-path="estimación-puntual.html"><a href="estimación-puntual.html#consistencia"><i class="fa fa-check"></i><b>7.2.2</b> Consistencia</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="estimación-puntual.html"><a href="estimación-puntual.html#propiedades-de-los-estimadores-consistentes"><i class="fa fa-check"></i><b>7.3</b> Propiedades de los estimadores consistentes</a>
<ul>
<li class="chapter" data-level="7.3.1" data-path="estimación-puntual.html"><a href="estimación-puntual.html#eficiencia"><i class="fa fa-check"></i><b>7.3.1</b> Eficiencia</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="estimación-puntual.html"><a href="estimación-puntual.html#informaci%C3%B3n-de-fisher-y-cota-de-cramerrao"><i class="fa fa-check"></i><b>7.4</b> Información de Fisher y cota de CramerRao</a></li>
<li class="chapter" data-level="7.5" data-path="estimación-puntual.html"><a href="estimación-puntual.html#informaci%C3%B3n-y-verosimilitud-de-un-modelo-estad%C3%ADstico"><i class="fa fa-check"></i><b>7.5</b> Información y verosimilitud de un modelo estadístico</a></li>
<li class="chapter" data-level="7.6" data-path="estimación-puntual.html"><a href="estimación-puntual.html#informaci%C3%B3n-de-fisher"><i class="fa fa-check"></i><b>7.6</b> Información de Fisher</a></li>
<li class="chapter" data-level="7.7" data-path="estimación-puntual.html"><a href="estimación-puntual.html#la-desigualdad-de-cramer-rao"><i class="fa fa-check"></i><b>7.7</b> La desigualdad de Cramer-Rao</a></li>
<li class="chapter" data-level="7.8" data-path="estimación-puntual.html"><a href="estimación-puntual.html#caracterizaci%C3%B3n-del-estimador-eficiente"><i class="fa fa-check"></i><b>7.8</b> Caracterización del estimador eficiente</a></li>
<li class="chapter" data-level="7.9" data-path="estimación-puntual.html"><a href="estimación-puntual.html#estad%C3%ADsticos-suficientes"><i class="fa fa-check"></i><b>7.9</b> Estadísticos suficientes</a>
<ul>
<li class="chapter" data-level="7.9.1" data-path="estimación-puntual.html"><a href="estimación-puntual.html#definici%C3%B3-de-estad%C3%ADsticop-suficiente"><i class="fa fa-check"></i><b>7.9.1</b> Definició de estadísticop suficiente</a></li>
<li class="chapter" data-level="7.9.2" data-path="estimación-puntual.html"><a href="estimación-puntual.html#teorema-de-factorizaci%C3%B3n"><i class="fa fa-check"></i><b>7.9.2</b> Teorema de factorización</a></li>
<li class="chapter" data-level="7.9.3" data-path="estimación-puntual.html"><a href="estimación-puntual.html#propiedades-de-los-estad%C3%ADsticos-suficientes"><i class="fa fa-check"></i><b>7.9.3</b> Propiedades de los estadísticos suficientes</a></li>
</ul></li>
<li class="chapter" data-level="7.10" data-path="estimación-puntual.html"><a href="estimación-puntual.html#obtenci%C3%B3n-de-estimadores"><i class="fa fa-check"></i><b>7.10</b> Obtención de estimadores</a></li>
<li class="chapter" data-level="7.11" data-path="estimación-puntual.html"><a href="estimación-puntual.html#el-m%C3%A9todo-de-los-momentos"><i class="fa fa-check"></i><b>7.11</b> El método de los momentos</a>
<ul>
<li class="chapter" data-level="7.11.1" data-path="estimación-puntual.html"><a href="estimación-puntual.html#observaciones-1"><i class="fa fa-check"></i><b>7.11.1</b> Observaciones</a></li>
</ul></li>
<li class="chapter" data-level="7.12" data-path="estimación-puntual.html"><a href="estimación-puntual.html#el-m%C3%A9todo-del-m%C3%A1ximo-de-verosimilitud"><i class="fa fa-check"></i><b>7.12</b> El método del máximo de verosimilitud</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html"><i class="fa fa-check"></i><b>8</b> Estimación por intérvalos</a>
<ul>
<li class="chapter" data-level="8.1" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#motivaci%C3%B3n-de-los-intervalos-de-confianza-la-estimaci%C3%B3n-puntual-casi-siempre-es-falsa"><i class="fa fa-check"></i><b>8.1</b> Motivación de los intervalos de confianza: la estimación puntual casi siempre es falsa</a></li>
<li class="chapter" data-level="8.2" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#definici%C3%B3n-formal-de-intervalo-de-confianza"><i class="fa fa-check"></i><b>8.2</b> Definición formal de intervalo de confianza</a></li>
<li class="chapter" data-level="8.3" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#un-ejemplo-de-construcci%C3%B3n-de-un-intervalo-de-confianza"><i class="fa fa-check"></i><b>8.3</b> Un ejemplo de construcción de un intervalo de confianza</a>
<ul>
<li class="chapter" data-level="8.3.1" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#planteamiento"><i class="fa fa-check"></i><b>8.3.1</b> Planteamiento</a></li>
<li class="chapter" data-level="8.3.2" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#desarrollo-de-la-construcci%C3%B3n"><i class="fa fa-check"></i><b>8.3.2</b> Desarrollo de la construcción</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#por-qu%C3%A9-hablamos-de-confianza-y-no-de-probabilidad"><i class="fa fa-check"></i><b>8.4</b> ¿Por qué hablamos de confianza y no de probabilidad?</a></li>
<li class="chapter" data-level="8.5" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#elementos-de-un-intervalo-de-confianza"><i class="fa fa-check"></i><b>8.5</b> Elementos de un intervalo de confianza</a></li>
<li class="chapter" data-level="8.6" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#m%C3%A9todo-del-pivote"><i class="fa fa-check"></i><b>8.6</b> Método del pivote</a></li>
<li class="chapter" data-level="8.7" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#algunos-estad%C3%ADsticos-pivote"><i class="fa fa-check"></i><b>8.7</b> Algunos estadísticos pivote</a></li>
<li class="chapter" data-level="8.8" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#intervalo-de-confianza-para-la-media-de-una-distribuci%C3%B3n-normal"><i class="fa fa-check"></i><b>8.8</b> Intervalo de confianza para la media de una distribución Normal</a>
<ul>
<li class="chapter" data-level="8.8.1" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#caso-de-varianza-conocida"><i class="fa fa-check"></i><b>8.8.1</b> Caso de varianza conocida</a></li>
<li class="chapter" data-level="8.8.2" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#caso-de-varianza-desconocida"><i class="fa fa-check"></i><b>8.8.2</b> Caso de varianza desconocida</a></li>
<li class="chapter" data-level="8.8.3" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#calculo-con-r"><i class="fa fa-check"></i><b>8.8.3</b> Calculo con R</a></li>
<li class="chapter" data-level="8.8.4" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#tama%C3%B1o-de-muestra-para-la-media-de-una-distribuci%C3%B3n-normal"><i class="fa fa-check"></i><b>8.8.4</b> Tamaño de muestra para la media de una distribución Normal</a></li>
</ul></li>
<li class="chapter" data-level="8.9" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#intervalo-de-confianza-para-la-varianza-de-una-distribuci%C3%B3n-normal"><i class="fa fa-check"></i><b>8.9</b> Intervalo de confianza para la varianza de una distribución Normal</a></li>
<li class="chapter" data-level="8.10" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#intervalo-de-confianza-para-una-proporci%C3%B3n"><i class="fa fa-check"></i><b>8.10</b> Intervalo de confianza para una proporción</a>
<ul>
<li class="chapter" data-level="8.10.1" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#aproximaci%C3%B3n-asint%C3%B3tica"><i class="fa fa-check"></i><b>8.10.1</b> Aproximación asintótica</a></li>
<li class="chapter" data-level="8.10.2" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#intervalo-exacto"><i class="fa fa-check"></i><b>8.10.2</b> Intervalo exacto</a></li>
<li class="chapter" data-level="8.10.3" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#tama%C3%B1o-muestral-para-una-proporci%C3%B3n"><i class="fa fa-check"></i><b>8.10.3</b> Tamaño muestral para una proporción</a></li>
</ul></li>
<li class="chapter" data-level="8.11" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#intervalo-de-confianza-para-el-par%C3%A1metro-de-una-distribuci%C3%B3n-de-poisson"><i class="fa fa-check"></i><b>8.11</b> Intervalo de confianza para el parámetro de una distribución de Poisson</a>
<ul>
<li class="chapter" data-level="8.11.1" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#aproximaci%C3%B3n-asint%C3%B3tica-1"><i class="fa fa-check"></i><b>8.11.1</b> Aproximación asintótica</a></li>
<li class="chapter" data-level="8.11.2" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#intervalo-exacto-1"><i class="fa fa-check"></i><b>8.11.2</b> Intervalo exacto</a></li>
<li class="chapter" data-level="8.11.3" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#tama%C3%B1o-de-muestra-para-el-par%C3%A1metro-de-una-distribuci%C3%B3n-de-poisson"><i class="fa fa-check"></i><b>8.11.3</b> Tamaño de muestra para el parámetro de una distribución de Poisson</a></li>
</ul></li>
<li class="chapter" data-level="8.12" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#intervalo-de-confianza-para-la-diferencia-de-medias-de-distribuciones-normales-independientes."><i class="fa fa-check"></i><b>8.12</b> Intervalo de confianza para la diferencia de medias de distribuciones normales independientes.</a>
<ul>
<li class="chapter" data-level="8.12.1" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#varianza-com%C3%BAn"><i class="fa fa-check"></i><b>8.12.1</b> Varianza común</a></li>
</ul></li>
<li class="chapter" data-level="8.13" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#intervalo-de-confianza-para-la-diferencia-de-medias-de-distribuciones-normales-independientes.-1"><i class="fa fa-check"></i><b>8.13</b> Intervalo de confianza para la diferencia de medias de distribuciones normales independientes.</a>
<ul>
<li class="chapter" data-level="8.13.1" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#varianza-diferente"><i class="fa fa-check"></i><b>8.13.1</b> Varianza diferente</a></li>
<li class="chapter" data-level="8.13.2" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#caso-de-varianzas-desconocidas-y-diferentes"><i class="fa fa-check"></i><b>8.13.2</b> Caso de varianzas desconocidas y diferentes</a></li>
<li class="chapter" data-level="8.13.3" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#int%C3%A9rvalos-de-confianza-y-decisiones-estad%C3%ADsticas"><i class="fa fa-check"></i><b>8.13.3</b> Intérvalos de confianza y decisiones estadísticas</a></li>
</ul></li>
<li class="chapter" data-level="8.14" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#intervalo-de-confianza-para-el-cociente-de-varianzas-de-distribuciones-normales-independientes"><i class="fa fa-check"></i><b>8.14</b> Intervalo de confianza para el cociente de varianzas de distribuciones normales independientes</a></li>
<li class="chapter" data-level="8.15" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#complementos"><i class="fa fa-check"></i><b>8.15</b> Complementos</a>
<ul>
<li class="chapter" data-level="8.15.1" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#interpretaci%C3%B3n-geom%C3%A9trica-de-los-intervalos-de-confianza"><i class="fa fa-check"></i><b>8.15.1</b> Interpretación geométrica de los intervalos de confianza</a></li>
<li class="chapter" data-level="8.15.2" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#intervalos-para-muestras-grandes"><i class="fa fa-check"></i><b>8.15.2</b> Intervalos para muestras grandes</a></li>
<li class="chapter" data-level="8.15.3" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#intervalos-exactos-para-distribuciones-discretas"><i class="fa fa-check"></i><b>8.15.3</b> Intervalos exactos para distribuciones discretas</a></li>
<li class="chapter" data-level="8.15.4" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#una-aproximaci%C3%B3n-diferente-para-la-distribuci%C3%B3n-de-poisson"><i class="fa fa-check"></i><b>8.15.4</b> Una aproximación diferente para la distribución de Poisson</a></li>
<li class="chapter" data-level="8.15.5" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#aproximaci%C3%B3n-mediante-ch%C3%A9bishev"><i class="fa fa-check"></i><b>8.15.5</b> Aproximación mediante Chébishev</a></li>
</ul></li>
<li class="chapter" data-level="8.16" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#introducci%C3%B3n-2"><i class="fa fa-check"></i><b>8.16</b> Introducción</a>
<ul>
<li class="chapter" data-level="8.16.1" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#de-las-hip%C3%B3tesis-cient%C3%ADficas-a-las-hip%C3%B3tesis-estad%C3%ADsticas"><i class="fa fa-check"></i><b>8.16.1</b> De las hipótesis científicas a las hipótesis estadísticas</a></li>
<li class="chapter" data-level="8.16.2" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#del-lenguaje-natural-a-la-hip%C3%B3tesis-estad%C3%ADstica"><i class="fa fa-check"></i><b>8.16.2</b> Del lenguaje natural a la hipótesis estadística</a></li>
<li class="chapter" data-level="8.16.3" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#caso-1-presentaci%C3%B3n"><i class="fa fa-check"></i><b>8.16.3</b> Caso 1: Presentación</a></li>
<li class="chapter" data-level="8.16.4" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#caso-1-modelo-de-probabilidad"><i class="fa fa-check"></i><b>8.16.4</b> Caso 1: Modelo de probabilidad</a></li>
<li class="chapter" data-level="8.16.5" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#caso-2-presentaci%C3%B3n"><i class="fa fa-check"></i><b>8.16.5</b> Caso 2: Presentación</a></li>
<li class="chapter" data-level="8.16.6" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#caso-2-modelo-de-probabilidad"><i class="fa fa-check"></i><b>8.16.6</b> Caso 2: Modelo de probabilidad</a></li>
</ul></li>
<li class="chapter" data-level="8.17" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#las-hip%C3%B3tesis-del-contraste-de-hip%C3%B3tesis"><i class="fa fa-check"></i><b>8.17</b> Las hipótesis del contraste de hipótesis</a>
<ul>
<li class="chapter" data-level="8.17.1" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#caso-1-hip%C3%B3tesis-para-dirimir-la-controversia-sobre-el-n%C3%BAmero-de-hembras"><i class="fa fa-check"></i><b>8.17.1</b> Caso 1: Hipótesis para dirimir la controversia sobre el número de hembras</a></li>
<li class="chapter" data-level="8.17.2" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#caso-2-hip%C3%B3tesis-a-contrastar-en-el-problema-de-la-tasa-de-statdrolona"><i class="fa fa-check"></i><b>8.17.2</b> Caso 2: Hipótesis a contrastar en el problema de la tasa de statdrolona</a></li>
</ul></li>
<li class="chapter" data-level="8.18" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#compatibilidad-de-resultados-e-hip%C3%B3tesis"><i class="fa fa-check"></i><b>8.18</b> Compatibilidad de resultados e hipótesis</a>
<ul>
<li class="chapter" data-level="8.18.1" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#caso-1-compatibilidad-de-resultados-e-hip%C3%B3tesis"><i class="fa fa-check"></i><b>8.18.1</b> Caso 1: Compatibilidad de resultados e hipótesis</a></li>
<li class="chapter" data-level="8.18.2" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#caso-2-compatibilidad-de-resultados-e-hip%C3%B3tesis"><i class="fa fa-check"></i><b>8.18.2</b> Caso 2: Compatibilidad de resultados e hipótesis</a></li>
</ul></li>
<li class="chapter" data-level="8.19" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#no-todo-es-igualmente-probable"><i class="fa fa-check"></i><b>8.19</b> No todo es igualmente probable…</a>
<ul>
<li class="chapter" data-level="8.19.1" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#caso-1-una-regi%C3%B3n-con-n%C3%BAmero-de-hembras-con-baja-probabilidad-bajo-mathrmh_0"><i class="fa fa-check"></i><b>8.19.1</b> Caso 1: Una región con número de hembras con baja probabilidad bajo <span class="math inline">\(\mathrm{H}_{0}\)</span></a></li>
<li class="chapter" data-level="8.19.2" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#caso-2-medias-de-las-tasas-de-statdrolona-improbables-si-se-cumple-mathrmh_0"><i class="fa fa-check"></i><b>8.19.2</b> Caso 2: Medias de las tasas de statdrolona improbables si se cumple <span class="math inline">\(\mathrm{H}_{0}\)</span></a></li>
</ul></li>
<li class="chapter" data-level="8.20" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#el-papel-privilegiado-de-la-hip%C3%B3tesis-nula-criterio-de-decisi%C3%B3n"><i class="fa fa-check"></i><b>8.20</b> El papel privilegiado de la hipótesis nula: criterio de decisión</a>
<ul>
<li class="chapter" data-level="8.20.1" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#caso-1-n.%C2%BA-de-nidos-propuestos-ad-hoc-como-inicio-de-regi%C3%B3n-cr%C3%ADtica.-regla-de-decisi%C3%B3n-resultante"><i class="fa fa-check"></i><b>8.20.1</b> Caso 1: N.º de nidos propuestos ad hoc como inicio de región crítica. Regla de decisión resultante</a></li>
</ul></li>
<li class="chapter" data-level="8.21" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#hip%C3%B3tesis-nula-y-nivel-de-significaci%C3%B3n"><i class="fa fa-check"></i><b>8.21</b> Hipótesis nula y nivel de significación</a>
<ul>
<li class="chapter" data-level="8.21.1" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#caso-1-nivel-de-significaci%C3%B3n"><i class="fa fa-check"></i><b>8.21.1</b> Caso 1: Nivel de significación</a></li>
<li class="chapter" data-level="8.21.2" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#caso-1-elecci%C3%B3n-de-la-regi%C3%B3n-cr%C3%ADtica"><i class="fa fa-check"></i><b>8.21.2</b> Caso 1: Elección de la región crítica</a></li>
<li class="chapter" data-level="8.21.3" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#caso-2-elecci%C3%B3n-de-la-regi%C3%B3n-cr%C3%ADtica"><i class="fa fa-check"></i><b>8.21.3</b> Caso 2: Elección de la región crítica</a></li>
</ul></li>
<li class="chapter" data-level="8.22" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#regi%C3%B3n-cr%C3%ADtica-y-formalizaci%C3%B3n-del-contraste"><i class="fa fa-check"></i><b>8.22</b> Región crítica y formalización del contraste</a>
<ul>
<li class="chapter" data-level="8.22.1" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#caso-1-resumen-de-conceptos-asociados-al-contraste.-regi%C3%B3n-cr%C3%ADtica"><i class="fa fa-check"></i><b>8.22.1</b> Caso 1: Resumen de conceptos asociados al contraste. Región crítica</a></li>
<li class="chapter" data-level="8.22.2" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#caso-2-tabla-resumen-de-la-regi%C3%B3n-cr%C3%ADtica-el-estad%C3%ADstico-de-test-y-del-criterio-de-decisi%C3%B3n"><i class="fa fa-check"></i><b>8.22.2</b> Caso 2: Tabla resumen de la región crítica, el estadístico de test y del criterio de decisión</a></li>
<li class="chapter" data-level="8.22.3" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#regi%C3%B3n-cr%C3%ADtica-frente-a-estad%C3%ADstico-de-test"><i class="fa fa-check"></i><b>8.22.3</b> Región crítica frente a Estadístico de Test</a></li>
</ul></li>
<li class="chapter" data-level="8.23" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#tabla-de-decisi%C3%B3n-del-contraste"><i class="fa fa-check"></i><b>8.23</b> Tabla de decisión del contraste</a>
<ul>
<li class="chapter" data-level="8.23.1" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#caso-1-evaluaci%C3%B3n-de-los-dos-errores-asociados-al-contraste"><i class="fa fa-check"></i><b>8.23.1</b> Caso 1: Evaluación de los dos errores asociados al contraste</a></li>
<li class="chapter" data-level="8.23.2" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#caso-2-c%C3%A1lculo-expl%C3%ADcito-de-los-errores-de-primera-alpha-y-segunda-especie-1--beta"><i class="fa fa-check"></i><b>8.23.2</b> Caso 2: Cálculo explícito de los errores de primera ( <span class="math inline">\(\alpha\)</span> ) y segunda especie (1- <span class="math inline">\(\beta\)</span> )</a></li>
</ul></li>
<li class="chapter" data-level="8.24" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#relaci%C3%B3n-entre-el-error-de-tipo-i-y-el-de-tipo-ii"><i class="fa fa-check"></i><b>8.24</b> Relación entre el error de tipo I y el de tipo II</a>
<ul>
<li class="chapter" data-level="8.24.1" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#caso-1-evaluaci%C3%B3n-de-alpha-y-1--beta-para-diferentes-regiones-cr%C3%ADticas"><i class="fa fa-check"></i><b>8.24.1</b> Caso 1: Evaluación de <span class="math inline">\(\alpha\)</span> y 1- <span class="math inline">\(\beta\)</span> para diferentes regiones críticas</a></li>
<li class="chapter" data-level="8.24.2" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#caso-2-relaci%C3%B3n-entre-los-errores-de-primera-alpha-y-segunda-especie-1--beta"><i class="fa fa-check"></i><b>8.24.2</b> Caso 2: Relación entre los errores de primera ( <span class="math inline">\(\alpha\)</span> ) y segunda especie (1- <span class="math inline">\(\beta\)</span> )</a></li>
</ul></li>
<li class="chapter" data-level="8.25" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#potencia-y-test-m%C3%A1s-potente"><i class="fa fa-check"></i><b>8.25</b> Potencia y test más potente</a>
<ul>
<li class="chapter" data-level="8.25.1" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#caso-1-potencia-en-hip%C3%B3tesis-simple-vs-simple"><i class="fa fa-check"></i><b>8.25.1</b> Caso 1: Potencia en hipótesis simple vs simple</a></li>
<li class="chapter" data-level="8.25.2" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#caso-2-potencia-en-hip%C3%B3tesis-simple-vs-simple"><i class="fa fa-check"></i><b>8.25.2</b> Caso 2: Potencia en hipótesis simple vs simple</a></li>
</ul></li>
<li class="chapter" data-level="8.26" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#efecto-del-tama%C3%B1o-muestral"><i class="fa fa-check"></i><b>8.26</b> Efecto del tamaño muestral</a>
<ul>
<li class="chapter" data-level="8.26.1" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#caso-1"><i class="fa fa-check"></i><b>8.26.1</b> Caso 1</a></li>
<li class="chapter" data-level="8.26.2" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#caso-2"><i class="fa fa-check"></i><b>8.26.2</b> Caso 2</a></li>
</ul></li>
<li class="chapter" data-level="8.27" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#hip%C3%B3tesis-simples-vs.-hip%C3%B3tesis-compuestas"><i class="fa fa-check"></i><b>8.27</b> Hipótesis simples vs. hipótesis compuestas</a>
<ul>
<li class="chapter" data-level="8.27.1" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#caso-1-hip%C3%B3tesis-compuestas"><i class="fa fa-check"></i><b>8.27.1</b> Caso 1: Hipótesis compuestas</a></li>
<li class="chapter" data-level="8.27.2" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#caso-2-hip%C3%B3tesis-compuestas"><i class="fa fa-check"></i><b>8.27.2</b> Caso 2: Hipótesis compuestas</a></li>
</ul></li>
<li class="chapter" data-level="8.28" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#funci%C3%B3n-de-potencia"><i class="fa fa-check"></i><b>8.28</b> Función de potencia</a>
<ul>
<li class="chapter" data-level="8.28.1" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#caso-1-funci%C3%B3n-de-potencia"><i class="fa fa-check"></i><b>8.28.1</b> Caso 1: Función de potencia</a></li>
<li class="chapter" data-level="8.28.2" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#caso-2-funci%C3%B3n-de-potencia"><i class="fa fa-check"></i><b>8.28.2</b> Caso 2: Función de potencia</a></li>
</ul></li>
<li class="chapter" data-level="8.29" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#tests-%C3%B3ptimos"><i class="fa fa-check"></i><b>8.29</b> Tests óptimos</a></li>
<li class="chapter" data-level="8.30" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#pruebas-bilaterales-y-pruebas-unilaterales"><i class="fa fa-check"></i><b>8.30</b> Pruebas bilaterales y pruebas unilaterales</a>
<ul>
<li class="chapter" data-level="8.30.1" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#caso-1-prueba-unilateral"><i class="fa fa-check"></i><b>8.30.1</b> Caso 1: Prueba unilateral</a></li>
<li class="chapter" data-level="8.30.2" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#caso-2-prueba-unilateral"><i class="fa fa-check"></i><b>8.30.2</b> Caso 2: Prueba unilateral</a></li>
</ul></li>
<li class="chapter" data-level="8.31" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#elecci%C3%B3n-del-nivel-de-significaci%C3%B3n"><i class="fa fa-check"></i><b>8.31</b> Elección del nivel de significación</a></li>
<li class="chapter" data-level="8.32" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#el-p-valor"><i class="fa fa-check"></i><b>8.32</b> El p-valor</a>
<ul>
<li class="chapter" data-level="8.32.1" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#caso-1-c%C3%A1lculo-del-p-valor-prueba-unilateral"><i class="fa fa-check"></i><b>8.32.1</b> Caso 1: Cálculo del p-valor (prueba unilateral)</a></li>
<li class="chapter" data-level="8.32.2" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#caso-2-c%C3%A1lculo-del-p-valor-prueba-unilateral"><i class="fa fa-check"></i><b>8.32.2</b> Caso 2: Cálculo del p-valor (prueba unilateral)</a></li>
<li class="chapter" data-level="8.32.3" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#caso-2-c%C3%A1lculo-del-p-valor-prueba-bilateral"><i class="fa fa-check"></i><b>8.32.3</b> Caso 2: Cálculo del p-valor (prueba bilateral)</a></li>
</ul></li>
<li class="chapter" data-level="8.33" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#pruebas-exactas-y-pruebas-asint%C3%B3ticas"><i class="fa fa-check"></i><b>8.33</b> Pruebas exactas y pruebas asintóticas</a>
<ul>
<li class="chapter" data-level="8.33.1" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#caso-1-test-asint%C3%B3tico"><i class="fa fa-check"></i><b>8.33.1</b> Caso 1: Test asintótico</a></li>
<li class="chapter" data-level="8.33.2" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#caso-2-test-exacto"><i class="fa fa-check"></i><b>8.33.2</b> Caso 2: Test exacto</a></li>
</ul></li>
<li class="chapter" data-level="8.34" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#relaci%C3%B3n-con-los-intervalos-de-confianza"><i class="fa fa-check"></i><b>8.34</b> Relación con los intervalos de confianza</a>
<ul>
<li class="chapter" data-level="8.34.1" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#caso-2-relaci%C3%B3n-con-los-intervalos-de-confianza"><i class="fa fa-check"></i><b>8.34.1</b> Caso 2: Relación con los intervalos de confianza</a></li>
</ul></li>
<li class="chapter" data-level="8.35" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#tama%C3%B1os-de-muestra.-diferencia-m%C3%ADnima-significativa"><i class="fa fa-check"></i><b>8.35</b> Tamaños de muestra. Diferencia mínima significativa</a>
<ul>
<li class="chapter" data-level="8.35.1" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#caso-2-c%C3%A1lculo-del-tama%C3%B1o-de-la-muestra"><i class="fa fa-check"></i><b>8.35.1</b> Caso 2: Cálculo del tamaño de la muestra</a></li>
</ul></li>
<li class="chapter" data-level="8.36" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#esquema-de-un-contraste-correctamente-planteado"><i class="fa fa-check"></i><b>8.36</b> Esquema de un contraste correctamente planteado</a></li>
<li class="chapter" data-level="8.37" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#significaci%C3%B3n-estad%C3%ADstica-y-significaci%C3%B3n-aplicada"><i class="fa fa-check"></i><b>8.37</b> Significación estadística y significación aplicada</a>
<ul>
<li class="chapter" data-level="8.37.1" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#caso-2-significaci%C3%B3n-estad%C3%ADstica-y-aplicada"><i class="fa fa-check"></i><b>8.37.1</b> Caso 2: Significación estadística y aplicada</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="construcción-de-contrastes-de-hipótesis.html"><a href="construcción-de-contrastes-de-hipótesis.html"><i class="fa fa-check"></i><b>9</b> Construcción de contrastes de hipótesis</a>
<ul>
<li class="chapter" data-level="9.1" data-path="construcción-de-contrastes-de-hipótesis.html"><a href="construcción-de-contrastes-de-hipótesis.html#qu%C3%A9-significa-construir-un-contraste-de-hip%C3%B3tesis"><i class="fa fa-check"></i><b>9.1</b> ¿Qué significa “construir” un contraste de hipótesis?</a>
<ul>
<li class="chapter" data-level="9.1.1" data-path="construcción-de-contrastes-de-hipótesis.html"><a href="construcción-de-contrastes-de-hipótesis.html#el-contraste-como-regla-de-decisi%C3%B3n"><i class="fa fa-check"></i><b>9.1.1</b> El contraste como regla de decisión</a></li>
<li class="chapter" data-level="9.1.2" data-path="construcción-de-contrastes-de-hipótesis.html"><a href="construcción-de-contrastes-de-hipótesis.html#nivel-de-significaci%C3%B3n-como-restricci%C3%B3n-b%C3%A1sica"><i class="fa fa-check"></i><b>9.1.2</b> Nivel de significación como restricción básica</a></li>
<li class="chapter" data-level="9.1.3" data-path="construcción-de-contrastes-de-hipótesis.html"><a href="construcción-de-contrastes-de-hipótesis.html#primer-ejemplo-distintos-contrastes-con-el-mismo-nivel"><i class="fa fa-check"></i><b>9.1.3</b> Primer ejemplo: distintos contrastes con el mismo nivel</a></li>
<li class="chapter" data-level="9.1.4" data-path="construcción-de-contrastes-de-hipótesis.html"><a href="construcción-de-contrastes-de-hipótesis.html#potencia-como-criterio-de-comparaci%C3%B3n"><i class="fa fa-check"></i><b>9.1.4</b> Potencia como criterio de comparación</a></li>
<li class="chapter" data-level="9.1.5" data-path="construcción-de-contrastes-de-hipótesis.html"><a href="construcción-de-contrastes-de-hipótesis.html#segundo-ejemplo-misma-alpha-distinta-potencia"><i class="fa fa-check"></i><b>9.1.5</b> Segundo ejemplo: misma alpha, distinta potencia</a></li>
<li class="chapter" data-level="9.1.6" data-path="construcción-de-contrastes-de-hipótesis.html"><a href="construcción-de-contrastes-de-hipótesis.html#de-contrastes-razonables-a-contrastes-%C3%B3ptimos"><i class="fa fa-check"></i><b>9.1.6</b> De contrastes razonables a contrastes óptimos</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="construcción-de-contrastes-de-hipótesis.html"><a href="construcción-de-contrastes-de-hipótesis.html#evidencia-y-decisi%C3%B3n-dos-enfoques-cl%C3%A1sicos"><i class="fa fa-check"></i><b>9.2</b> Evidencia y decisión: dos enfoques clásicos</a>
<ul>
<li class="chapter" data-level="9.2.1" data-path="construcción-de-contrastes-de-hipótesis.html"><a href="construcción-de-contrastes-de-hipótesis.html#el-enfoque-de-fisher-tests-de-significaci%C3%B3n"><i class="fa fa-check"></i><b>9.2.1</b> El enfoque de Fisher: tests de significación</a></li>
<li class="chapter" data-level="9.2.2" data-path="construcción-de-contrastes-de-hipótesis.html"><a href="construcción-de-contrastes-de-hipótesis.html#el-enfoque-de-neymanpearson-contraste-como-regla-de-decisi%C3%B3n"><i class="fa fa-check"></i><b>9.2.2</b> El enfoque de Neyman–Pearson: contraste como regla de decisión</a></li>
<li class="chapter" data-level="9.2.3" data-path="construcción-de-contrastes-de-hipótesis.html"><a href="construcción-de-contrastes-de-hipótesis.html#diferencias-conceptuales-clave"><i class="fa fa-check"></i><b>9.2.3</b> Diferencias conceptuales clave</a></li>
<li class="chapter" data-level="9.2.4" data-path="construcción-de-contrastes-de-hipótesis.html"><a href="construcción-de-contrastes-de-hipótesis.html#convivencia-de-ambos-enfoques-en-la-pr%C3%A1ctica"><i class="fa fa-check"></i><b>9.2.4</b> Convivencia de ambos enfoques en la práctica</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="construcción-de-contrastes-de-hipótesis.html"><a href="construcción-de-contrastes-de-hipótesis.html#tests-%C3%B3ptimos-el-lema-de-neymanpearson"><i class="fa fa-check"></i><b>9.3</b> Tests óptimos: el lema de Neyman–Pearson</a>
<ul>
<li class="chapter" data-level="9.3.1" data-path="construcción-de-contrastes-de-hipótesis.html"><a href="construcción-de-contrastes-de-hipótesis.html#hip%C3%B3tesis-simples-y-raz%C3%B3n-de-verosimilitudes"><i class="fa fa-check"></i><b>9.3.1</b> Hipótesis simples y razón de verosimilitudes</a></li>
<li class="chapter" data-level="9.3.2" data-path="construcción-de-contrastes-de-hipótesis.html"><a href="construcción-de-contrastes-de-hipótesis.html#enunciado-del-lema-de-neymanpearson"><i class="fa fa-check"></i><b>9.3.2</b> Enunciado del lema de Neyman–Pearson</a></li>
<li class="chapter" data-level="9.3.3" data-path="construcción-de-contrastes-de-hipótesis.html"><a href="construcción-de-contrastes-de-hipótesis.html#ejemplo-modelo-normal-con-varianza-conocida"><i class="fa fa-check"></i><b>9.3.3</b> Ejemplo: modelo normal con varianza conocida</a></li>
<li class="chapter" data-level="9.3.4" data-path="construcción-de-contrastes-de-hipótesis.html"><a href="construcción-de-contrastes-de-hipótesis.html#ejemplo-modelo-de-poisson"><i class="fa fa-check"></i><b>9.3.4</b> Ejemplo: modelo de Poisson</a></li>
<li class="chapter" data-level="9.3.5" data-path="construcción-de-contrastes-de-hipótesis.html"><a href="construcción-de-contrastes-de-hipótesis.html#extensiones-del-lema-de-neymanpearson"><i class="fa fa-check"></i><b>9.3.5</b> Extensiones del lema de Neyman–Pearson</a></li>
<li class="chapter" data-level="9.3.6" data-path="construcción-de-contrastes-de-hipótesis.html"><a href="construcción-de-contrastes-de-hipótesis.html#l%C3%ADmites-del-enfoque"><i class="fa fa-check"></i><b>9.3.6</b> Límites del enfoque</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="construcción-de-contrastes-de-hipótesis.html"><a href="construcción-de-contrastes-de-hipótesis.html#contrastes-de-raz%C3%B3n-de-verosimilitudes-generalizados"><i class="fa fa-check"></i><b>9.4</b> Contrastes de razón de verosimilitudes generalizados</a>
<ul>
<li class="chapter" data-level="9.4.1" data-path="construcción-de-contrastes-de-hipótesis.html"><a href="construcción-de-contrastes-de-hipótesis.html#definici%C3%B3n-del-estad%C3%ADstico-de-raz%C3%B3n-de-verosimilitudes"><i class="fa fa-check"></i><b>9.4.1</b> Definición del estadístico de razón de verosimilitudes</a></li>
<li class="chapter" data-level="9.4.2" data-path="construcción-de-contrastes-de-hipótesis.html"><a href="construcción-de-contrastes-de-hipótesis.html#regla-de-decisi%C3%B3n-y-aproximaci%C3%B3n-asint%C3%B3tica"><i class="fa fa-check"></i><b>9.4.2</b> Regla de decisión y aproximación asintótica</a></li>
<li class="chapter" data-level="9.4.3" data-path="construcción-de-contrastes-de-hipótesis.html"><a href="construcción-de-contrastes-de-hipótesis.html#ejemplo-comparaci%C3%B3n-de-par%C3%A1metros-en-un-modelo-de-poisson"><i class="fa fa-check"></i><b>9.4.3</b> Ejemplo: comparación de parámetros en un modelo de Poisson</a></li>
<li class="chapter" data-level="9.4.4" data-path="construcción-de-contrastes-de-hipótesis.html"><a href="construcción-de-contrastes-de-hipótesis.html#ejemplo-contraste-en-un-modelo-exponencial"><i class="fa fa-check"></i><b>9.4.4</b> Ejemplo: contraste en un modelo exponencial</a></li>
<li class="chapter" data-level="9.4.5" data-path="construcción-de-contrastes-de-hipótesis.html"><a href="construcción-de-contrastes-de-hipótesis.html#ejemplo-contraste-en-un-modelo-trinomial"><i class="fa fa-check"></i><b>9.4.5</b> Ejemplo: contraste en un modelo trinomial</a></li>
<li class="chapter" data-level="9.4.6" data-path="construcción-de-contrastes-de-hipótesis.html"><a href="construcción-de-contrastes-de-hipótesis.html#del-contraste-de-raz%C3%B3n-de-verosimilitudes-al-test-ji-cuadrado"><i class="fa fa-check"></i><b>9.4.6</b> Del contraste de razón de verosimilitudes al test ji-cuadrado</a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="construcción-de-contrastes-de-hipótesis.html"><a href="construcción-de-contrastes-de-hipótesis.html#tests-de-permutaciones"><i class="fa fa-check"></i><b>9.5</b> Tests de permutaciones</a>
<ul>
<li class="chapter" data-level="9.5.1" data-path="construcción-de-contrastes-de-hipótesis.html"><a href="construcción-de-contrastes-de-hipótesis.html#idea-b%C3%A1sica"><i class="fa fa-check"></i><b>9.5.1</b> Idea básica</a></li>
<li class="chapter" data-level="9.5.2" data-path="construcción-de-contrastes-de-hipótesis.html"><a href="construcción-de-contrastes-de-hipótesis.html#ejemplo-1-test-de-permutaciones-con-enumeraci%C3%B3n-completa"><i class="fa fa-check"></i><b>9.5.2</b> Ejemplo 1: test de permutaciones con enumeración completa</a></li>
<li class="chapter" data-level="9.5.3" data-path="construcción-de-contrastes-de-hipótesis.html"><a href="construcción-de-contrastes-de-hipótesis.html#valor-observado-del-estad%C3%ADstico"><i class="fa fa-check"></i><b>9.5.3</b> Valor observado del estadístico</a></li>
<li class="chapter" data-level="9.5.4" data-path="construcción-de-contrastes-de-hipótesis.html"><a href="construcción-de-contrastes-de-hipótesis.html#distribuci%C3%B3n-exacta-por-permutaciones"><i class="fa fa-check"></i><b>9.5.4</b> Distribución exacta por permutaciones</a></li>
<li class="chapter" data-level="9.5.5" data-path="construcción-de-contrastes-de-hipótesis.html"><a href="construcción-de-contrastes-de-hipótesis.html#ejemplo-2-test-de-permutaciones-mediante-simulaci%C3%B3n"><i class="fa fa-check"></i><b>9.5.5</b> Ejemplo 2: test de permutaciones mediante simulación</a></li>
<li class="chapter" data-level="9.5.6" data-path="construcción-de-contrastes-de-hipótesis.html"><a href="construcción-de-contrastes-de-hipótesis.html#comentario-final"><i class="fa fa-check"></i><b>9.5.6</b> Comentario final</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="pruebas-de-una-muestra.html"><a href="pruebas-de-una-muestra.html"><i class="fa fa-check"></i><b>10</b> Pruebas de una muestra</a>
<ul>
<li class="chapter" data-level="10.1" data-path="pruebas-de-una-muestra.html"><a href="pruebas-de-una-muestra.html#introducci%C3%B3n-a-los-contrastes-de-una-muestra."><i class="fa fa-check"></i><b>10.1</b> Introducción a los contrastes de una muestra.</a>
<ul>
<li class="chapter" data-level="10.1.1" data-path="pruebas-de-una-muestra.html"><a href="pruebas-de-una-muestra.html#esquema-de-los-contrastes-presentados"><i class="fa fa-check"></i><b>10.1.1</b> Esquema de los contrastes presentados</a></li>
<li class="chapter" data-level="10.1.2" data-path="pruebas-de-una-muestra.html"><a href="pruebas-de-una-muestra.html#contrastes-sobre-los-par%C3%A1metros-de-una-distribuci%C3%B3n-normal"><i class="fa fa-check"></i><b>10.1.2</b> Contrastes sobre los parámetros de una distribución Normal</a></li>
<li class="chapter" data-level="10.1.3" data-path="pruebas-de-una-muestra.html"><a href="pruebas-de-una-muestra.html#contrastes-sobre-una-proporci%C3%B3n"><i class="fa fa-check"></i><b>10.1.3</b> Contrastes sobre una proporción</a></li>
<li class="chapter" data-level="10.1.4" data-path="pruebas-de-una-muestra.html"><a href="pruebas-de-una-muestra.html#esquema-general-de-los-contrastes-presentados"><i class="fa fa-check"></i><b>10.1.4</b> Esquema general de los contrastes presentados</a></li>
<li class="chapter" data-level="10.1.5" data-path="pruebas-de-una-muestra.html"><a href="pruebas-de-una-muestra.html#contrastes-param%C3%A9tricos-frente-a-no-param%C3%A9tricos"><i class="fa fa-check"></i><b>10.1.5</b> Contrastes paramétricos frente a no paramétricos</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="pruebas-de-una-muestra.html"><a href="pruebas-de-una-muestra.html#contraste-de-hip%C3%B3tesis-para-la-media-de-una-distribuci%C3%B3n-normal-con-varianza-conocida-z-test."><i class="fa fa-check"></i><b>10.2</b> Contraste de hipótesis para la media de una distribución Normal con varianza conocida: <span class="math inline">\(Z\)</span>-test.</a>
<ul>
<li class="chapter" data-level="10.2.1" data-path="pruebas-de-una-muestra.html"><a href="pruebas-de-una-muestra.html#introducci%C3%B3n-3"><i class="fa fa-check"></i><b>10.2.1</b> Introducción</a></li>
<li class="chapter" data-level="10.2.2" data-path="pruebas-de-una-muestra.html"><a href="pruebas-de-una-muestra.html#resoluci%C3%B3n-del-contraste-para-la-media-de-una-distribuci%C3%B3n-normal-con-varianza-conocida."><i class="fa fa-check"></i><b>10.2.2</b> Resolución del contraste para la media de una distribución Normal con varianza conocida.</a></li>
<li class="chapter" data-level="10.2.3" data-path="pruebas-de-una-muestra.html"><a href="pruebas-de-una-muestra.html#intervalo-de-confianza-para-la-media-de-una-distribuci%C3%B3n-normal-con-varianza-conocida."><i class="fa fa-check"></i><b>10.2.3</b> Intervalo de confianza para la media de una distribución Normal con varianza conocida.</a></li>
<li class="chapter" data-level="10.2.4" data-path="pruebas-de-una-muestra.html"><a href="pruebas-de-una-muestra.html#c%C3%A1lculo-del-tama%C3%B1o-muestral-para-la-media-de-una-distribuci%C3%B3n-normal-con-varianza-conocida."><i class="fa fa-check"></i><b>10.2.4</b> Cálculo del tamaño muestral para la media de una distribución Normal con varianza conocida.</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="pruebas-de-una-muestra.html"><a href="pruebas-de-una-muestra.html#contraste-de-hip%C3%B3tesis-para-la-media-de-una-distribuci%C3%B3n-normal-con-varianza-desconocida-t-test."><i class="fa fa-check"></i><b>10.3</b> Contraste de hipótesis para la media de una distribución Normal con varianza desconocida: <span class="math inline">\(T\)</span>-test.</a>
<ul>
<li class="chapter" data-level="10.3.1" data-path="pruebas-de-una-muestra.html"><a href="pruebas-de-una-muestra.html#introducci%C3%B3n-4"><i class="fa fa-check"></i><b>10.3.1</b> Introducción</a></li>
<li class="chapter" data-level="10.3.2" data-path="pruebas-de-una-muestra.html"><a href="pruebas-de-una-muestra.html#resoluci%C3%B3n-del-contraste-para-la-media-de-una-distribuci%C3%B3n-normal-con-varianza-desconocida."><i class="fa fa-check"></i><b>10.3.2</b> Resolución del contraste para la media de una distribución Normal con varianza desconocida.</a></li>
<li class="chapter" data-level="10.3.3" data-path="pruebas-de-una-muestra.html"><a href="pruebas-de-una-muestra.html#intervalo-de-confianza-para-la-media-de-una-distribuci%C3%B3n-normal-con-varianza-desconocida."><i class="fa fa-check"></i><b>10.3.3</b> Intervalo de confianza para la media de una distribución Normal con varianza desconocida.</a></li>
<li class="chapter" data-level="10.3.4" data-path="pruebas-de-una-muestra.html"><a href="pruebas-de-una-muestra.html#c%C3%A1lculo-del-tama%C3%B1o-muestral-para-la-media-de-una-distribuci%C3%B3n-normal-con-varianza-desconocida."><i class="fa fa-check"></i><b>10.3.4</b> Cálculo del tamaño muestral para la media de una distribución Normal con varianza desconocida.</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="pruebas-de-una-muestra.html"><a href="pruebas-de-una-muestra.html#contraste-de-hip%C3%B3tesis-para-la-varianza-de-una-distribuci%C3%B3n-normal."><i class="fa fa-check"></i><b>10.4</b> Contraste de hipótesis para la varianza de una distribución Normal.</a>
<ul>
<li class="chapter" data-level="10.4.1" data-path="pruebas-de-una-muestra.html"><a href="pruebas-de-una-muestra.html#introducci%C3%B3n-5"><i class="fa fa-check"></i><b>10.4.1</b> Introducción</a></li>
<li class="chapter" data-level="10.4.2" data-path="pruebas-de-una-muestra.html"><a href="pruebas-de-una-muestra.html#informaci%C3%B3n-previa-premisas"><i class="fa fa-check"></i><b>10.4.2</b> Información previa (premisas)</a></li>
<li class="chapter" data-level="10.4.3" data-path="pruebas-de-una-muestra.html"><a href="pruebas-de-una-muestra.html#resoluci%C3%B3n-del-contraste-para-la-varianza-de-una-distribuci%C3%B3n-normal."><i class="fa fa-check"></i><b>10.4.3</b> Resolución del contraste para la varianza de una distribución Normal.</a></li>
<li class="chapter" data-level="10.4.4" data-path="pruebas-de-una-muestra.html"><a href="pruebas-de-una-muestra.html#intervalo-de-confianza-para-la-varianza-de-una-distribuci%C3%B3n-normal."><i class="fa fa-check"></i><b>10.4.4</b> Intervalo de confianza para la varianza de una distribución Normal.</a></li>
</ul></li>
<li class="chapter" data-level="10.5" data-path="pruebas-de-una-muestra.html"><a href="pruebas-de-una-muestra.html#contraste-de-hip%C3%B3tesis-para-la-proporci%C3%B3n."><i class="fa fa-check"></i><b>10.5</b> Contraste de hipótesis para la proporción.</a>
<ul>
<li class="chapter" data-level="10.5.1" data-path="pruebas-de-una-muestra.html"><a href="pruebas-de-una-muestra.html#introducci%C3%B3n-6"><i class="fa fa-check"></i><b>10.5.1</b> Introducción:</a></li>
<li class="chapter" data-level="10.5.2" data-path="pruebas-de-una-muestra.html"><a href="pruebas-de-una-muestra.html#informaci%C3%B3n-previa-premisas-1"><i class="fa fa-check"></i><b>10.5.2</b> Información previa (premisas)</a></li>
<li class="chapter" data-level="10.5.3" data-path="pruebas-de-una-muestra.html"><a href="pruebas-de-una-muestra.html#resoluci%C3%B3n-del-contraste-para-la-proporci%C3%B3n."><i class="fa fa-check"></i><b>10.5.3</b> Resolución del contraste para la proporción.</a></li>
<li class="chapter" data-level="10.5.4" data-path="pruebas-de-una-muestra.html"><a href="pruebas-de-una-muestra.html#intervalo-de-confianza-para-la-proporci%C3%B3n."><i class="fa fa-check"></i><b>10.5.4</b> Intervalo de confianza para la proporción.</a></li>
<li class="chapter" data-level="10.5.5" data-path="pruebas-de-una-muestra.html"><a href="pruebas-de-una-muestra.html#c%C3%A1lculo-del-tama%C3%B1o-muestral-para-la-proporci%C3%B3n."><i class="fa fa-check"></i><b>10.5.5</b> Cálculo del tamaño muestral para la proporción.</a></li>
</ul></li>
<li class="chapter" data-level="10.6" data-path="pruebas-de-una-muestra.html"><a href="pruebas-de-una-muestra.html#tabla-resumen-para-una-muestra."><i class="fa fa-check"></i><b>10.6</b> Tabla resumen para una muestra.</a></li>
<li class="chapter" data-level="10.7" data-path="pruebas-de-una-muestra.html"><a href="pruebas-de-una-muestra.html#la-importancia-de-elegir-correctamente-la-hip%C3%B3tesis-nula."><i class="fa fa-check"></i><b>10.7</b> La importancia de elegir correctamente la hipótesis nula.</a></li>
<li class="chapter" data-level="10.8" data-path="pruebas-de-una-muestra.html"><a href="pruebas-de-una-muestra.html#relaci%C3%B3n-con-los-intervalos-de-confianza-1"><i class="fa fa-check"></i><b>10.8</b> Relación con los intervalos de confianza</a></li>
<li class="chapter" data-level="10.9" data-path="pruebas-de-una-muestra.html"><a href="pruebas-de-una-muestra.html#relaci%C3%B3n-entre-el-intervalo-y-el-contraste"><i class="fa fa-check"></i><b>10.9</b> Relación entre el intervalo y el contraste</a>
<ul>
<li class="chapter" data-level="10.9.1" data-path="pruebas-de-una-muestra.html"><a href="pruebas-de-una-muestra.html#intervalo-de-confianza-para-la-varianza-de-una-distribuci%C3%B3n-normal-1"><i class="fa fa-check"></i><b>10.9.1</b> Intervalo de confianza para la varianza de una distribución Normal</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="contrastes-con-dos-muestras.html"><a href="contrastes-con-dos-muestras.html"><i class="fa fa-check"></i><b>11</b> Contrastes con dos muestras</a>
<ul>
<li class="chapter" data-level="11.1" data-path="contrastes-con-dos-muestras.html"><a href="contrastes-con-dos-muestras.html#introducci%C3%B3n-7"><i class="fa fa-check"></i><b>11.1</b> Introducción</a></li>
<li class="chapter" data-level="11.2" data-path="contrastes-con-dos-muestras.html"><a href="contrastes-con-dos-muestras.html#premisas-independencia-vs-datos-apareados"><i class="fa fa-check"></i><b>11.2</b> Premisas: independencia vs datos apareados</a>
<ul>
<li class="chapter" data-level="11.2.1" data-path="contrastes-con-dos-muestras.html"><a href="contrastes-con-dos-muestras.html#ejemplo-datos-independientes-vs-apareados"><i class="fa fa-check"></i><b>11.2.1</b> Ejemplo: Datos independientes vs apareados</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="contrastes-con-dos-muestras.html"><a href="contrastes-con-dos-muestras.html#premisas-e-hip%C3%B3tesis-en-comparaciones-de-medias-de-datos-normales-independientes."><i class="fa fa-check"></i><b>11.3</b> Premisas e hipótesis en comparaciones de medias de datos normales independientes.</a></li>
<li class="chapter" data-level="11.4" data-path="contrastes-con-dos-muestras.html"><a href="contrastes-con-dos-muestras.html#comparaci%C3%B3n-de-medias-de-datos-normales-independientes."><i class="fa fa-check"></i><b>11.4</b> Comparación de medias de datos normales independientes.</a>
<ul>
<li class="chapter" data-level="11.4.1" data-path="contrastes-con-dos-muestras.html"><a href="contrastes-con-dos-muestras.html#estad%C3%ADstico-de-test-y-valores-cr%C3%ADticos"><i class="fa fa-check"></i><b>11.4.1</b> Estadístico de test y valores críticos</a></li>
<li class="chapter" data-level="11.4.2" data-path="contrastes-con-dos-muestras.html"><a href="contrastes-con-dos-muestras.html#intervalo-de-confianza-para-la-diferencia-de-medias."><i class="fa fa-check"></i><b>11.4.2</b> Intervalo de confianza para la diferencia de medias.</a></li>
<li class="chapter" data-level="11.4.3" data-path="contrastes-con-dos-muestras.html"><a href="contrastes-con-dos-muestras.html#c%C3%A1lculo-del-tama%C3%B1o-de-muestra"><i class="fa fa-check"></i><b>11.4.3</b> Cálculo del tamaño de muestra</a></li>
</ul></li>
<li class="chapter" data-level="11.5" data-path="contrastes-con-dos-muestras.html"><a href="contrastes-con-dos-muestras.html#comparaci%C3%B3n-de-varianzas-de-datos-normales-independientes."><i class="fa fa-check"></i><b>11.5</b> Comparación de varianzas de datos normales independientes.</a>
<ul>
<li class="chapter" data-level="11.5.1" data-path="contrastes-con-dos-muestras.html"><a href="contrastes-con-dos-muestras.html#premisas-e-hip%C3%B3tesis-en"><i class="fa fa-check"></i><b>11.5.1</b> Premisas e hipótesis en</a></li>
<li class="chapter" data-level="11.5.2" data-path="contrastes-con-dos-muestras.html"><a href="contrastes-con-dos-muestras.html#estad%C3%ADstico-de-test-y-valores-cr%C3%ADticos-1"><i class="fa fa-check"></i><b>11.5.2</b> Estadístico de test y valores críticos</a></li>
<li class="chapter" data-level="11.5.3" data-path="contrastes-con-dos-muestras.html"><a href="contrastes-con-dos-muestras.html#intervalo-de-confianza-para-la-raz%C3%B3n-de-varianzas"><i class="fa fa-check"></i><b>11.5.3</b> Intervalo de confianza para la razón de varianzas</a></li>
</ul></li>
<li class="chapter" data-level="11.6" data-path="contrastes-con-dos-muestras.html"><a href="contrastes-con-dos-muestras.html#comparaciones-de-medias-de-datos-normales-apareados"><i class="fa fa-check"></i><b>11.6</b> Comparaciones de medias de datos normales apareados</a>
<ul>
<li class="chapter" data-level="11.6.1" data-path="contrastes-con-dos-muestras.html"><a href="contrastes-con-dos-muestras.html#premisas-e-hip%C3%B3tesis"><i class="fa fa-check"></i><b>11.6.1</b> Premisas e hipótesis</a></li>
<li class="chapter" data-level="11.6.2" data-path="contrastes-con-dos-muestras.html"><a href="contrastes-con-dos-muestras.html#relaci%C3%B3n-entre-el-contraste-de-datos-apareados-y-el-de-una-media-datos-normales."><i class="fa fa-check"></i><b>11.6.2</b> Relación entre el contraste de datos apareados y el de una media (datos normales).</a></li>
<li class="chapter" data-level="11.6.3" data-path="contrastes-con-dos-muestras.html"><a href="contrastes-con-dos-muestras.html#int%C3%A9rvalos-de-confianza-para-la-diferencia"><i class="fa fa-check"></i><b>11.6.3</b> Intérvalos de confianza para la diferencia</a></li>
<li class="chapter" data-level="11.6.4" data-path="contrastes-con-dos-muestras.html"><a href="contrastes-con-dos-muestras.html#tama%C3%B1o-muestral"><i class="fa fa-check"></i><b>11.6.4</b> Tamaño muestral</a></li>
<li class="chapter" data-level="11.6.5" data-path="contrastes-con-dos-muestras.html"><a href="contrastes-con-dos-muestras.html#ejemplo-efecto-de-una-intervenci%C3%B3n-sobre-el-colesterol-hdl"><i class="fa fa-check"></i><b>11.6.5</b> Ejemplo: efecto de una intervención sobre el colesterol HDL</a></li>
<li class="chapter" data-level="11.6.6" data-path="contrastes-con-dos-muestras.html"><a href="contrastes-con-dos-muestras.html#resumen-datos-independientes-frente-a-datos-apareados"><i class="fa fa-check"></i><b>11.6.6</b> Resumen: Datos independientes frente a datos apareados</a></li>
</ul></li>
<li class="chapter" data-level="11.7" data-path="contrastes-con-dos-muestras.html"><a href="contrastes-con-dos-muestras.html#comparaciones-de-2-proporciones-datos-independientes"><i class="fa fa-check"></i><b>11.7</b> Comparaciones de 2 proporciones (datos independientes)</a>
<ul>
<li class="chapter" data-level="11.7.1" data-path="contrastes-con-dos-muestras.html"><a href="contrastes-con-dos-muestras.html#premisas-e-hip%C3%B3tesis-1"><i class="fa fa-check"></i><b>11.7.1</b> Premisas e hipótesis</a></li>
<li class="chapter" data-level="11.7.2" data-path="contrastes-con-dos-muestras.html"><a href="contrastes-con-dos-muestras.html#estad%C3%ADstico-de-test-y-valores-cr%C3%ADticos-2"><i class="fa fa-check"></i><b>11.7.2</b> Estadístico de test y valores críticos</a></li>
<li class="chapter" data-level="11.7.3" data-path="contrastes-con-dos-muestras.html"><a href="contrastes-con-dos-muestras.html#condiciones-de-aplicaci%C3%B3n-del-test"><i class="fa fa-check"></i><b>11.7.3</b> Condiciones de aplicación del test</a></li>
<li class="chapter" data-level="11.7.4" data-path="contrastes-con-dos-muestras.html"><a href="contrastes-con-dos-muestras.html#intervalo-de-confianza-para-la-diferencia-de-proporciones-datos-independientes."><i class="fa fa-check"></i><b>11.7.4</b> Intervalo de confianza para la diferencia de proporciones (datos independientes).</a></li>
<li class="chapter" data-level="11.7.5" data-path="contrastes-con-dos-muestras.html"><a href="contrastes-con-dos-muestras.html#c%C3%A1lculo-del-tama%C3%B1o-de-muestra-en-el-contraste-de-proporciones-de-datos-independientes."><i class="fa fa-check"></i><b>11.7.5</b> Cálculo del tamaño de muestra en el contraste de proporciones de datos independientes.</a></li>
</ul></li>
<li class="chapter" data-level="11.8" data-path="contrastes-con-dos-muestras.html"><a href="contrastes-con-dos-muestras.html#comparaciones-de-dos-muestras-tabla-resumen"><i class="fa fa-check"></i><b>11.8</b> Comparaciones de dos muestras: Tabla resumen</a></li>
<li class="chapter" data-level="11.9" data-path="contrastes-con-dos-muestras.html"><a href="contrastes-con-dos-muestras.html#complementos-efecto-de-las-transformaciones-de-los-datos-en-el-test-t"><i class="fa fa-check"></i><b>11.9</b> Complementos: efecto de las transformaciones de los datos en el test t</a>
<ul>
<li class="chapter" data-level="11.9.1" data-path="contrastes-con-dos-muestras.html"><a href="contrastes-con-dos-muestras.html#efecto-del-cambi-de-posici%C3%B3n"><i class="fa fa-check"></i><b>11.9.1</b> Efecto del cambi de posición</a></li>
<li class="chapter" data-level="11.9.2" data-path="contrastes-con-dos-muestras.html"><a href="contrastes-con-dos-muestras.html#efecto-de-un-cambio-de-escala"><i class="fa fa-check"></i><b>11.9.2</b> Efecto de un cambio de escala</a></li>
</ul></li>
<li class="chapter" data-level="11.10" data-path="contrastes-con-dos-muestras.html"><a href="contrastes-con-dos-muestras.html#presentaci%C3%B3n-del-caso-1"><i class="fa fa-check"></i><b>11.10</b> Presentación del caso 1</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="las-pruebas-chi-cuadrado.html"><a href="las-pruebas-chi-cuadrado.html"><i class="fa fa-check"></i><b>12</b> Las pruebas “Chi-cuadrado”</a>
<ul>
<li class="chapter" data-level="12.1" data-path="las-pruebas-chi-cuadrado.html"><a href="las-pruebas-chi-cuadrado.html#introducci%C3%B3n-8"><i class="fa fa-check"></i><b>12.1</b> Introducción</a></li>
<li class="chapter" data-level="12.2" data-path="las-pruebas-chi-cuadrado.html"><a href="las-pruebas-chi-cuadrado.html#pruebas-chi2-de-bondad-de-ajuste"><i class="fa fa-check"></i><b>12.2</b> Pruebas <span class="math inline">\(\chi^2\)</span> de bondad de ajuste</a>
<ul>
<li class="chapter" data-level="12.2.1" data-path="las-pruebas-chi-cuadrado.html"><a href="las-pruebas-chi-cuadrado.html#hip%C3%B3tesis-simples"><i class="fa fa-check"></i><b>12.2.1</b> Hipótesis simples</a></li>
<li class="chapter" data-level="12.2.2" data-path="las-pruebas-chi-cuadrado.html"><a href="las-pruebas-chi-cuadrado.html#estad%C3%ADstico-de-pearson"><i class="fa fa-check"></i><b>12.2.2</b> Estadístico de Pearson</a></li>
<li class="chapter" data-level="12.2.3" data-path="las-pruebas-chi-cuadrado.html"><a href="las-pruebas-chi-cuadrado.html#observaciones-pr%C3%A1cticas"><i class="fa fa-check"></i><b>12.2.3</b> Observaciones prácticas</a></li>
<li class="chapter" data-level="12.2.4" data-path="las-pruebas-chi-cuadrado.html"><a href="las-pruebas-chi-cuadrado.html#ejemplo-1-bondad-de-ajuste"><i class="fa fa-check"></i><b>12.2.4</b> Ejemplo 1: Bondad de ajuste</a></li>
<li class="chapter" data-level="12.2.5" data-path="las-pruebas-chi-cuadrado.html"><a href="las-pruebas-chi-cuadrado.html#ejemplo-2-ajuste-de-modelo-gen%C3%A9tico"><i class="fa fa-check"></i><b>12.2.5</b> Ejemplo 2: Ajuste de modelo genético</a></li>
<li class="chapter" data-level="12.2.6" data-path="las-pruebas-chi-cuadrado.html"><a href="las-pruebas-chi-cuadrado.html#hip%C3%B3tesis-compuestas"><i class="fa fa-check"></i><b>12.2.6</b> Hipótesis compuestas</a></li>
<li class="chapter" data-level="12.2.7" data-path="las-pruebas-chi-cuadrado.html"><a href="las-pruebas-chi-cuadrado.html#ejemplo-3-ajuste-de-modelo-gen%C3%A9tico-con-probabilidades-desconocidas"><i class="fa fa-check"></i><b>12.2.7</b> Ejemplo 3: Ajuste de modelo genético con probabilidades desconocidas</a></li>
<li class="chapter" data-level="12.2.8" data-path="las-pruebas-chi-cuadrado.html"><a href="las-pruebas-chi-cuadrado.html#ejemplo-4-ajuste-a-una-distribuci%C3%B3n-normal"><i class="fa fa-check"></i><b>12.2.8</b> Ejemplo 4: Ajuste a una distribución normal</a></li>
</ul></li>
<li class="chapter" data-level="12.3" data-path="las-pruebas-chi-cuadrado.html"><a href="las-pruebas-chi-cuadrado.html#pruebas-de-independencia-en-tablas-de-contingencia"><i class="fa fa-check"></i><b>12.3</b> Pruebas de independencia en tablas de contingencia</a>
<ul>
<li class="chapter" data-level="12.3.1" data-path="las-pruebas-chi-cuadrado.html"><a href="las-pruebas-chi-cuadrado.html#ejemplo-5-relaci%C3%B3n-entre-nivel-de-estudios-y-preferencias"><i class="fa fa-check"></i><b>12.3.1</b> Ejemplo 5: Relación entre nivel de estudios y preferencias</a></li>
</ul></li>
<li class="chapter" data-level="12.4" data-path="las-pruebas-chi-cuadrado.html"><a href="las-pruebas-chi-cuadrado.html#pruebas-de-homogeneidad"><i class="fa fa-check"></i><b>12.4</b> Pruebas de homogeneidad</a>
<ul>
<li class="chapter" data-level="12.4.1" data-path="las-pruebas-chi-cuadrado.html"><a href="las-pruebas-chi-cuadrado.html#ejemplo-6-homogeneidad-en-los-grupos-sangu%C3%ADneos"><i class="fa fa-check"></i><b>12.4.1</b> Ejemplo 6: Homogeneidad en los grupos sanguíneos</a></li>
</ul></li>
<li class="chapter" data-level="12.5" data-path="las-pruebas-chi-cuadrado.html"><a href="las-pruebas-chi-cuadrado.html#ejemplos-resueltos-en-r"><i class="fa fa-check"></i><b>12.5</b> Ejemplos resueltos en R</a>
<ul>
<li class="chapter" data-level="12.5.1" data-path="las-pruebas-chi-cuadrado.html"><a href="las-pruebas-chi-cuadrado.html#ejemplo-1-bondad-de-ajuste-1"><i class="fa fa-check"></i><b>12.5.1</b> Ejemplo 1: Bondad de ajuste</a></li>
<li class="chapter" data-level="12.5.2" data-path="las-pruebas-chi-cuadrado.html"><a href="las-pruebas-chi-cuadrado.html#ejemplo-2-ajuste-de-modelo-gen%C3%A9tico-1"><i class="fa fa-check"></i><b>12.5.2</b> Ejemplo 2: Ajuste de modelo genético</a></li>
<li class="chapter" data-level="12.5.3" data-path="las-pruebas-chi-cuadrado.html"><a href="las-pruebas-chi-cuadrado.html#ejemplo-3-ajuste-de-modelo-gen%C3%A9tico-con-probabilidades-desconocidas-1"><i class="fa fa-check"></i><b>12.5.3</b> Ejemplo 3: Ajuste de modelo genético con probabilidades desconocidas</a></li>
<li class="chapter" data-level="12.5.4" data-path="las-pruebas-chi-cuadrado.html"><a href="las-pruebas-chi-cuadrado.html#ejemplo-4-ajuste-a-una-distribuci%C3%B3n-normal-1"><i class="fa fa-check"></i><b>12.5.4</b> Ejemplo 4: Ajuste a una distribución normal</a></li>
<li class="chapter" data-level="12.5.5" data-path="las-pruebas-chi-cuadrado.html"><a href="las-pruebas-chi-cuadrado.html#ejemplo-5-relaci%C3%B3n-entre-nivel-de-estudios-y-preferencias-1"><i class="fa fa-check"></i><b>12.5.5</b> Ejemplo 5: Relación entre nivel de estudios y preferencias</a></li>
<li class="chapter" data-level="12.5.6" data-path="las-pruebas-chi-cuadrado.html"><a href="las-pruebas-chi-cuadrado.html#ejemplo-6-homogeneidad-en-los-grupos-sangu%C3%ADneos-1"><i class="fa fa-check"></i><b>12.5.6</b> Ejemplo 6: Homogeneidad en los grupos sanguíneos</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="13" data-path="estadística-no-paramétrica.html"><a href="estadística-no-paramétrica.html"><i class="fa fa-check"></i><b>13</b> Estadística no paramétrica</a>
<ul>
<li class="chapter" data-level="13.1" data-path="estadística-no-paramétrica.html"><a href="estadística-no-paramétrica.html#introducci%C3%B3n-9"><i class="fa fa-check"></i><b>13.1</b> Introducción</a></li>
<li class="chapter" data-level="13.2" data-path="estadística-no-paramétrica.html"><a href="estadística-no-paramétrica.html#test-de-los-signos"><i class="fa fa-check"></i><b>13.2</b> Test de los signos</a>
<ul>
<li class="chapter" data-level="13.2.1" data-path="estadística-no-paramétrica.html"><a href="estadística-no-paramétrica.html#test-para-la-mediana"><i class="fa fa-check"></i><b>13.2.1</b> Test para la mediana</a></li>
<li class="chapter" data-level="13.2.2" data-path="estadística-no-paramétrica.html"><a href="estadística-no-paramétrica.html#test-de-los-signos-para-datos-apareados"><i class="fa fa-check"></i><b>13.2.2</b> Test de los signos para datos apareados</a></li>
<li class="chapter" data-level="13.2.3" data-path="estadística-no-paramétrica.html"><a href="estadística-no-paramétrica.html#test-para-datos-binarios"><i class="fa fa-check"></i><b>13.2.3</b> Test para datos binarios</a></li>
</ul></li>
<li class="chapter" data-level="13.3" data-path="estadística-no-paramétrica.html"><a href="estadística-no-paramétrica.html#test-de-mcnemar"><i class="fa fa-check"></i><b>13.3</b> Test de McNemar</a></li>
<li class="chapter" data-level="13.4" data-path="estadística-no-paramétrica.html"><a href="estadística-no-paramétrica.html#test-de-los-rangos-con-signo-de-wilcoxon"><i class="fa fa-check"></i><b>13.4</b> Test de los rangos con signo de Wilcoxon</a>
<ul>
<li class="chapter" data-level="13.4.1" data-path="estadística-no-paramétrica.html"><a href="estadística-no-paramétrica.html#observaciones-3"><i class="fa fa-check"></i><b>13.4.1</b> Observaciones</a></li>
<li class="chapter" data-level="13.4.2" data-path="estadística-no-paramétrica.html"><a href="estadística-no-paramétrica.html#ejemplo-4-1"><i class="fa fa-check"></i><b>13.4.2</b> Ejemplo 4</a></li>
<li class="chapter" data-level="13.4.3" data-path="estadística-no-paramétrica.html"><a href="estadística-no-paramétrica.html#ejemplo-5-1"><i class="fa fa-check"></i><b>13.4.3</b> Ejemplo 5</a></li>
</ul></li>
<li class="chapter" data-level="13.5" data-path="estadística-no-paramétrica.html"><a href="estadística-no-paramétrica.html#el-test-u-de-mann-whitney"><i class="fa fa-check"></i><b>13.5</b> El test <span class="math inline">\(U\)</span> de Mann-Whitney</a>
<ul>
<li class="chapter" data-level="13.5.1" data-path="estadística-no-paramétrica.html"><a href="estadística-no-paramétrica.html#observaciones-4"><i class="fa fa-check"></i><b>13.5.1</b> Observaciones</a></li>
<li class="chapter" data-level="13.5.2" data-path="estadística-no-paramétrica.html"><a href="estadística-no-paramétrica.html#ejemplo-6-1"><i class="fa fa-check"></i><b>13.5.2</b> Ejemplo 6</a></li>
</ul></li>
<li class="chapter" data-level="13.6" data-path="estadística-no-paramétrica.html"><a href="estadística-no-paramétrica.html#comparaci%C3%B3n-de-medianas"><i class="fa fa-check"></i><b>13.6</b> Comparación de medianas</a>
<ul>
<li class="chapter" data-level="13.6.1" data-path="estadística-no-paramétrica.html"><a href="estadística-no-paramétrica.html#ejemplo-7-1"><i class="fa fa-check"></i><b>13.6.1</b> Ejemplo 7</a></li>
</ul></li>
<li class="chapter" data-level="13.7" data-path="estadística-no-paramétrica.html"><a href="estadística-no-paramétrica.html#test-de-kolmogorov-smirnov-para-la-homogeneidad"><i class="fa fa-check"></i><b>13.7</b> Test de Kolmogorov-Smirnov para la homogeneidad</a>
<ul>
<li class="chapter" data-level="13.7.1" data-path="estadística-no-paramétrica.html"><a href="estadística-no-paramétrica.html#ejemplo-8-1"><i class="fa fa-check"></i><b>13.7.1</b> Ejemplo 8</a></li>
</ul></li>
<li class="chapter" data-level="13.8" data-path="estadística-no-paramétrica.html"><a href="estadística-no-paramétrica.html#test-h-de-kruskal-wallis"><i class="fa fa-check"></i><b>13.8</b> Test <span class="math inline">\(H\)</span> de Kruskal-Wallis</a>
<ul>
<li class="chapter" data-level="13.8.1" data-path="estadística-no-paramétrica.html"><a href="estadística-no-paramétrica.html#observaciones-5"><i class="fa fa-check"></i><b>13.8.1</b> Observaciones</a></li>
<li class="chapter" data-level="13.8.2" data-path="estadística-no-paramétrica.html"><a href="estadística-no-paramétrica.html#ejemplo-9-1"><i class="fa fa-check"></i><b>13.8.2</b> Ejemplo 9</a></li>
</ul></li>
<li class="chapter" data-level="13.9" data-path="estadística-no-paramétrica.html"><a href="estadística-no-paramétrica.html#test-de-friedman"><i class="fa fa-check"></i><b>13.9</b> Test de Friedman</a>
<ul>
<li class="chapter" data-level="13.9.1" data-path="estadística-no-paramétrica.html"><a href="estadística-no-paramétrica.html#ejemplo-10"><i class="fa fa-check"></i><b>13.9.1</b> Ejemplo 10</a></li>
<li class="chapter" data-level="13.9.2" data-path="estadística-no-paramétrica.html"><a href="estadística-no-paramétrica.html#observaciones-6"><i class="fa fa-check"></i><b>13.9.2</b> Observaciones</a></li>
</ul></li>
<li class="chapter" data-level="13.10" data-path="estadística-no-paramétrica.html"><a href="estadística-no-paramétrica.html#coeficientes-de-correlaci%C3%B3n-no-param%C3%A9tricos"><i class="fa fa-check"></i><b>13.10</b> Coeficientes de correlación no paramétricos</a>
<ul>
<li class="chapter" data-level="13.10.1" data-path="estadística-no-paramétrica.html"><a href="estadística-no-paramétrica.html#coeficiente-tau-de-kendall"><i class="fa fa-check"></i><b>13.10.1</b> Coeficiente <span class="math inline">\(\tau\)</span> de Kendall</a></li>
<li class="chapter" data-level="13.10.2" data-path="estadística-no-paramétrica.html"><a href="estadística-no-paramétrica.html#coeficiente-de-correlaci%C3%B3n-por-rangos-de-spearman"><i class="fa fa-check"></i><b>13.10.2</b> Coeficiente de correlación por rangos de Spearman</a></li>
<li class="chapter" data-level="13.10.3" data-path="estadística-no-paramétrica.html"><a href="estadística-no-paramétrica.html#el-par%C3%A1metro-poblacional-asociado-a-los-coeficientes-de-correlaci%C3%B3n-no-param%C3%A9tricos"><i class="fa fa-check"></i><b>13.10.3</b> El parámetro poblacional asociado a los coeficientes de correlación no paramétricos</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="14" data-path="métodos-de-computación-intensiva-el-bootstrap.html"><a href="métodos-de-computación-intensiva-el-bootstrap.html"><i class="fa fa-check"></i><b>14</b> Métodos de computación intensiva: El <em>Bootstrap</em></a>
<ul>
<li class="chapter" data-level="14.1" data-path="métodos-de-computación-intensiva-el-bootstrap.html"><a href="métodos-de-computación-intensiva-el-bootstrap.html#introducci%C3%B3n-precisi%C3%B3n-de-un-estimador"><i class="fa fa-check"></i><b>14.1</b> Introducción: Precisión de un estimador</a>
<ul>
<li class="chapter" data-level="14.1.1" data-path="métodos-de-computación-intensiva-el-bootstrap.html"><a href="métodos-de-computación-intensiva-el-bootstrap.html#ejemplos-de-introducci%C3%B3n"><i class="fa fa-check"></i><b>14.1.1</b> Ejemplos de introducción</a></li>
<li class="chapter" data-level="14.1.2" data-path="métodos-de-computación-intensiva-el-bootstrap.html"><a href="métodos-de-computación-intensiva-el-bootstrap.html#estimaci%C3%B3n-del-error-est%C3%A1ndar-soluciones-cl%C3%A1sica-y-bootstrap"><i class="fa fa-check"></i><b>14.1.2</b> Estimación del error estándar: soluciones clásica y bootstrap</a></li>
<li class="chapter" data-level="14.1.3" data-path="métodos-de-computación-intensiva-el-bootstrap.html"><a href="métodos-de-computación-intensiva-el-bootstrap.html#estimadores-de-substituci%C3%B3n-plug-in-y-funcionales-estad%C3%ADsticos"><i class="fa fa-check"></i><b>14.1.3</b> Estimadores de substitución (“plug-in”) y Funcionales estadísticos</a></li>
<li class="chapter" data-level="14.1.4" data-path="métodos-de-computación-intensiva-el-bootstrap.html"><a href="métodos-de-computación-intensiva-el-bootstrap.html#el-bootstrap"><i class="fa fa-check"></i><b>14.1.4</b> El bootstrap</a></li>
<li class="chapter" data-level="14.1.5" data-path="métodos-de-computación-intensiva-el-bootstrap.html"><a href="métodos-de-computación-intensiva-el-bootstrap.html#ejemplo-1-continuaci%C3%B3n"><i class="fa fa-check"></i><b>14.1.5</b> Ejemplo 1 (continuación)</a></li>
<li class="chapter" data-level="14.1.6" data-path="métodos-de-computación-intensiva-el-bootstrap.html"><a href="métodos-de-computación-intensiva-el-bootstrap.html#resumen-del-mundo-real-al-mundo-bootstrap"><i class="fa fa-check"></i><b>14.1.6</b> Resumen: del mundo real al mundo bootstrap</a></li>
<li class="chapter" data-level="14.1.7" data-path="métodos-de-computación-intensiva-el-bootstrap.html"><a href="métodos-de-computación-intensiva-el-bootstrap.html#otros-aspectos"><i class="fa fa-check"></i><b>14.1.7</b> Otros aspectos</a></li>
<li class="chapter" data-level="14.1.8" data-path="métodos-de-computación-intensiva-el-bootstrap.html"><a href="métodos-de-computación-intensiva-el-bootstrap.html#ejercicios"><i class="fa fa-check"></i><b>14.1.8</b> Ejercicios</a></li>
<li class="chapter" data-level="14.1.9" data-path="métodos-de-computación-intensiva-el-bootstrap.html"><a href="métodos-de-computación-intensiva-el-bootstrap.html#practicas"><i class="fa fa-check"></i><b>14.1.9</b> Practicas</a></li>
</ul></li>
<li class="chapter" data-level="14.2" data-path="métodos-de-computación-intensiva-el-bootstrap.html"><a href="métodos-de-computación-intensiva-el-bootstrap.html#estimaci%C3%B3n-y-correcci%C3%B3n-del-sesgo-de-un-estimador"><i class="fa fa-check"></i><b>14.2</b> Estimación y corrección del sesgo de un estimador</a>
<ul>
<li class="chapter" data-level="14.2.1" data-path="métodos-de-computación-intensiva-el-bootstrap.html"><a href="métodos-de-computación-intensiva-el-bootstrap.html#introducci%C3%B3n-10"><i class="fa fa-check"></i><b>14.2.1</b> Introducción</a></li>
<li class="chapter" data-level="14.2.2" data-path="métodos-de-computación-intensiva-el-bootstrap.html"><a href="métodos-de-computación-intensiva-el-bootstrap.html#estimaci%C3%B3n-bootstrap-del-sesgo"><i class="fa fa-check"></i><b>14.2.2</b> Estimación bootstrap del sesgo</a></li>
<li class="chapter" data-level="14.2.3" data-path="métodos-de-computación-intensiva-el-bootstrap.html"><a href="métodos-de-computación-intensiva-el-bootstrap.html#ejemplo-1.-estimaci%C3%B3n-del-sesgo-del-estimador-de-la-varianza"><i class="fa fa-check"></i><b>14.2.3</b> Ejemplo 1. Estimación del sesgo del estimador de la varianza</a></li>
<li class="chapter" data-level="14.2.4" data-path="métodos-de-computación-intensiva-el-bootstrap.html"><a href="métodos-de-computación-intensiva-el-bootstrap.html#correcci%C3%B3n-del-sesgo-de-un-estimador"><i class="fa fa-check"></i><b>14.2.4</b> Corrección del sesgo de un estimador</a></li>
<li class="chapter" data-level="14.2.5" data-path="métodos-de-computación-intensiva-el-bootstrap.html"><a href="métodos-de-computación-intensiva-el-bootstrap.html#el-jackknife"><i class="fa fa-check"></i><b>14.2.5</b> El jackknife</a></li>
<li class="chapter" data-level="14.2.6" data-path="métodos-de-computación-intensiva-el-bootstrap.html"><a href="métodos-de-computación-intensiva-el-bootstrap.html#ejercicios-1"><i class="fa fa-check"></i><b>14.2.6</b> Ejercicios</a></li>
</ul></li>
<li class="chapter" data-level="14.3" data-path="métodos-de-computación-intensiva-el-bootstrap.html"><a href="métodos-de-computación-intensiva-el-bootstrap.html#intervalos-de-confianza-bootstrap"><i class="fa fa-check"></i><b>14.3</b> Intervalos de confianza bootstrap</a>
<ul>
<li class="chapter" data-level="14.3.1" data-path="métodos-de-computación-intensiva-el-bootstrap.html"><a href="métodos-de-computación-intensiva-el-bootstrap.html#introducci%C3%B3n-11"><i class="fa fa-check"></i><b>14.3.1</b> Introducción</a></li>
<li class="chapter" data-level="14.3.2" data-path="métodos-de-computación-intensiva-el-bootstrap.html"><a href="métodos-de-computación-intensiva-el-bootstrap.html#intervalos-de-confianza-est%C3%A1ndar"><i class="fa fa-check"></i><b>14.3.2</b> Intervalos de confianza estándar</a></li>
</ul></li>
<li class="chapter" data-level="14.4" data-path="métodos-de-computación-intensiva-el-bootstrap.html"><a href="métodos-de-computación-intensiva-el-bootstrap.html#intervalos-de-confianza-basados-en-tablas-bootstrap"><i class="fa fa-check"></i><b>14.4</b> Intervalos de confianza basados en tablas bootstrap</a>
<ul>
<li class="chapter" data-level="14.4.1" data-path="métodos-de-computación-intensiva-el-bootstrap.html"><a href="métodos-de-computación-intensiva-el-bootstrap.html#el-caso-de-la-t-de-student"><i class="fa fa-check"></i><b>14.4.1</b> El caso de la t de Student</a></li>
<li class="chapter" data-level="14.4.2" data-path="métodos-de-computación-intensiva-el-bootstrap.html"><a href="métodos-de-computación-intensiva-el-bootstrap.html#intervalos-de-confianza-basados-en-tablas-bootstrap-1"><i class="fa fa-check"></i><b>14.4.2</b> Intervalos de confianza basados en tablas bootstrap</a></li>
<li class="chapter" data-level="14.4.3" data-path="métodos-de-computación-intensiva-el-bootstrap.html"><a href="métodos-de-computación-intensiva-el-bootstrap.html#el-bootstrap-t-y-las-transformaciones"><i class="fa fa-check"></i><b>14.4.3</b> El bootstrap-t y las transformaciones</a></li>
</ul></li>
<li class="chapter" data-level="14.5" data-path="métodos-de-computación-intensiva-el-bootstrap.html"><a href="métodos-de-computación-intensiva-el-bootstrap.html#intervalos-de-confianza-basados-en-percentiles-bootstrap"><i class="fa fa-check"></i><b>14.5</b> Intervalos de confianza basados en percentiles bootstrap</a>
<ul>
<li class="chapter" data-level="14.5.1" data-path="métodos-de-computación-intensiva-el-bootstrap.html"><a href="métodos-de-computación-intensiva-el-bootstrap.html#intervalos-basados-en-percentiles-de-una-distribuci%C3%B3n-normal"><i class="fa fa-check"></i><b>14.5.1</b> Intervalos basados en percentiles de una distribución normal</a></li>
<li class="chapter" data-level="14.5.2" data-path="métodos-de-computación-intensiva-el-bootstrap.html"><a href="métodos-de-computación-intensiva-el-bootstrap.html#los-intervalos-percentil"><i class="fa fa-check"></i><b>14.5.2</b> Los intervalos percentil</a></li>
<li class="chapter" data-level="14.5.3" data-path="métodos-de-computación-intensiva-el-bootstrap.html"><a href="métodos-de-computación-intensiva-el-bootstrap.html#ejercicios-2"><i class="fa fa-check"></i><b>14.5.3</b> Ejercicios</a></li>
<li class="chapter" data-level="14.5.4" data-path="métodos-de-computación-intensiva-el-bootstrap.html"><a href="métodos-de-computación-intensiva-el-bootstrap.html#pr%C3%A1cticas"><i class="fa fa-check"></i><b>14.5.4</b> Prácticas</a></li>
</ul></li>
<li class="chapter" data-level="14.6" data-path="métodos-de-computación-intensiva-el-bootstrap.html"><a href="métodos-de-computación-intensiva-el-bootstrap.html#contraste-de-hip%C3%B3tesis-mediante-bootstrap"><i class="fa fa-check"></i><b>14.6</b> Contraste de hipótesis mediante bootstrap</a>
<ul>
<li class="chapter" data-level="14.6.1" data-path="métodos-de-computación-intensiva-el-bootstrap.html"><a href="métodos-de-computación-intensiva-el-bootstrap.html#introducci%C3%B3n-12"><i class="fa fa-check"></i><b>14.6.1</b> Introducción</a></li>
</ul></li>
<li class="chapter" data-level="14.7" data-path="métodos-de-computación-intensiva-el-bootstrap.html"><a href="métodos-de-computación-intensiva-el-bootstrap.html#elecci%C3%B3n-del-estad%C3%ADstico-de-test"><i class="fa fa-check"></i><b>14.7</b> Elección del estadístico de test</a></li>
<li class="chapter" data-level="14.8" data-path="métodos-de-computación-intensiva-el-bootstrap.html"><a href="métodos-de-computación-intensiva-el-bootstrap.html#c%C3%A1lculo-del-p-valor-1"><i class="fa fa-check"></i><b>14.8</b> Cálculo del p-valor</a>
<ul>
<li class="chapter" data-level="14.8.1" data-path="métodos-de-computación-intensiva-el-bootstrap.html"><a href="métodos-de-computación-intensiva-el-bootstrap.html#contrastes-de-hip%C3%B3tesis-bootstrap"><i class="fa fa-check"></i><b>14.8.1</b> Contrastes de hipótesis bootstrap</a></li>
</ul></li>
<li class="chapter" data-level="14.9" data-path="métodos-de-computación-intensiva-el-bootstrap.html"><a href="métodos-de-computación-intensiva-el-bootstrap.html#ejemplo-1-comparaci%C3%B3n-de-medias"><i class="fa fa-check"></i><b>14.9</b> Ejemplo 1: Comparación de medias</a></li>
<li class="chapter" data-level="14.10" data-path="métodos-de-computación-intensiva-el-bootstrap.html"><a href="métodos-de-computación-intensiva-el-bootstrap.html#ejemplo-2-prueba-de-independencia"><i class="fa fa-check"></i><b>14.10</b> Ejemplo 2: Prueba de independencia</a>
<ul>
<li class="chapter" data-level="14.10.1" data-path="métodos-de-computación-intensiva-el-bootstrap.html"><a href="métodos-de-computación-intensiva-el-bootstrap.html#relaci%C3%B3n-entre-contrastes-de-hip%C3%B3tesis-intervalos-de-confianza-y-el-bootstrap"><i class="fa fa-check"></i><b>14.10.1</b> Relación entre contrastes de hipótesis, intervalos de confianza y el bootstrap</a></li>
</ul></li>
<li class="chapter" data-level="14.11" data-path="métodos-de-computación-intensiva-el-bootstrap.html"><a href="métodos-de-computación-intensiva-el-bootstrap.html#validez-de-los-m%C3%A9todos-bootstrap"><i class="fa fa-check"></i><b>14.11</b> Validez de los métodos bootstrap</a>
<ul>
<li class="chapter" data-level="14.11.1" data-path="métodos-de-computación-intensiva-el-bootstrap.html"><a href="métodos-de-computación-intensiva-el-bootstrap.html#introducci%C3%B3n-13"><i class="fa fa-check"></i><b>14.11.1</b> Introducción</a></li>
<li class="chapter" data-level="14.11.2" data-path="métodos-de-computación-intensiva-el-bootstrap.html"><a href="métodos-de-computación-intensiva-el-bootstrap.html#teorema-central-del-l%C3%ADmite-bootstrap"><i class="fa fa-check"></i><b>14.11.2</b> Teorema central del límite bootstrap</a></li>
<li class="chapter" data-level="14.11.3" data-path="métodos-de-computación-intensiva-el-bootstrap.html"><a href="métodos-de-computación-intensiva-el-bootstrap.html#validaci%C3%B3n-del-bootstrap-mediante-simulaci%C3%B3n"><i class="fa fa-check"></i><b>14.11.3</b> Validación del bootstrap mediante simulación</a></li>
</ul></li>
<li class="chapter" data-level="14.12" data-path="métodos-de-computación-intensiva-el-bootstrap.html"><a href="métodos-de-computación-intensiva-el-bootstrap.html#bibliograf%C3%ADa"><i class="fa fa-check"></i><b>14.12</b> Bibliografía</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="bibliografia.html"><a href="bibliografia.html"><i class="fa fa-check"></i>Bibliografia</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Fundamentos de Inferencia Estadistica</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="métodos-de-computación-intensiva-el-bootstrap" class="section level1 hasAnchor" number="14">
<h1><span class="header-section-number">Capítulo 14</span> Métodos de computación intensiva: El <em>Bootstrap</em><a href="métodos-de-computación-intensiva-el-bootstrap.html#m%C3%A9todos-de-computaci%C3%B3n-intensiva-el-bootstrap" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="introducción-precisión-de-un-estimador" class="section level2 hasAnchor" number="14.1">
<h2><span class="header-section-number">14.1</span> Introducción: Precisión de un estimador<a href="métodos-de-computación-intensiva-el-bootstrap.html#introducci%C3%B3n-precisi%C3%B3n-de-un-estimador" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Los métodos bootstrap que se expondrán a continuación son un conjunto de técnicas introducidas por Bradley Efron de la universidad de Stanford a finales de los años 70 ([7,9,12]) y que se han revelado como una poderosa herramienta de gran utilidad en diversos campos de la estadística.</p>
<div id="ejemplos-de-introducción" class="section level3 hasAnchor" number="14.1.1">
<h3><span class="header-section-number">14.1.1</span> Ejemplos de introducción<a href="métodos-de-computación-intensiva-el-bootstrap.html#ejemplos-de-introducci%C3%B3n" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Empezaremos por considerar algunas situaciones aparentemente sencillas en donde se nos presentarán algunos problemas difíciles de resolver mediante las técnicas “clásicas” de la inferencia estadística (convendremos en denominar así al conjunto de métodos paramétricos y no paramétricos que constituyen el soporte usual de la inferencia estadística).</p>
<div id="problema-1-tiempo-de-supervivencia" class="section level4 hasAnchor" number="14.1.1.1">
<h4><span class="header-section-number">14.1.1.1</span> Problema 1: Tiempo de supervivencia<a href="métodos-de-computación-intensiva-el-bootstrap.html#problema-1-tiempo-de-supervivencia" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Supongamos que estamos interesados en estudiar el tiempo de vida, en años, de unos enfermos después de un trasplante de riñón. El parámetro que nos interesa estimar es el tiempo medio de vida tras la intervención. Para estimarlo podemos utilizar la media muestral, <span class="math inline">\(\bar{X}\)</span> o la mediana muestral <span class="math inline">\(\widehat{\operatorname{Med}}(X)\)</span>. Los datos de que disponemos <span class="math inline">\((n=9)\)</span> son los siguientes:</p>
<p>Tabla 1
Tiempos de supervivencia
| .14 | .18 | .25 | .26 | .37 |
| :— | :— | :— | :— | :— |
| .43 | .51 | .61 | .71 | |</p>
<p>El valor de los estimadores seleccionados, calculados sobre la muestra es:</p>
<ul>
<li>Media muestral, <span class="math inline">\(\bar{X}=0,384\)</span>,</li>
<li>Mediana muestral, <span class="math inline">\(\widehat{\operatorname{Med}}(X)=0,37\)</span></li>
</ul>
</div>
<div id="problema-2-coeficiente-de-correlación" class="section level4 hasAnchor" number="14.1.1.2">
<h4><span class="header-section-number">14.1.1.2</span> Problema 2: Coeficiente de correlación<a href="métodos-de-computación-intensiva-el-bootstrap.html#problema-2-coeficiente-de-correlaci%C3%B3n" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Disponemos de las notas de C.O.U. y de selectividad de un grupo de 15 alumnos de primer curso de la universidad y deseamos determinar cuán relacionadas se hallan las dos. Puesto que se trata de medidas numéricas podemos medir el grado de relación entre ambas mediante el coeficiente de correlación de Pearson:
<span class="math inline">\(\hat{\rho}=\frac{\sum_{i=1}^{n}\left(X_{i}-\bar{X}\right)\left(Y_{i}-\bar{Y}\right)}{\sqrt{\sum_{i=1}^{n}\left(X_{i}-\bar{X}\right)^{2}} \sqrt{\sum_{i=1}^{n}\left(Y_{i}-\bar{Y}\right)^{2}}}, \quad\left(\bar{X}=\sum_{i=1}^{n} X_{i} / n, \bar{Y}=\sum_{i=1}^{n} Y_{i} / n\right)\)</span>.
La muestra de 15 estudiantes que utilizaremos para el estudio es la siguiente:</p>
<p>Tabla 2
Notas de COU y de selectividad de 15 estudiantes
| Estudiante núm: | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 |
| :— | :—: | :—: | :—: | :—: | :—: | :—: | :—: | :—: |
| Selectividad | 5.76 | 6.35 | 5.58 | 5.78 | 6.66 | 5.8 | 5.55 | 6.61 |
| COU | 6.78 | 6.6 | 5.62 | 6.06 | 6.88 | 6.14 | 6 | 6.86 |
| Estudiante núm: | 9 | 10 | 11 | 12 | 13 | 14 | 15 | |
| Selectividad | 6.51 | 6.05 | 6.53 | 5.75 | 5.45 | 5.72 | 5.94 | |
| COU | 6.72 | 6.26 | 6.24 | 5.48 | 5.52 | 5.76 | 5.92 | |</p>
<p>El valor del coeficiente de correlación sobre esta muestra es:</p>
<p><span class="math display">\[
\hat{\rho}=0,776
\]</span></p>
</div>
</div>
<div id="estimación-del-error-estándar-soluciones-clásica-y-bootstrap" class="section level3 hasAnchor" number="14.1.2">
<h3><span class="header-section-number">14.1.2</span> Estimación del error estándar: soluciones clásica y bootstrap<a href="métodos-de-computación-intensiva-el-bootstrap.html#estimaci%C3%B3n-del-error-est%C3%A1ndar-soluciones-cl%C3%A1sica-y-bootstrap" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Una vez calculadas las estimaciones sobre la muestra suele interesar disponer de alguna medida de su fiabilidad.</p>
<p>En el caso de la media muestral podemos calcular su error estándar</p>
<p><span class="math display">\[
\hat{\sigma}_{\bar{X}}=\frac{\sqrt{\sum\left(X_{i}-\bar{X}\right)^{2}}}{n}
\]</span></p>
<p>pero para la mediana y el coeficiente de correlación, no existe una fórmula de validez tan general -es decir rápida de calcular y libre de suposiciones sobre la distribución de la variable o variables- como la anterior.</p>
<p>Antes de pasar a buscar una solución general para el tipo de problema planteado vamos a establecer una formalización que nos permita tratar ambas situaciones de forma unificada:</p>
<ul>
<li>Partimos de unos datos, es decir una muestra de una variable aleatoria, unidimensional o multidimensional, que sigue una distribución <span class="math inline">\(F\)</span>.</li>
</ul>
<p><span class="math display">\[
\mathbf{X}=\left(X_{1}, X_{2}, \ldots, X_{n}\right), \quad \mathbf{X} \sim F
\]</span></p>
<ul>
<li>Existe un parámetro de interés <span class="math inline">\(\theta\)</span>, que, como en los ejemplos anteriores, puede ser el valor medio, la mediana o el coeficiente de correlación poblacionales. (Estamos pues suponiendo un modelo estadístico paramétrico <span class="math inline">\(\left(\Omega, \mathcal{A},\left\{P_{\theta}, \theta \in \Theta\right\}\right)\)</span> o abreviadamente <span class="math inline">\(\mathbf{X} \sim F_{\theta}, \theta \in \Theta\)</span> ).</li>
<li>Deseamos estimar <span class="math inline">\(\theta\)</span> a partir de la muestra.</li>
</ul>
<p>En las circunstancias anteriores se nos plantean usualmente dos cuestiones básicas:</p>
<ol style="list-style-type: decimal">
<li>Qué estadístico <span class="math inline">\(\hat{\theta}(\mathbf{X})\)</span> resulta el (más) adecuado para estimar <span class="math inline">\(\theta\)</span> ?</li>
<li>Qué tan “preciso” es <span class="math inline">\(\hat{\theta}\)</span> como estimador de <span class="math inline">\(\theta\)</span> ?</li>
</ol>
<p>Una primera aproximación para responder a estas dos preguntas consiste en acudir a la teoría clásica de la estimación máximo-verosímil, que sugiere como solución a la primera pregunta el uso del estimador máximo-verosímil de <span class="math inline">\(\theta\)</span>, llamémosle <span class="math inline">\(\hat{\theta}_{M V}\)</span>.</p>
<p>Bajo condiciones bastante generales esta teoría establece que el estimador máximo verosímil, <span class="math inline">\(\hat{\theta}_{M V}\)</span>, es asintóticamente normal de media igual al valor del parámetro <span class="math inline">\(\theta\)</span> y de varianza asintótica <span class="math inline">\(\frac{1}{I(\theta)}\)</span>, donde <span class="math inline">\(I(\theta)\)</span> representa la información de Fisher del modelo.</p>
<p><span class="math display">\[
\hat{\theta}_{M V} \simeq A N\left(\theta, \frac{1}{I(\theta)}\right)
\]</span></p>
<p>Como es habitual en estos casos error estándar de <span class="math inline">\(\hat{\theta}_{M V}\)</span> se puede aproximar, en muestras grandes, por</p>
<p><span class="math display">\[
\hat{\sigma}_{\hat{\theta}_{M V}} \simeq \frac{1}{\sqrt{I(\hat{\theta})}}
\]</span></p>
<p>donde <span class="math inline">\(I(\hat{\theta})\)</span> suele denominarse información observada del modelo, frente a <span class="math inline">\(I(\theta)\)</span> que se denomina al hacer esta distinción, información esperada.</p>
<p>La aplicación del método de la máxima verosimilitud al caso de <span class="math inline">\(\theta= E_{F}(X)\)</span> nos conduce a</p>
<p><span class="math display">\[
\begin{aligned}
\hat{\theta}_{M V} &amp; =\bar{X} \\
\hat{\sigma}_{\hat{\theta}_{M V}} &amp; =\sqrt{\frac{1}{n^{2}} \sum_{i=1}^{n}\left(X_{i}-\bar{X}\right)^{2}}
\end{aligned}
\]</span></p>
<p>En el caso de la mediana y el coeficiente de correlación la respuesta no es tan inmediata. Antes de considerar el enfoque bootstrap</p>
</div>
<div id="estimadores-de-substitución-plug-in-y-funcionales-estadísticos" class="section level3 hasAnchor" number="14.1.3">
<h3><span class="header-section-number">14.1.3</span> Estimadores de substitución (“plug-in”) y Funcionales estadísticos<a href="métodos-de-computación-intensiva-el-bootstrap.html#estimadores-de-substituci%C3%B3n-plug-in-y-funcionales-estad%C3%ADsticos" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="la-distribución-muestral-o-empírica" class="section level4 hasAnchor" number="14.1.3.1">
<h4><span class="header-section-number">14.1.3.1</span> La distribución muestral (o empírica)<a href="métodos-de-computación-intensiva-el-bootstrap.html#la-distribuci%C3%B3n-muestral-o-emp%C3%ADrica" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Sea <span class="math inline">\(\left(x_{1}, x_{2}, \ldots, x_{n}\right)\)</span> una realización de una muestra aleatoria simple <span class="math inline">\(\left(X_{1}, X_{2}, \ldots, X_{n}\right)\)</span> de una v.a. <span class="math inline">\(X\)</span>. Podemos asociar una distribución a las observaciones <span class="math inline">\(\left(x_{1}, x_{2}, \ldots, x_{n}\right)\)</span> con la pretensión de que, en tanto que la muestra “emula” a la población, <span class="math inline">\(X\)</span>, la distribución de la muestra <span class="math inline">\(F_{n}(x)\)</span> emule la distribución de la población. Esta distribución se denomina distribución empírica o distribució muestral y se representa por:</p>
<p><span class="math display">\[
F_{n}(x)=F^{*}(x)=F_{n}^{*}(x)=\frac{\sharp d^{\prime} \text { elementos muestrales }}{n}
\]</span></p>
<p>En la práctica se construye ordenando la muestra</p>
<p><span class="math display">\[
x_{1}, x_{2}, \ldots, x_{n} \longrightarrow x_{(1)}&lt;x_{(2)} \ldots&lt;x_{(n)}
\]</span></p>
<p>definiendo:</p>
<p><span class="math display">\[
F_{n}(x)=\left\{\begin{array}{ll}
0 &amp; \text { si } x&lt;x_{(1)} \\
\frac{k}{n} &amp; \text { si } x_{(k)} \leq x&lt;x_{(k+1)} \\
1 &amp; \text { si } x \geq x_{(n)}
\end{array}\right\}
\]</span></p>
<p>Ejemplo 1 Extraemos una muestra y obtenemos:</p>
<table>
<thead>
<tr class="header">
<th align="center"><span class="math inline">\(x_{1}\)</span></th>
<th align="left"><span class="math inline">\(x_{2}\)</span></th>
<th align="left"><span class="math inline">\(x_{3}\)</span></th>
<th align="center"><span class="math inline">\(x_{4}\)</span></th>
<th align="left"><span class="math inline">\(x_{5}\)</span></th>
<th align="center"><span class="math inline">\(x_{6}\)</span></th>
<th align="left"><span class="math inline">\(x_{7}\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">5.1</td>
<td align="left">3.4</td>
<td align="left">1.2</td>
<td align="center">17.6</td>
<td align="left">2.1</td>
<td align="center">16.4</td>
<td align="left">4.3</td>
</tr>
</tbody>
</table>
<p>Una vez ordenada queda:</p>
<p><span class="math display">\[
\begin{array}{lllllll}
x_{(3)} &amp; x_{(5)} &amp; x_{(2)} &amp; x_{(7)} &amp; x_{(1)} &amp; x_{(6)} &amp; x_{(4)}
\end{array}
\]</span></p>
<p>y si hacemos la representación gráfica:</p>
<p>La distribución muestral refleja exclusivamente los valores contenidos en la muestra y por lo tanto no se relaciona directamente ni con la distribución (conjunta) de la muestra <span class="math inline">\(G\left(\left(X_{1}, X_{2}, \ldots, X_{n}\right)\right)\)</span> ni con la distribución de la población: <span class="math inline">\(F\)</span>. Sin embargo, como es razonable esperar <span class="math inline">\(F_{n}(x)\)</span> proporciona una imagen aproximada de la distribución de la población de donde se ha extraido la muestra. Pueden repasarse las propiedades de la distribución empírica en Velez y García (1993,[23]).</p>
</div>
<div id="estimadores-de-substitución" class="section level4 hasAnchor" number="14.1.3.2">
<h4><span class="header-section-number">14.1.3.2</span> Estimadores de substitución<a href="métodos-de-computación-intensiva-el-bootstrap.html#estimadores-de-substituci%C3%B3n" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>El principio de substitución es un método de estimación, o más bien una idea subyacente en algunos métodos de estimación. El estimador de substitución de un parámetro <span class="math inline">\(\theta=T(F)\)</span> se define como</p>
<p><span class="math display">\[
\hat{\theta}=T\left(F_{n}\right) .
\]</span></p>
<p>En otras palabras, estimamos la función <span class="math inline">\(\theta=T(F)\)</span> por la misma función de la función de distribución empírica, <span class="math inline">\(F_{n}\)</span>, o, en general, por la misma función de algún estimador de la función de distribución, <span class="math inline">\(\hat{F}\)</span>.</p>
<p>Un enfoque algo más formal que el anterior consiste en introducir los el concepto de funcional estadístico. Este concepto ha resultado de gran utilidad en diversas áreas de la estadística -como el de la robustez- y que permite
utilizar una notación que arroja considerable luz sobre el significado del bootstrap .</p>
<p>Consideremos la situación introducida en el párrafo anterior donde se tiene una muestra de observaciones iid de una cierta función de distribución <span class="math inline">\(F\)</span>, siendo <span class="math inline">\(F_{n}\)</span> la función de distribución empírica de la muestra. Como hemos indicado, muchos estadísticos importantes pueden representarse como funciones de la función de distribución empírica, llamémosles <span class="math inline">\(T\left(F_{n}\right)\)</span>. Obsérvese que esto significa que <span class="math inline">\(T\)</span> es una función de algún conjunto <span class="math inline">\(\mathcal{F}\)</span> de funciones de distribución -al que pertenecen <span class="math inline">\(F_{n}\)</span> y <span class="math inline">\(F\)</span> - en <span class="math inline">\(\mathbf{R}\)</span> :</p>
<p><span class="math display">\[
\begin{aligned}
T: \mathcal{F} &amp; \longrightarrow \mathbf{R} \\
G &amp; \longrightarrow T(G) .
\end{aligned}
\]</span></p>
<p>(Es habitual denominar funcional a aquellas funciones en que el conjunto origen es, a su vez, un conjunto de funciones).</p>
<p>Por ejemplo para la varianza de <span class="math inline">\(F, \sigma^{2}\)</span>, el funcional relevante es:</p>
<p><span class="math display">\[
T(F)=\int\left[x-\int x d F(x)\right]^{2} d F(x)
\]</span></p>
<p>donde la integral <span class="math inline">\(\int() d F(x)\)</span> se puede tomar en el sentido de Riemann-Stieltjes, con lo que <span class="math inline">\(T\left(F_{n}\right)\)</span> es la varianza muestral</p>
<p><span class="math display">\[
T\left(F_{n}\right)=S^{2}=1 / n \sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}
\]</span></p>
<p>La idea central del uso de estimadores que son funcionales estadísticos y que, por tanto se basan en el principio de substitución es la siguiente: dado que <span class="math inline">\(F_{n}\)</span> es un estimador razonable de <span class="math inline">\(F\)</span> puede esperarse que <span class="math inline">\(T\left(F_{n}\right)\)</span> se relacione con <span class="math inline">\(T(F)\)</span> de forma similar siempre que el funcional <span class="math inline">\(T(\cdot)\)</span> se comporte “suficientemente bien” en una entorno de <span class="math inline">\(F\)</span>. Esta idea conduce a la consideración de <span class="math inline">\(F\)</span> como un punto en una colección <span class="math inline">\(\mathcal{F}\)</span> de funciones de distribución y a nociones de continuidad, diferenciabilidad y otras propiedades de regularidad que no discutiremos aquí dado que escapan de nuestro objetivo. Para un estudio de los funcionales estadísticos puede consultarse p.ej. Serfling (1980, [20]).</p>
<p>Veamos la utilidad de este enfoque en el problema que nos ocupa:</p>
<ul>
<li>Como hemos indicado en la sección anterior nuestro objetivo es estimar algún parámetro <span class="math inline">\(\theta\)</span>, que generalmente podrá expresarse como <span class="math inline">\(\theta(F)\)</span> siendo <span class="math inline">\(F\)</span> la función de distribución de cada <span class="math inline">\(X_{i}\)</span> en <span class="math inline">\(\left(X_{1}, X_{2}, \ldots, X_{n}\right)\)</span>. Por ejemplo:</li>
</ul>
<p><span class="math display">\[
\begin{aligned}
&amp; \theta=E_{F}(X)=\theta(F) \\
&amp; \theta=\operatorname{Med}(X)=\left\{m: P_{F}(X \leq m)=1 / 2\right\}=\theta(F)
\end{aligned}
\]</span></p>
<ul>
<li>Para estimar <span class="math inline">\(\theta\)</span> utilizaremos un estimador de substitución <span class="math inline">\(\hat{\theta}\)</span> ( <span class="math inline">\(\hat{\theta}\)</span> es un funcional de <span class="math inline">\(F_{n}\)</span> ), es decir <span class="math inline">\(\hat{\theta}=\theta\left(F_{n}\right)\)</span>. Por ejemplo:</li>
</ul>
<p><span class="math display">\[
\begin{aligned}
&amp; \hat{\theta}=\bar{X}=\int X d F_{n}(x)=\frac{1}{n} \sum_{i=1}^{n} x_{i}=\theta\left(F_{n}\right) \\
&amp; \hat{\theta}=\widehat{\operatorname{Med}}(X)=\left\{m: \frac{\# x_{i} \leq m}{n}=1 / 2\right\}=\theta\left(F_{n}\right)
\end{aligned}
\]</span></p>
<ul>
<li>Centrándonos en <span class="math inline">\(\hat{\theta}=\bar{X}\)</span> podemos ver como la medida de precisión que deseamos obtener, su error estándar, también es un funcional de <span class="math inline">\(F\)</span> :</li>
</ul>
<p><span class="math display">\[
\sigma_{\bar{X}}=\frac{\sigma(X)}{\sqrt{n}}=\frac{\sqrt{\int\left[x-\int x d F(x)\right]^{2} d F(x)}}{\sqrt{n}}=\sigma_{\bar{X}}(F)
\]</span></p>
<p>y que el estimador de su varianza es el mismo funcional aplicado sobre <span class="math inline">\(F_{n}\)</span> es decir:</p>
<p><span class="math display">\[
\hat{\sigma}_{\bar{X}}=\frac{\hat{\sigma}(X)}{\sqrt{n}}=\frac{\sqrt{1 / n \sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}}}{\sqrt{n}}=\sigma_{\bar{X}}\left(F_{n}\right) .
\]</span></p>
<p>Vemos pues que una forma de obtener una estimación del error estándar de un estimador, <span class="math inline">\(\hat{\sigma}_{\hat{\theta}}\)</span> consiste en substituir <span class="math inline">\(F\)</span> por <span class="math inline">\(F_{n}\)</span> en la expresión del error estándar “poblacional” de <span class="math inline">\(\hat{\theta}, \sigma_{\hat{\theta}}=\sigma_{\hat{\theta}}(F)\)</span>, siempre que ésta sea conocida. De forma esquemática el proceso consistirá en:</p>
<p><span class="math display">\[
\sigma_{\hat{\theta}}=\sigma_{\hat{\theta}}(F) \Longrightarrow \sigma_{\hat{\theta}}\left(F_{n}\right)=\widehat{\sigma}_{\hat{\theta}} .
\]</span></p>
<p>El método anterior presenta el inconveniente obvio de que cuando la forma (el funcional) de <span class="math inline">\(\sigma_{\hat{\theta}}(F)\)</span> es desconocida no es posible realizar la substitución de <span class="math inline">\(F\)</span> por <span class="math inline">\(F_{n}\)</span>. Este es por ejemplo el caso del error estándar de la mediana o el coeficiente de correlación. En el apartado siguiente se presenta el método bootstrap a partir del cual es posible realizar la aproximación que nos interesa</p>
<p><span class="math display">\[
\hat{\sigma}_{\hat{\theta}} \simeq \sigma_{\hat{\theta}}\left(F_{n}\right)
\]</span></p>
<p>sin que sea necesario conocer la forma de <span class="math inline">\(\sigma_{\hat{\theta}}(F)\)</span>.</p>
</div>
</div>
<div id="el-bootstrap" class="section level3 hasAnchor" number="14.1.4">
<h3><span class="header-section-number">14.1.4</span> El bootstrap<a href="métodos-de-computación-intensiva-el-bootstrap.html#el-bootstrap" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="estimación-de-la-distribución-en-el-muestreo" class="section level4 hasAnchor" number="14.1.4.1">
<h4><span class="header-section-number">14.1.4.1</span> Estimación de la distribución en el muestreo<a href="métodos-de-computación-intensiva-el-bootstrap.html#estimaci%C3%B3n-de-la-distribuci%C3%B3n-en-el-muestreo" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>En la sección anterior se ha propuesto el principio de substitución para estimar el error estándar de un estimador, es decir:</p>
<p><span class="math display">\[
\widehat{\sigma_{\hat{\theta}}(F)}=\sigma_{\hat{\theta}}(\hat{F}),
\]</span></p>
<p>supuesta conocida la forma del funcional <span class="math inline">\(\sigma_{\hat{\theta}}(F)\)</span> y dado un estimador <span class="math inline">\(\hat{F}\)</span> de <span class="math inline">\(F\)</span>, que suele ser la función de distribución empírica <span class="math inline">\(F_{n}\)</span>.</p>
<p>Sea <span class="math inline">\(\mathbf{X} \stackrel{\text { i.i.d. }}{\sim} F\)</span> una muestra aleatoria simple y <span class="math inline">\(\mathcal{R}_{n}(\mathbf{X}, F)\)</span> una función medible de la muestra que por tanto depende también de <span class="math inline">\(F\)</span> y de <span class="math inline">\(n\)</span>. El estimador <span class="math inline">\(\hat{\theta}\)</span>, considerado hasta el momento, es un caso particular de tal función. Llamemos <span class="math inline">\(H_{F, n}(x)\)</span> a la función de distribución de <span class="math inline">\(\mathcal{R}_{n}(\mathbf{X}, F)\)</span>, es decir:</p>
<p><span class="math display">\[
H_{F, n}(x)=P\left\{\mathcal{R}_{n}(\mathbf{X}, F) \leq x\right\}
\]</span></p>
<p>En general nos referiremos a ella simplemente como <span class="math inline">\(H_{F}(x)\)</span>, omitiendo la dependencia de <span class="math inline">\(n\)</span>.</p>
<p>Cuando <span class="math inline">\(\mathcal{R}_{n}(\mathbf{X}, F)=\hat{\theta}\)</span> entonces el error estándar de <span class="math inline">\(\hat{\theta}\)</span> coincide con la desviación típica de <span class="math inline">\(\mathcal{R}_{n}(\mathbf{X}, F)\)</span> es decir:</p>
<p><span class="math display">\[
\sigma_{F}(\hat{\theta})=\sqrt{\operatorname{var}_{H}\left(\mathcal{R}_{n}(\mathbf{X}, F)\right)}
\]</span></p>
<p>Una alternativa al conocimiento de la forma de <span class="math inline">\(\sigma_{\hat{\theta}}(F)\)</span> consiste en estimar directamente la distribución <span class="math inline">\(H_{F}(x)\)</span> y tomar su varianza como un estimador de <span class="math inline">\(\operatorname{var}_{H}\left(\mathcal{R}_{n}(\mathbf{X}, F)\right)\)</span>, es decir:</p>
<p><span class="math display">\[
\begin{aligned}
P_{F}\left\{\mathcal{R}_{n}(\mathbf{X}, F) \leq x\right\}=H_{F}(x) &amp; \doteq H_{F_{n}}(x)=P_{F_{n}}\left\{\mathcal{R}_{n}\left(\mathbf{X}^{*}, F_{n}\right) \leq x\right\} \\
\sigma_{\hat{\theta}}^{2}(F)=\operatorname{var}_{H_{F}}(\hat{\theta}) &amp; \doteq \operatorname{var}_{H_{F_{n}}}(\hat{\theta})=\sigma_{\hat{\theta}}^{2}\left(F_{n}\right)
\end{aligned}
\]</span></p>
<p>La notación <span class="math inline">\(\mathbf{X}^{*}\)</span> hace referencia al hecho de que la muestra proviene de la distribución <span class="math inline">\(F_{n}\)</span> en vez de <span class="math inline">\(F\)</span>.</p>
<ul>
<li><span class="math inline">\(\mathbf{X}^{*}\)</span> recibe el nombre de muestra bootstrap o remuestra.</li>
<li>La distribución <span class="math inline">\(H_{F_{n}}(x)\)</span> se denominará la distribución bootstrap de <span class="math inline">\(\mathcal{R}_{n}(\mathbf{X}, F)\)</span>.</li>
</ul>
<p>El principio bootstrap consistirá en utilizar la distribución bootstrap como base para realizar inferencias sobre <span class="math inline">\(\mathcal{R}_{n}(\mathbf{X}, F)\)</span>.</p>
<p>Esta aproximación resulta muy atractiva de entrada, en tanto que permite prescindir de suposiciones previas -como la normalidad de los datos- para hacer inferencia. Existe sin embargo un problema importante y es que, en la mayoria de los casos de interés, no es posible obtener analíticamente la distribución bootstrap de <span class="math inline">\(\mathcal{R}_{n}(\mathbf{X}, F)\)</span>, con lo que aparentemente, el método bootstrap no tendrá ningún interés práctico.</p>
<p>Resulta sin embargo que, en muchos casos, es posible aproximar de forma, razonablemente buena la distribución bootstrap mediante simulación de Monte Carlo. Esto determina que, en la práctica, la situación haya sido la contraria de la descrita en el parrafo anterior, es decir el método ha adquirido una gran popularidad dada la posibilidad que ofrece de realizar inferencia de forma relativamente automática.</p>
<p>Ejemplo 1 Sea <span class="math inline">\(X\)</span> una variable discreta con distribución de Bernouilli,</p>
<p><span class="math display">\[
X \sim b(1, p), \quad\left\{\begin{array}{l}
P(X=1)=\theta \\
P(X=0)=1-\theta
\end{array}\right.
\]</span></p>
<p>Sea <span class="math inline">\(\theta(F)=P(X=1)\)</span> el parámetro de interés cuyo estimador de substitución es <span class="math inline">\(\widehat{\theta(F)}=\theta\left(F_{n}\right)=\bar{X}\)</span>. Sea</p>
<p><span class="math display">\[
\mathcal{R}_{n}(\mathbf{X}, F)=\bar{X}-\hat{\theta}(F)
\]</span></p>
<p>el error de estimación. Dada una muestra <span class="math inline">\(\mathbf{x}=\left(X_{1}, X_{2}, \ldots, X_{n}\right)\)</span> la probabilidat, condicional, de que en una muestra de <span class="math inline">\(F_{n}, \mathbf{x}^{*}\)</span>, un valor valga 1 será <span class="math inline">\(\bar{x}\)</span> :</p>
<p><span class="math display">\[
P\left(X_{i}^{*}=1\right)=\bar{x} \Leftrightarrow X^{*} \sim b(1, \bar{x}) .
\]</span></p>
<p>Esto permitirá estimar <span class="math inline">\(\mathcal{R}_{n}(\mathbf{X}, F)=\bar{X}-\hat{\theta}(F)\)</span> por</p>
<p><span class="math display">\[
\mathcal{R}_{n}\left(\mathbf{X}^{*}, F_{n}\right)=\bar{X}^{*}-\hat{\theta}\left(F_{n}\right)=\bar{X}^{*}-\bar{x}
\]</span></p>
<p>El cálculo exacto de las características de la distribución bootstrap de <span class="math inline">\(\mathcal{R}_{n}\left(\mathbf{X}^{*}, F_{n}\right)\)</span> puede ahora realizarse, simplemente teniendo en cuenta que, para cualquier remuestra, el valor <span class="math inline">\(\bar{x}\)</span> es fijo: entre las remuestras <span class="math inline">\(\bar{x}\)</span> hace el papel de parámetro que correspondia a <span class="math inline">\(\theta\)</span> entre las muestras. Tenemos pues que, si denominamos <span class="math inline">\(\operatorname{var}_{B}\left(\mathcal{R}^{*}\right)\)</span> a la varianza bajo la distribución bootstrap de <span class="math inline">\(\mathcal{R}_{n}\left(\mathbf{X}^{*}, F_{n}\right)\)</span> :</p>
<p><span class="math display">\[
\begin{aligned}
\operatorname{var}_{H_{n}}\left(\mathcal{R}_{n}\left(\mathbf{X}^{*}, F_{n}\right)\right) &amp; =\operatorname{var}_{B}\left(\mathcal{R}^{*}\right)=\operatorname{var}_{B}\left(\bar{X}^{*}-\bar{x}\right)=\operatorname{var}_{B}\left(\bar{X}^{*}\right)+0= \\
&amp; =n \frac{\bar{x}}{n} \frac{1-\bar{x}}{n}=\frac{\bar{x}(1-\bar{x})}{n}
\end{aligned}
\]</span></p>
<p>El ejemplo anterior refleja una situación, poco habitual, en que es posible obtener el estimador bootstrap de la varianza, analíticamente. Otro ejemplo, correspondiente a la distribución bootstrap de la mediana se encuentra en Efron ([9]). Otra forma de plantear el estudio de la distribución bootstrap consiste en construirla por enumeración de todas las muestras posibles. Excepto en los casos en que el tamaño de la muestra es muy pequeño el problema es difícil de abordar computacionalmente. Holmes ([14]) discute brevemente este problema en <a href="http://www-stat.stanford.edu/~susan/courses/s208/node12.html" class="uri">http://www-stat.stanford.edu/~susan/courses/s208/node12.html</a>.</p>
</div>
<div id="muestreo-bootstrap-de-monte-carlo" class="section level4 hasAnchor" number="14.1.4.2">
<h4><span class="header-section-number">14.1.4.2</span> Muestreo bootstrap de Monte Carlo<a href="métodos-de-computación-intensiva-el-bootstrap.html#muestreo-bootstrap-de-monte-carlo" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Como se ha discutido en la sección anterior la idea básica del bootstrap consiste en estimar el error estándar - u otra característica de la distribución-
a partir de muestras bootstrap de <span class="math inline">\(F_{n}\)</span>, que se obtienen substituyendo <span class="math inline">\(F_{n}\)</span> por <span class="math inline">\(F\)</span> en la etapa de muestreo (en vez de hacerlo en la cálculo de <span class="math inline">\(\sigma_{\hat{\theta}}(F)\)</span> ). Es decir en vez de realizar el muestreo:</p>
<p><span class="math display">\[
F \xrightarrow{\text { m.a.s. }} \mathbf{X}=\left(X_{1}, X_{2}, \ldots, X_{n}\right)
\]</span></p>
<p>se hace</p>
<p><span class="math display">\[
F_{n} \xrightarrow{\text { m.a.s. }} \mathrm{X}^{*}=\left(X_{1}^{*}, X_{2}^{*}, \ldots, X_{n}^{*}\right) .
\]</span></p>
<p>Esto significa que muestreamos extrayendo muestras de tamaño <span class="math inline">\(n\)</span> de <span class="math inline">\(F_{n}\)</span>, es decir que <span class="math inline">\(\mathrm{X}^{*}=\left(X_{1}^{*}, X_{2}^{*}, \ldots, X_{n}^{*}\right)\)</span> es una muestra aleatoria de tamaño <span class="math inline">\(n\)</span> obtenida con reemplazamiento de la muestra original ( <span class="math inline">\(X_{1}, X_{2}, \ldots, X_{n}\)</span> ).</p>
<p>La muestra resultante del muestreo bootstrap, <span class="math inline">\(\mathbf{X}^{*}\)</span>, recibe el nombre de muestra bootstrap o remuestra.</p>
<p>El cálculo del error estándar a partir de las remuestras debe de aproximarse habitualmente mediante un algoritmo de Monte Carlo, dado que no suele conocerse explícitamente la forma de la distribución bootstrap . Este algoritmo consiste en:</p>
<ol style="list-style-type: decimal">
<li>Extraer <span class="math inline">\(B\)</span> muestras, <span class="math inline">\(\mathbf{x}_{1}^{*}, \mathbf{x}_{2}^{*}, \ldots, \mathbf{x}_{B}^{*}\)</span> de <span class="math inline">\(F_{n}\)</span></li>
<li>Calcular <span class="math inline">\(\hat{\theta}\left(\mathrm{x}_{1}^{*}\right), \ldots, \hat{\theta}\left(\mathrm{x}_{B}^{*}\right)\)</span></li>
<li>Sea</li>
</ol>
<p><span class="math display">\[
\overline{\hat{\theta}} \equiv \frac{1}{B} \sum_{b=1}^{B} \hat{\theta}\left(\mathrm{x}_{b}^{*}\right)
\]</span></p>
<p>entonces el el error estándar bootstrap de <span class="math inline">\(\hat{\theta}, \sigma_{B}(\hat{\theta})\)</span> se puede aproximar por:</p>
<p><span class="math display">\[
\hat{\sigma}_{B}(\hat{\theta})=\sqrt{\frac{1}{(B-1)} \sum_{b=1}^{B}\left(\hat{\theta}\left(\mathbf{x}_{\mathbf{i}}^{*}\right)-\overline{\hat{\theta}}\right)^{2}}
\]</span></p>
<p>Cuando</p>
<p><span class="math display">\[
B \rightarrow \infty \quad \Longrightarrow \hat{\sigma}_{B}(\hat{\theta}) \rightarrow \hat{\sigma}_{\infty}(\hat{\theta})=\sigma_{B}(\hat{\theta})=\hat{\sigma}_{\hat{\theta}}\left(F_{n}\right)
\]</span></p>
</div>
</div>
<div id="ejemplo-1-continuación" class="section level3 hasAnchor" number="14.1.5">
<h3><span class="header-section-number">14.1.5</span> Ejemplo 1 (continuación)<a href="métodos-de-computación-intensiva-el-bootstrap.html#ejemplo-1-continuaci%C3%B3n" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Como hemos visto anteriormente el muestreo bootstrap equivale a tomar muestras con reemplazamiento de la muestra original -es decir extraer <span class="math inline">\(n\)</span> valores de la muestra original con probabilidad <span class="math inline">\(1 / n\)</span> para cada uno de ellossobre las que se calcula el estimador del parámetro de interés. En los listados
siguientes <span class="math inline">\({ }^{1}\)</span> se puede ver el aspecto de las primeras remuestras de un muestreo bootstrap con <span class="math inline">\(B=100\)</span> y el valor del parámetro calculado sobre cada una de ellas.</p>
<p>Ejemplo núm 1: Muestra inicial de 9 tiempos de supervivencia y 5 remuestras extraídas de ella.</p>
<pre><code>Muestra original
Datos para calcular el error estándar bootstrap de la mediana.
    0.14 0.18 0.25 0.26 0.37 0.44 0.51 0.61 0.71
El valor del parámetro sobre la muestra original es: 0.37
Remuestra numero: 1
    0.14 0.26 0.37 0.44 0.51 0.51 0.61 0.61 0.71
El valor del parámetro sobre la remuestra n : 1 es: 0.51
Remuestra numero: 2
    0.14 0.18 0.18 0.26 0.37 0.61 0.71 0.71 0.71
El valor del parámetro sobre la remuestra n : 2 es: 0.37
Remuestra numero: 3
    0.18 0.18 0.18 0.25 0.26 0.26 0.26 0.37 0.44
El valor del parámetro sobre la remuestra n : 3 es: 0.26
Remuestra numero: 4
    0.14 0.18 0.18 0.25 0.26 0.44 0.51 0.51 0.71
El valor del parámetro sobre la remuestra n : 4 es: 0.26
Remuestra numero: 5
    0.18 0.26 0.26 0.26 0.37 0.37 0.44 0.51 0.71
El valor del parámetro sobre la remuestra n : 5 es: 0.37</code></pre>
<p>Ejemplo núm 2: Muestra de notas de COU y selectividad de 15 estudiantes de primer curso de universidad</p>
<p>[^0]Un hecho que merece la pena comentar, en tanto que en una primera aproximación puede generar dudas, es como se realiza el remuestreo en variables bidimensionales o <span class="math inline">\(k\)</span>-dimensionales. La respuesta es, evidentemente, que en una muestra de tamaño <span class="math inline">\(n\)</span> deben sacarse <span class="math inline">\(n\)</span> pares, en variables bidimensionales, o <span class="math inline">\(k\)</span>-tuplas, en variables <span class="math inline">\(k\)</span>-dimensionales, con reemplazamiento, manteniendo siempre juntos las coordenadas originales en cada punto.</p>
<pre><code>Muestra inicial Datos para calcular el error estándar bootstrap
del coeficiente de correlaci\&#39;on.
(5.76, 6.78)( 6.35, 6.60)( 5.58, 5.62)( 5.78, 6.06)
( 6.66, 6.88)( 5.80, 6.14)( 5.55, 6.00) ( 6.61, 6.86)
( 6.51, 6.72) ( 6.05, 6.26) ( 6.53, 6.24) ( 5.75, 5.48)
( 5.45, 5.52) ( 5.72, 5.76) ( 5.94, 5.92)
El valor del parámetro sobre la muestra original es: 0.776
Remuestra no 1:(5.55, 6.00)( 6.66, 6.88)( 5.72, 5.76)
( 6.51, 6.72) ( 6.53, 6.24) ( 5.75, 5.48) ( 5.72, 5.76)
(5.78, 6.06)( 5.58, 5.62)( 5.94, 5.92)( 6.35, 6.60)
( 6.05, 6.26)( 5.78, 6.06)( 5.76, 6.78)( 5.80, 6.14)
El valor del parámetro sobre la remuestra n : 1 es: 0.71
Remuestra numero: 2 ( 6.61, 6.86) ( 6.35, 6.60) ( 6.61,
6.86) ( 6.66, 6.88)( 6.61, 6.86)( 5.75, 5.48)( 6.53,
6.24) ( 5.72, 5.76) ( 5.80, 6.14) ( 6.35, 6.60) ( 5.72,
5.76) ( 6.35, 6.60) ( 5.45, 5.52) ( 5.55, 6.00) ( 5.80,
6.14) El valor del parámetro sobre la remuestra no: 2 es: 0.91
Remuestra numero: 3( 5.76, 6.78)( 5.45, 5.52)( 5.58,
5.62) ( 5.45, 5.52) ( 5.80, 6.14) ( 6.61, 6.86) ( 6.35,
6.60) ( 6.66, 6.88)( 5.76, 6.78)( 5.94, 5.92)(5.55,
6.00) ( 5.75, 5.48)( 5.45, 5.52)( 6.05, 6.26)( 5.55,
6.00) El valor del parámetro sobre la remuestra no: 3 es: 0.75</code></pre>
<p>Realizado el remuestreo se obtienen los siguientes resultados:</p>
<ul>
<li>Media muestral
<span class="math inline">\(\hat{\sigma}_{100}=0,156, \quad \sigma_{\infty}=, 155\)</span>
<span class="math inline">\(\left(\hat{\sigma}_{\infty}=\frac{1}{n^{2}}\left[\sum\left(X_{i}-\bar{X}\right)^{2}\right]^{1 / 2}\right)\)</span>
El ajuste entre la estimación bootstrap y el valor teórico es muy bueno.</li>
<li>Mediana muestral
<span class="math inline">\(\hat{\sigma}_{100}=, 106\)</span></li>
<li>Coeficiente de correlación
<span class="math inline">\(\hat{\sigma}_{100}=, 130 \quad \sigma_{\infty}=, 114\)</span>
( <span class="math inline">\(\hat{\sigma}_{\infty}\)</span> representa el error estándar del coeficiente de correlación según la teoría normal).</li>
</ul>
<p>En general para estimar errores estándares basta con <span class="math inline">\(B=100\)</span>. P.ej. en el caso de la mediana del ejemplo anterior se obtiene:</p>
<table>
<thead>
<tr class="header">
<th align="left">B</th>
<th align="center">25</th>
<th align="center">50</th>
<th align="center">100</th>
<th align="center">250</th>
<th align="center">1000</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><span class="math inline">\(\widehat{\sigma_{B}}\)</span></td>
<td align="center">.087</td>
<td align="center">.109</td>
<td align="center">.106</td>
<td align="center">.102</td>
<td align="center">.102</td>
</tr>
</tbody>
</table>
</div>
<div id="resumen-del-mundo-real-al-mundo-bootstrap" class="section level3 hasAnchor" number="14.1.6">
<h3><span class="header-section-number">14.1.6</span> Resumen: del mundo real al mundo bootstrap<a href="métodos-de-computación-intensiva-el-bootstrap.html#resumen-del-mundo-real-al-mundo-bootstrap" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Podemos resumir lo visto hasta aquí en las ideas siguientes:</p>
<ul>
<li>La mayoría de los métodos usuales para obtener errores estándares son versiones aproximadas de:</li>
</ul>
<p><span class="math display">\[
\hat{\sigma}=\sigma\left(F_{n}\right)
\]</span></p>
<ul>
<li>El bootstrap permite evaluar <span class="math inline">\(\sigma\left(F_{n}\right)\)</span> directamente a base de “fuerza bruta” (método de Monte Carlo).</li>
</ul>
<p>Esquemáticamente se puede considerar que al aplicar el bootstrap disponemos de alguna forma de estimación del modelo completo de probabilidad <span class="math inline">\(P=P_{F}\)</span>, a partir de los datos observados <span class="math inline">\(\mathbf{X}\)</span>, y sabemos como producir un modelo estimado <span class="math inline">\(\hat{P}=P_{F_{n}}\)</span>. Este el paso crucial del método.</p>
<p>Una vez disponemos del modelo estimado <span class="math inline">\(\hat{P}\)</span> podemos utilizar el método de Monte Carlo para generar remuestras, <span class="math inline">\(\mathbf{X}^{*}\)</span>, según las mismas reglas por las que los datos originales son generados de <span class="math inline">\(P\)</span>. La variable bootstrap, <span class="math inline">\(\hat{\theta}\left(\mathbf{X}^{*}\right)\)</span>, es observable dado que conocemos <span class="math inline">\(\hat{P}\)</span> y <span class="math inline">\(\mathbf{X}^{*}\)</span> con lo que el muestreo de Monte Carlo nos permitirá conocer la distribución de <span class="math inline">\(\hat{\theta}\left(\mathbf{X}^{*}\right)\)</span>. De esta forma podremos sobre ella estimar las características que nos interesen como, en nuestro ejemplo, el error estándar, que será <span class="math inline">\(\operatorname{Var}_{\hat{P}}\left[\hat{\theta}\left(\mathbf{X}^{*}\right)\right]\)</span>.</p>
<!-- ![](https://cdn.mathpix.com/cropped/836c905e-932d-4fa3-ba7e-9be0ed67ca89-18.jpg?height=330&width=1227&top_left_y=431&top_left_x=484) -->
</div>
<div id="otros-aspectos" class="section level3 hasAnchor" number="14.1.7">
<h3><span class="header-section-number">14.1.7</span> Otros aspectos<a href="métodos-de-computación-intensiva-el-bootstrap.html#otros-aspectos" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="bootstrap-paramétrico-y-no-paramétrico" class="section level4 hasAnchor" number="14.1.7.1">
<h4><span class="header-section-number">14.1.7.1</span> Bootstrap paramétrico y no paramétrico<a href="métodos-de-computación-intensiva-el-bootstrap.html#bootstrap-param%C3%A9trico-y-no-param%C3%A9trico" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>En ocasiones puede disponerse de información sobre la forma de <span class="math inline">\(F\)</span>. Por ejemplo, se puede aceptar razonablemente que el tiempo de vida sigue una distribución exponencial. Si esta suposición se considera válida, la distribución dependerá tan solo del parámetro <span class="math inline">\(\alpha=\left(E_{F} X\right)^{-1}\)</span>.</p>
<p>En vez de estimar <span class="math inline">\(F\)</span> por <span class="math inline">\(F_{n}\)</span> y generar las remuestras a partir de ésta, un procedimiento alternativo consiste en estimar el parámetro -p.ej. mediante su estimador máximo-verosímil, llamémosle <span class="math inline">\(\hat{\alpha}_{M V}-\)</span> y obtener las remuestras generando muestras de una distribución exponencial de parámetro <span class="math inline">\(\hat{\alpha}_{M V}\)</span>.</p>
<p>Es decir en vez de remuestrear aproximando <span class="math inline">\(F=F_{\alpha}\)</span> por <span class="math inline">\(F_{n}\)</span> y extrayendo muestras con reemplazamiento de la muestra original:</p>
<p><span class="math display">\[
F_{\alpha} \simeq F_{n} \quad \xrightarrow{\text { m.a.s. }} \mathbf{X}^{*}=\left(X_{1}^{*}, X_{2}^{*}, \ldots, X_{n}^{*}\right) .
\]</span></p>
<p>lo haremos aproximando <span class="math inline">\(F_{\alpha}\)</span> por <span class="math inline">\(F_{\hat{\alpha}_{M V}}\)</span> y generando mediante métodos de Monte Carlo, muestras a partir de esta distribución.</p>
<p><span class="math display">\[
F_{\alpha} \simeq F_{\hat{\alpha}_{M V}} \quad \xrightarrow{\text { m.a.s. }} \quad \mathbf{Y}^{*}=\left(Y_{1}^{*}, Y_{2}^{*}, \ldots, Y_{n}^{*}\right)
\]</span></p>
<p>Este procedimiento se conoce como Bootstrap paramétrico. Una diferencia obvia con el procedimiento anterior es que, en general, los elementos de la remuestra no pertenecerán a la muestra original.</p>
<p>Ejemplo 1 Si en el ejemplo número 1, en donde considerábamos los tiempos de supervivencia, realizamos la suposición, razonable, de que éstos se distribuyen según una distribución exponencial</p>
<p><span class="math display">\[
f(x ; \alpha)=\alpha \exp (-\alpha x), x&gt;0, \alpha&gt;0
\]</span></p>
<p>podemos utilizar el estimador <span class="math inline">\(F_{\alpha} \simeq F_{\hat{\alpha}_{M V}}\)</span>, en donde</p>
<p><span class="math display">\[
\hat{\alpha}_{M V}=\frac{1}{\bar{x}}
\]</span></p>
<p>Para generar muestras según la distribución exponencial nos podemos basar en el procedimiento de inversión que, aplicado a la función de distribución de la ley exponencial, garantiza que la variable</p>
<p><span class="math display">\[
X=-\frac{1}{\alpha} \ln (1-U), \quad U \sim \operatorname{Unif.}(0,1),
\]</span></p>
<p>sigue una distribución exponencial <span class="math inline">\({ }^{2}\)</span>.</p>
</div>
<div id="numero-de-remuestras-necesario" class="section level4 hasAnchor" number="14.1.7.2">
<h4><span class="header-section-number">14.1.7.2</span> Numero de remuestras necesario<a href="métodos-de-computación-intensiva-el-bootstrap.html#numero-de-remuestras-necesario" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Como ya hemos comentado suele ser necesario recurrir a algún tipo de muestreo de Monte Carlo para evaluar los estimadores bootstrap. Una pregunta de inminente interés práctico es cuantas remuestras deben realizarse para que la aproximación de Monte Carlo del estimador bootstrap se aproxime “razonablemente” al auténtico valor del estimador bootstrap. Si llamamos:</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(\sigma_{F}(\hat{\theta})\)</span> al error estándar de <span class="math inline">\(\hat{\theta}\)</span>,</li>
<li><span class="math inline">\(\sigma_{\hat{F}}(\hat{\theta})=\sigma_{B}(\hat{\theta})\)</span> al estimador bootstrap del error estándar de <span class="math inline">\(\hat{\theta}\)</span>, y</li>
<li><span class="math inline">\(\hat{\sigma}_{\hat{F}}(\hat{\theta})=\hat{\sigma}_{B}(\hat{\theta})\)</span> a la aproximación de Monte Carlo del estimador bootstrap del error estándar de <span class="math inline">\(\hat{\theta}\)</span>,
la pregunta relevante es cuan mayor es el error cometido para estimar <span class="math inline">\(\sigma_{F}(\hat{\theta})\)</span> si, en vez de basarnos en <span class="math inline">\(\sigma_{B}(\hat{\theta})\)</span> lo hacemos en <span class="math inline">\(\hat{\sigma}_{B}(\hat{\theta})\)</span> ?</li>
</ol>
<p>Efron ([11])deduce una fórmula para el coeficiente de variación (es decir el cociente entre la desviación típica y la esperanza) de <span class="math inline">\(\hat{\sigma}_{B}(\hat{\theta})\)</span>, condicional sobre una muestra dada:</p>
<p><span class="math display">\[
C V\left(\hat{\sigma}_{B}(\hat{\theta}) \mid \mathbf{x}\right)=\left[\frac{\hat{\Delta}+2}{4 B}\right]^{1 / 2}
\]</span></p>
<p>donde <span class="math inline">\(\hat{\Delta}\)</span> es la curtosis de la distribución (bootstrap) de <span class="math inline">\(\hat{\theta}\)</span>. La notación indica que los datos observados, <span class="math inline">\(\mathbf{x}\)</span>, se consideran fijos en esta expresión. Cuando <span class="math inline">\(B \rightarrow \infty\)</span> la expresión anterior <span class="math inline">\(\rightarrow 0\)</span> y <span class="math inline">\(\sigma_{B} \rightarrow \hat{\sigma}\)</span> el estimador bootstrap del error estándar, “ideal”. Sean ahora <span class="math inline">\(C V\left(\sigma_{B}(\hat{\theta})\right)\)</span> y <span class="math inline">\(C V\left(\hat{\sigma}_{B}(\hat{\theta})\right)\)</span> los coeficientes de variación incondicionales (es decir los CV condicionales promediados sobre</p>
<p>[^1]todas las posibles muestras) de <span class="math inline">\(\sigma_{B}(\hat{\theta})\)</span> y <span class="math inline">\(\hat{\sigma}_{B}(\hat{\theta})\)</span> respectivamente. En este caso la relación entre ambos coeficientes de variación viene dada por la expresión:
<span class="math display">\[
C V\left(\hat{\sigma}_{B}(\hat{\theta})\right) \doteq\left\{\left[C V\left(\sigma_{B}\right)\right]^{2}+\left[\frac{E[\hat{\Delta}]+2}{4 B}\right]^{1 / 2}\right\}
\]</span></p>
<p>La tabla siguiente muestra <span class="math inline">\(C V\left(\hat{\sigma}_{B}\right)\)</span> para diversos valores de <span class="math inline">\(B\)</span> y <span class="math inline">\(C V\left(\sigma_{B}\right)\)</span> suponiendo que sea <span class="math inline">\(E[\hat{\Delta}]=0\)</span>. Valores de <span class="math inline">\(C V\left(\sigma_{B}\right) \geq 0,10\)</span>, lo que, según Efron [11], es habitual en la práctica, se observa poca mejora a partir de <span class="math inline">\(B=100\)</span>, lo que sugiere que, para estimar el error estándar este puede ser un número de remuestras adecuado.</p>
<table>
<thead>
<tr class="header">
<th align="center"></th>
<th align="center"></th>
<th align="center"></th>
<th align="center">B</th>
<th align="center"></th>
<th align="center"></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center"><span class="math inline">\(C V(\hat{\sigma})\)</span></td>
<td align="center">25</td>
<td align="center">50</td>
<td align="center">100</td>
<td align="center">200</td>
<td align="center"><span class="math inline">\(\infty\)</span></td>
</tr>
<tr class="even">
<td align="center">.25</td>
<td align="center">.29</td>
<td align="center">.27</td>
<td align="center">.26</td>
<td align="center">.25</td>
<td align="center">.25</td>
</tr>
<tr class="odd">
<td align="center">.20</td>
<td align="center">.24</td>
<td align="center">.22</td>
<td align="center">.21</td>
<td align="center">.21</td>
<td align="center">.20</td>
</tr>
<tr class="even">
<td align="center">.15</td>
<td align="center">.21</td>
<td align="center">.18</td>
<td align="center">.17</td>
<td align="center">.16</td>
<td align="center">.15</td>
</tr>
<tr class="odd">
<td align="center">.10</td>
<td align="center">.17</td>
<td align="center">.14</td>
<td align="center">.12</td>
<td align="center">.11</td>
<td align="center">.10</td>
</tr>
<tr class="even">
<td align="center">.05</td>
<td align="center">.15</td>
<td align="center">.11</td>
<td align="center">.09</td>
<td align="center">.07</td>
<td align="center">.05</td>
</tr>
<tr class="odd">
<td align="center">0</td>
<td align="center">.14</td>
<td align="center">.10</td>
<td align="center">.07</td>
<td align="center">.05</td>
<td align="center">0</td>
</tr>
</tbody>
</table>
<p>Razonamientos similares sugieren que para los intervalos de confianza el número de remuestras debería ser mucho mayor, del orden de 1000 o 2000.</p>
<p>En un artículo de Booth y Sarkar ([3]) se critica el razonamiento anterior argumentando que es preciso controlar la variabilidad derivada del remuestreo de Monte Carlo, para evitar que las conclusiones del uso del bootstrap difieran entre dos repeticiones del mismo cálculo como resultado de factores como las semillas de los generadores de números aleatorios.</p>
</div>
</div>
<div id="ejercicios" class="section level3 hasAnchor" number="14.1.8">
<h3><span class="header-section-number">14.1.8</span> Ejercicios<a href="métodos-de-computación-intensiva-el-bootstrap.html#ejercicios" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ol style="list-style-type: decimal">
<li>Sea <span class="math inline">\(x_{(1)}&lt;x_{(2)}&lt;\ldots&lt;x_{(7)}\)</span> una muestra ordenada de tamaño <span class="math inline">\(n=7\)</span>. Sea <span class="math inline">\(\mathbf{x}^{*}\)</span> una muestra bootstrap y <span class="math inline">\(s\left(\mathbf{x}^{*}\right)\)</span> la correspondiente replica bootstrap de la mediana. Muestre que:</li>
</ol>
<ol style="list-style-type: lower-alpha">
<li><span class="math inline">\(s\left(\mathbf{x}^{*}\right)\)</span> coincide con uno de los valores originales de la muestra <span class="math inline">\(x_{(i)}\)</span>, <span class="math inline">\(i=1,2, \ldots 7\)</span>.</li>
<li><span class="math inline">\(P\left[s(\mathbf{x})=x_{(i)}\right]=p(i)\)</span> donde</li>
</ol>
<p><span class="math display">\[
p(i)=\sum_{j=0}^{3}\left\{\operatorname{Bi}\left(j ; n, \frac{i-1}{n}\right)-\operatorname{Bi}\left(j ; n, \frac{i}{n}\right)\right\},
\]</span></p>
<p>donde <span class="math inline">\(\operatorname{Bi}(j ; n, p)\)</span> es la probabilidad binomial <span class="math inline">\(\binom{n}{j} p^{j}(1-p)^{n-j}\)</span>. (Los valores numéricos de <span class="math inline">\(p(i)\)</span> son <span class="math inline">\(.0102, .0981, .2386, .3062, .2386\)</span>, .0981, .0102.) Estos valores se utilizaron para calcular un error estándar <span class="math inline">\(\hat{\sigma}_{B=\infty}\)</span> (mediana) <span class="math inline">\(=37,83\)</span> con el grupo tratamiento con los datos de supervivencia de <span class="math inline">\(9+9\)</span> ratones (“mouse data set”).
2. Aplique la ley débil de los grandes números para demostrar que, si <span class="math inline">\(s\left(\mathbf{x}^{*}\right)\)</span> es la media muestral <span class="math inline">\(\bar{x}\)</span> el estimador bootstrap del error estándar</p>
<p><span class="math display">\[
\hat{\sigma}_{B}=\left\{\frac{1}{(B-1)} \sum_{b=1}^{B}\left[s\left(\mathbf{x}_{b}^{*}\right)-s(\cdot)\right]^{2}\right\}^{\frac{1}{2}}
\]</span></p>
<p>siendo <span class="math inline">\(s(\cdot)=\sum_{b=1}^{B} s\left(\mathbf{x}^{*}{ }_{b}\right) / B\)</span>, tiende hacia</p>
<p><span class="math display">\[
\left\{\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2} / n^{2}\right\}^{\frac{1}{2}},
\]</span></p>
<p>cuando <span class="math inline">\(n\)</span> tiende a infinito.
3. Tomamos una muestra aleatoria de tamaño <span class="math inline">\(n\)</span> con reemplazamiento de una población de tamaño <span class="math inline">\(N\)</span>. Muestre que la probabilidad de que no aparezcan repeticiones en la muestra se obtiene del producto:</p>
<p><span class="math display">\[
\prod_{j=0}^{n-1}\left(1-\frac{j}{N}\right) .
\]</span></p>
<ol start="4" style="list-style-type: decimal">
<li>Sean <span class="math inline">\(\bar{X}\)</span> y <span class="math inline">\(S\)</span> la media y la desviación típica de un conjunto de <span class="math inline">\(N\)</span> valores <span class="math inline">\(X_{1}, X_{2}, \ldots, X_{N}\)</span> :</li>
</ol>
<p><span class="math display">\[
\bar{X}=\sum_{i=1}^{N} X_{i}, \quad S=\left\{\sum_{i=1}^{N}\left(X_{i}-\bar{X}\right)^{2} / N\right\}^{\frac{1}{2}} .
\]</span></p>
<ol style="list-style-type: lower-alpha">
<li>Extraemos una muestra <span class="math inline">\(\left(x_{1}, x_{2}, \ldots, x_{n}\right)\)</span> de <span class="math inline">\(X_{1}, \ldots, X_{N}\)</span> con reemplazamiento. La desviación típica de la media muestral, <span class="math inline">\(\bar{x}=\sum_{i=1}^{n} x_{i}\)</span>, se indica como <span class="math inline">\(\sigma_{\bar{x}}, \mathrm{y}\)</span> suele denominarse el error estándar de la media. Demuestre que:</li>
</ol>
<p><span class="math display">\[
\sigma_{\bar{x}}=\frac{S}{\sqrt{n}}
\]</span></p>
<ol start="2" style="list-style-type: lower-alpha">
<li>Supongamos que el muestreo se realiza sin reemplazamiento (es decir es preciso que <span class="math inline">\(n \leq N\)</span>. Demuestre que:</li>
</ol>
<p><span class="math display">\[
\sigma_{\bar{x}}=\frac{S}{\sqrt{n}}\left[\frac{N-n}{N-1}\right]^{\frac{1}{2}}
\]</span></p>
<ol start="3" style="list-style-type: lower-alpha">
<li>Como puede verse el muestreo sin reemplazamiento da lugar a un menor error para <span class="math inline">\(\bar{x}\)</span>. Proporcionalmente qué tan menor será en el caso de los datos de las escuelas norteamericanas de abogacía <span class="math inline">\((N=84, n=15)\)</span> ?.</li>
</ol>
<ol start="5" style="list-style-type: decimal">
<li>Dado un conjunto de <span class="math inline">\(n\)</span> valores distintos pruebe que el número de muestras bootstrap distintas es:</li>
</ol>
<p><span class="math display">\[
\binom{2 n-1}{n .}
\]</span></p>
<p>Cuantas son para <span class="math inline">\(n=15 ?\)</span>.</p>
</div>
<div id="practicas" class="section level3 hasAnchor" number="14.1.9">
<h3><span class="header-section-number">14.1.9</span> Practicas<a href="métodos-de-computación-intensiva-el-bootstrap.html#practicas" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ol style="list-style-type: decimal">
<li>Es posible, aunque poco práctico implementar el bootstrap por uno mismo en cualquier lenguaje de ordenador.</li>
</ol>
<ol style="list-style-type: lower-alpha">
<li>Las instrucciones contenidas en BOOTSD1. R y BOOTSD1B. <span class="math inline">\(\mathrm{R}^{3}\)</span> en realizan remuestreo no paramétrico sobre la media de tiempo de supervivencia de 9 ratones asignados a un tratamiento destinado a prolongar la supervivencia después de una intervención quirúrgica. El resultado se compara con el error estándar de la media.</li>
<li>Las instrucciones contenidas en BOOTSD2.R realizan remuestreo no paramétrico sobre la mediana del tiempo de supervivencia de 9 ratones asignados a un tratamiento destinado a prolongar la supervivencia después de una intervención quirúrgica. El resultado se compara con el error estándar teórico de la mediana,</li>
</ol>
<p><span class="math display">\[
\sqrt{\operatorname{var}(\widehat{M})}=\sqrt{\frac{1}{4 n \cdot[f(\widehat{M})]^{2}}},
\]</span></p>
<p>donde <span class="math inline">\(f()\)</span> es la función de densidad evaluada en la mediana de la muestra, <span class="math inline">\(\widehat{M} . f()\)</span> se estima mediante un estimador kernel.
c) Las instrucciones contenidas en BOOTSD3. R y BOOTSD3B. R realizan remuestreo no paramétrico sobre el coeficiente de correlación de las notas de COU y selectividad de 15 estudiantes. En este caso no disponemos, en general, de una fórmula teórica con la que comparar y, en todo caso la varianza auténtica debe estimarse por simulación.
2. En ocasiones se dispone de alguna hipótesis razonable sobre la distribución de los datos, lo cual sugiere la utilización de un remuestreo paramétrico. Modificar el mecanismo de remuestreo del ejercicio anterior para que las remuestras del tiempo de supervivencia se realicen mediante la generación de muestras de tamaño <span class="math inline">\(\mathrm{n}=9\)</span> de una distribución exponencial de parámetro <span class="math inline">\(\alpha=\bar{x}\)</span>. Comparar los resultados obtenidos con los del remuestreo no paramétrico.
3. El jackknife ofrece un procedimiento alternativo al bootstrap para estimar la varianza de un estimador. Modifique las rutinas del ejercicio 1</p>
<p>[^2]para calcular también el estimador jackknife de la varianza de los estimadores. Compárelo con el estimador bootstrap. (Solución JACKSD1.R )
4. El bootstrap, junto con el jackknife pueden utilizarse para estimar el sesgo y para obtener estimadores corregidos para el sesgo.A partir de la siguiente muestra obtenida mediante un generador de números aleatorios <span class="math inline">\(\mathrm{N}(0,1)\)</span>
<span class="math display">\[
\begin{aligned}
\mathbf{X}= &amp; 0,3996,1,5074,0,5640,-1,3542,-0,5846,0,4271,-0,7518 \\
&amp; 0,4622,-0,4563,-0,0783
\end{aligned}
\]</span>
a) Calcular el sesgo bootstrap y jackknife del estimador de la varianza muestral y compararlo con el estimador exacto <span class="math inline">\(\frac{-1}{n} \sigma^{2}\)</span>, que en este ejemplo es conocido al proceder los datos de una distribución simulada.
b) Repetir varias veces los cálculos y observe como varían las estimaciones. ¿Como explica la variación observada?.
c) ¿Como se modifica la variabilidad de las estimaciones del sesgo al aumentar el número de remuestras?. Podemos estimar esta variabilidad mediante simulación o mediante jackknife after bootstrap
5. Si no se dispone de un “patron-oro” con el que comparar las estimacionesbootstrap, jackknife u otras- del error estándar pueden validarse estos métodos por simulación. Por ejemplo, para realizar esta validación en el caso del error estándar bootstrap del coeficiente de correlación ejecutaremos el siguiente algoritmo:
a) Repetir un elevado número de veces <span class="math inline">\(s=1, \ldots, S\)</span> (p.ej. <span class="math inline">\(S=10000\)</span> ) el siguiente proceso:</p>
<ol style="list-style-type: decimal">
<li>Generar una muestra de una distribución bivariante, p.ej. una normal, <span class="math inline">\(N(\mu, \boldsymbol{\Sigma})\)</span>.</li>
<li>Calcular sobre ella el coeficiente de correlación <span class="math inline">\(\hat{\rho}_{s}\)</span>, y el error estándar bootstrap <span class="math inline">\(\hat{\sigma}_{B}\)</span></li>
</ol>
<ol start="2" style="list-style-type: lower-alpha">
<li>La desviación típica muestral, <span class="math inline">\(s_{\hat{\rho}}\)</span> de los coeficientes de correlación <span class="math inline">\(\hat{\rho}_{1}, \ldots, \hat{\rho}_{s}\)</span> es una buena estimación del error estándar de <span class="math inline">\(\hat{\rho}\)</span> y la media muestral de los errores estándar bootstrap, <span class="math inline">\(\overline{\hat{\sigma}}_{B}\)</span> es una buena estimación del error estándar bootstrap del coeficiente de correlación.
<span class="math inline">\(c)\)</span> Cuanto más similares sean <span class="math inline">\(s_{\hat{\rho}}\)</span> y <span class="math inline">\(\overline{\hat{\sigma}}_{B}\)</span> mejor será la calidad de <span class="math inline">\(\hat{\sigma}_{B}\)</span> cómo estimador de <span class="math inline">\(\sigma_{\hat{\rho}}\)</span>.</li>
</ol>
</div>
</div>
<div id="estimación-y-corrección-del-sesgo-de-un-estimador" class="section level2 hasAnchor" number="14.2">
<h2><span class="header-section-number">14.2</span> Estimación y corrección del sesgo de un estimador<a href="métodos-de-computación-intensiva-el-bootstrap.html#estimaci%C3%B3n-y-correcci%C3%B3n-del-sesgo-de-un-estimador" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="introducción-10" class="section level3 hasAnchor" number="14.2.1">
<h3><span class="header-section-number">14.2.1</span> Introducción<a href="métodos-de-computación-intensiva-el-bootstrap.html#introducci%C3%B3n-10" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Supongamos que tenemos un modelo estadístico paramétrico, <span class="math inline">\(X \sim F_{\theta}\)</span>, una muestra aleatoria simple de <span class="math inline">\(X\)</span>,</p>
<p><span class="math display">\[
\mathbf{X}=X_{1}, X_{2}, \ldots, X_{n}
\]</span></p>
<p>y un estimador <span class="math inline">\(\hat{\theta}=T\left(X_{1}, X_{2}, \ldots, X_{n}\right)\)</span> del parámetro. Una forma razonable de valorar qué tan próximos son los valores de <span class="math inline">\(\hat{\theta}\)</span> de los de <span class="math inline">\(\theta\)</span> es ver si, en promedio, los valores de <span class="math inline">\(\hat{\theta}\)</span> coinciden con el valor medio de <span class="math inline">\(\theta\)</span>.</p>
<p>Bajo las condiciones anteriores, si <span class="math inline">\(E_{F}(T(\mathbf{X}))\)</span> representa la esperanza de <span class="math inline">\(\hat{\theta}=T\left(X_{1}, X_{2}, \ldots, X_{n}\right)\)</span> la diferencia</p>
<p><span class="math display">\[
b_{F}(T)=E_{F}(T(\mathbf{X}))-\theta
\]</span></p>
<p>recibe el nombre de sesgo del estimador <span class="math inline">\(\hat{\theta}\)</span> para estimar <span class="math inline">\(\theta\)</span>. Si el sesgo vale cero, es decir si:</p>
<p><span class="math display">\[
\theta=E_{F}(T(\mathbf{X})), \forall \theta \in \Theta
\]</span></p>
<p>diremos que <span class="math inline">\(\hat{\theta}\)</span> es un estimador insesgado de <span class="math inline">\(\theta\)</span>.
Ejemplo 1 Los dos ejemplos mas habituales son la media y la varianza muestrales</p>
<ul>
<li>La media muestral es un estimador sin sesgo de <span class="math inline">\(\mu\)</span>.</li>
<li>La varianza muestral es un estimador sesgado de la varianza poblacional. En concreto el sesgo vale <span class="math inline">\(\frac{-1}{n} \sigma^{2}\)</span>.</li>
</ul>
<p>El uso de estimadores sin sesgo es conveniente en muestras de tamaño grande puesto que, en éstas, <span class="math inline">\(\operatorname{Var}_{\theta}(\hat{\theta})\)</span> es a menudo pequeña y entonces si <span class="math inline">\(E_{F}(\hat{\theta})=\theta+b_{F}(\theta)\)</span> es muy probable obtener estimaciones centradas en este valor en vez de en el entorno de <span class="math inline">\(\theta\)</span>.</p>
<p>Ejemplo 2 Sea <span class="math inline">\(X_{1}, X_{2}, \ldots, X_{n}\)</span> una muestra aleatoria simple de <span class="math inline">\(X \sim U(0, \theta)\)</span>. Sea <span class="math inline">\(T_{1}=\max_{1 \leq i \leq n}\left(X_{i}\right)\)</span> el estimador del máximo de la distribución. Obviamente podemos decir que <span class="math inline">\(T_{1}&lt;\theta\)</span> y por tanto la estimación es siempre sesgada. La distribución en el muestreo de <span class="math inline">\(T_{1}\)</span> es</p>
<p><span class="math display">\[
H_{\theta}(\theta)=P_{\theta}\left[T_{1} \leq t\right]=\left(\frac{t}{\theta}\right)^{n}
\]</span></p>
<p><span class="math inline">\(y\)</span> su función de densidad</p>
<p><span class="math display">\[
f_{\theta}(\theta)=H_{\theta}^{\prime}(\theta)=\frac{n}{\theta}\left(\frac{t}{\theta}\right)^{n-1}
\]</span></p>
<p>El valor medio de vale:</p>
<p><span class="math display">\[
E_{F}(t)=\int_{0}^{\theta} t \cdot\left[\frac{n}{\theta}\left(\frac{t}{\theta}\right)^{n-1}\right] d t=\left.\frac{n}{\theta^{n}} \frac{t^{n+1}}{n+1}\right|_{0} ^{\theta}=\frac{n}{n+1} \frac{\theta^{n+1}}{\theta^{n}}=\frac{n}{n+1} \theta
\]</span></p>
<p>de donde el sesgo de <span class="math inline">\(T_{1}\)</span> para estimar <span class="math inline">\(\theta\)</span> vale</p>
<p><span class="math display">\[
b_{F}(\theta)=\frac{n}{n+1} \theta-\theta=-\frac{1}{n+1} \theta
\]</span></p>
<p>Podemos preguntarnos sino podríamos mejorar este estimador corrigiendo el sesgo de forma análoga a como se suele hacer con <span class="math inline">\(\hat{S}^{2} y\)</span> así utilizar el estimador <span class="math inline">\(T_{1}^{\prime}=\frac{n+1}{n} T_{1}\)</span> que verifica <span class="math inline">\(E\left(T_{1}^{\prime}\right)=\theta\)</span>. Si buscamos el estimador de mínimo, riesgo en el sentido del error cuadrático medio, es decir el estimador que minimiza <span class="math inline">\(E\left[(\theta-T)^{2}\right]\)</span> obtenemos</p>
<p><span class="math display">\[
T_{k}=\frac{n+2}{n+1} T_{1}
\]</span></p>
<p>que es preferible a <span class="math inline">\(T_{1}\)</span> pero no es insesgado puesto que:</p>
<p><span class="math display">\[
E_{\theta}\left(T^{\prime \prime}\right)=\frac{n+2}{n+1} E_{\theta}\left(T_{1}\right)=\frac{(n+2) n}{(n+1)^{2}} \theta
\]</span></p>
<p>El ejemplo anterior muestra que debido a la descomposición</p>
<p><span class="math display">\[
E Q M_{F}(T)=\operatorname{Var}_{F}(T)+b_{F}^{2}(T)
\]</span></p>
<p>puede ser preferible un estimador sesgado a uno insesgado.
En general, sin embargo, eliminar el sesgo no es una mala estrategia. Esto viene avalado por la teoría de la estimación sin sesgo en donde restringiendo la clase de los estimadores a los estimadores insesgados se obtiene una solución constructiva que permite obtener estimadores insesgados de varianza mínima en condiciones bastante generales. En el contexto actual dicha teoría no suele tener aplicación por lo que vamos considerar formas de reducir el sesgo mediante aplicación de los métodos de remuestreo.</p>
</div>
<div id="estimación-bootstrap-del-sesgo" class="section level3 hasAnchor" number="14.2.2">
<h3><span class="header-section-number">14.2.2</span> Estimación bootstrap del sesgo<a href="métodos-de-computación-intensiva-el-bootstrap.html#estimaci%C3%B3n-bootstrap-del-sesgo" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Si en la definición del sesgo</p>
<p><span class="math display">\[
b_{F}(T)=E_{F}(T(\mathbf{X}))-\theta(F)
\]</span></p>
<p>en donde <span class="math inline">\(\theta(F)\)</span> indica la dependencia del parámetro respecto de la distribución <span class="math inline">\(F\)</span>, substituimos <span class="math inline">\(F\)</span> por una estimación <span class="math inline">\(\hat{F}\)</span>, obtendremos el estimador bootstrap del sesgo:</p>
<p><span class="math display">\[
b_{\hat{F}}(T)=E_{\hat{F}}\left(T\left(\mathbf{X}^{*}\right)\right)-\theta(\hat{F})
\]</span></p>
<p>Algunas observaciones importantes sobre esta definición son las siguientes:</p>
<ul>
<li>La substitución de <span class="math inline">\(F\)</span> por <span class="math inline">\(\hat{F}\)</span> se realiza en dos puntos: <span class="math inline">\(E_{F}(T(\mathbf{X}))\)</span> y en <span class="math inline">\(\theta(F)\)</span></li>
<li>Definiendo así el estimador bootstrap del sesgo no es preciso que <span class="math inline">\(T(\mathbf{X})\)</span> sea un estimador de substitución pero si debe serlo <span class="math inline">\(\theta(\hat{F})\)</span>.</li>
</ul>
<p>En ocasiones será posible calcular directamente <span class="math inline">\(b_{\hat{F}}(T)\)</span>, siempre que sepamos cómo calcular <span class="math inline">\(E_{\hat{F}}\left(T\left(\mathbf{X}^{*}\right)\right)\)</span> y <span class="math inline">\(\theta(\hat{F})\)</span>, pero en general será preciso aproximarlos por remuestreo de Monte Carlo mediante los mismos algoritmos de remuestreo que hemos visto en el capítulo anterior. Veamos algunos ejemplos en los que es posible el cálculo exacto de <span class="math inline">\(b_{\hat{F}}(T)\)</span>.</p>
<p>Ejemplo 1 Si <span class="math inline">\(\theta(F)\)</span> es la media de la distribución, <span class="math inline">\(\theta(F)=\int X d F, y T(\mathbf{X})\)</span> es la media muestral entonces</p>
<p><span class="math display">\[
\begin{aligned}
b_{\hat{F}}(T) &amp; =E_{\hat{F}}\left(T\left(\mathbf{X}^{*}\right)\right)-\theta(\hat{F})=E_{\hat{F}}\left(\overline{\mathbf{X}^{*}}\right)-\bar{X} \\
&amp; =E_{\hat{F}}\left(\frac{1}{n} \sum X_{i}^{*}\right)-\bar{X}=\bar{X}-\bar{X}=0
\end{aligned}
\]</span></p>
<p>es decir el estimador bootstrap del sesgo es cero coincidiendo con el sesgo auténtico de <span class="math inline">\(\bar{X}\)</span> para estimar <span class="math inline">\(\mu\)</span>.</p>
<p>Ejemplo 2 Si <span class="math inline">\(\theta(F)\)</span> es la varianza de la distribución, <span class="math inline">\(\theta(F)=\int\left(X-\int X d F\right)^{2} d F\)</span>, y <span class="math inline">\(T(\mathbf{X})\)</span> es la varianza muestral entonces es fácil ver que</p>
<p><span class="math display">\[
\begin{array}{r}
b_{F}(T)=-\frac{1}{n} \sigma^{2} y \\
b_{\hat{F}}(T)=-\frac{1}{n^{2}} \sum\left(X_{i}-\bar{X}\right)^{2}
\end{array}
\]</span></p>
<p>Como hemos indicado estos y otros casos sencillos suelen ser la excepción. En general deberemos recorrer a remuestreo de Monte Carlo para estimar <span class="math inline">\(b_{\hat{F}}(T)\)</span>. El algoritmo es muy similar al utilizado para estimar el error estándar:</p>
<ol style="list-style-type: decimal">
<li>Extraer <span class="math inline">\(B\)</span> muestras, <span class="math inline">\(\mathbf{X}_{1}^{*}, \mathbf{X}_{2}^{*}, \ldots, \mathbf{X}_{B}^{*}\)</span> de <span class="math inline">\(F_{n}\)</span></li>
<li>Calcular <span class="math inline">\(T\left(\mathbf{X}_{1}^{*}\right), \ldots, T\left(\mathbf{X}_{B}^{*}\right)\)</span></li>
<li>Sea</li>
</ol>
<p><span class="math display">\[
\overline{\hat{\theta}^{*}} \equiv \frac{1}{B} \sum_{b=1}^{B} \hat{\theta}\left(\mathbf{X}_{b}^{*}\right)
\]</span></p>
<p>entonces el estimador bootstrap del sesgo, <span class="math inline">\(b_{\hat{F}}(T)\)</span> se puede aproximar por:</p>
<p><span class="math display">\[
\widehat{b_{\hat{F}}(T)}=\overline{\hat{\theta}^{*}}-T(\mathbf{X})
\]</span></p>
</div>
<div id="ejemplo-1.-estimación-del-sesgo-del-estimador-de-la-varianza" class="section level3 hasAnchor" number="14.2.3">
<h3><span class="header-section-number">14.2.3</span> Ejemplo 1. Estimación del sesgo del estimador de la varianza<a href="métodos-de-computación-intensiva-el-bootstrap.html#ejemplo-1.-estimaci%C3%B3n-del-sesgo-del-estimador-de-la-varianza" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Supongamos que tomamos una muestra de una población normal de media <span class="math inline">\(\mu=0\)</span> y varianza conocida <span class="math inline">\(\sigma^{2}=1\)</span>. El estimador de substitución de la varianza es</p>
<p><span class="math display">\[
T(\mathbf{X})=S^{2}=\frac{1}{n} \sum\left(X_{i}-\bar{X}\right)^{2}
\]</span></p>
<p>En este caso podemos calcular el sesgo exacto y el estimador bootstrap del sesgo. Si a continuación calculamos el estimador bootstrap aproximado del sesgo podremos compararlo con los anteriores.</p>
<p>Una muestra de tamaño <span class="math inline">\(n=10\)</span> de dicha distribución, obtenida mediante un generador de números aleatorios, es la siguiente</p>
<p><span class="math display">\[
\begin{gathered}
\mathbf{X}=0,3996,1,5074,0,5640,-1,3542,-0,5846,0,4271,-0,7518, \\
0,4622,-0,4563,-0,0783 .
\end{gathered}
\]</span></p>
<p>La varianza muestral estimada sobre la muestra vale:</p>
<p><span class="math display">\[
S^{2}(\mathbf{X})=0,60971 .
\]</span></p>
<p>El el sesgo exacto y el estimador bootstrap del sesgo valen respectivamente:</p>
<p><span class="math display">\[
\begin{aligned}
b_{F}(T) &amp; =-\frac{1}{n} \sigma^{2}=-0.1 \times 1=0.1 \\
b_{\hat{F}}(T) &amp; =-\frac{1}{n^{2}} \sum\left(X_{i}-\bar{X}\right)^{2}=-\frac{0,60971}{10}=-0,060971 .
\end{aligned}
\]</span></p>
<p>Para obtener la aproximación de Monte Carlo realizamos 100 remuestras, estimamos <span class="math inline">\(S^{2}\)</span> sobre cada una de ellas y calculamos el promedio de las 100 estimaciones. El estimador bootstrap aproximado del sesgo vale</p>
<p><span class="math display">\[
\begin{aligned}
\widehat{b_{\hat{F}}\left(S^{2}\right)} &amp; =\overline{S^{2 *}}-S^{2}(\mathbf{X})= \\
&amp; =0,57060-0,60971=-0,03911
\end{aligned}
\]</span></p>
<p>Es importante observar que este valor se halla influido por el remuestreo de Monte Carlo. Otras extracciones de 100 remuestras dan lugar a valores distintos del sesgo que pueden oscilar considerablemente desde -0.01 hasta 0.1. Esto sugiere la necesidad de realizar un remuestreo con un mayor número de remuestras. Un estudio de simulación en donde se repite 100 veces el remuestreo con <span class="math inline">\(100,250,500\)</span> y 1000 remuestras muestra el efecto del número de remuestras sobre la dispersión de las estimaciones del sesgo</p>
</div>
<div id="corrección-del-sesgo-de-un-estimador" class="section level3 hasAnchor" number="14.2.4">
<h3><span class="header-section-number">14.2.4</span> Corrección del sesgo de un estimador<a href="métodos-de-computación-intensiva-el-bootstrap.html#correcci%C3%B3n-del-sesgo-de-un-estimador" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>En ocasiones es deseable eliminar o reducir, dentro de lo posible, el sesgo de un estimador. Si <span class="math inline">\(\hat{\theta}=T(\mathbf{X})\)</span> és un estimador de <span class="math inline">\(\theta\)</span> y <span class="math inline">\(\widehat{b(T)}\)</span> un estimador del sesgo de <span class="math inline">\(T(\mathbf{X})\)</span>, entonces una forma de reducir el sesgo de la estimación es definir el estimador corregido para el sesgo,</p>
<p><span class="math display">\[
\tilde{\theta}=T(\mathbf{X})-\widehat{b(T)}
\]</span></p>
<p>El estimador del sesgo puede obtenerse por cualquier procedimiento que resulte adecuado, ya sea</p>
<ul>
<li>mediante una fórmula analítica “ad hoc”,</li>
<li>el estimador bootstrap del sesgo, <span class="math inline">\(b_{\hat{F}}(T)\)</span>, si se sabe cómo calcularlo,</li>
<li>el estimador bootstrap del sesgo aproximado por Monte Carlo <span class="math inline">\(\widehat{b_{\hat{F}}(T)}\)</span>, o</li>
<li>otros métodos como el jackknife que se discutirá más adelante.</li>
</ul>
<p>Si utilizamos el estimador bootstrap aproximado por Monte Carlo para corregir el sesgo obtenemos el estimador bootstrap de <span class="math inline">\(\theta\)</span> corregido para el sesgo o más precisamente, el estimador de <span class="math inline">\(\theta\)</span> corregido para el sesgo mediante bootstrap:</p>
<p><span class="math display">\[
\begin{aligned}
\tilde{\theta}^{*} &amp; =T(\mathbf{X})-\widehat{b_{\hat{F}}(T)}=T(\mathbf{X})-\left(\overline{\hat{\theta}^{*}}-T(\mathbf{X})\right)= \\
&amp; =2 \cdot T(\mathbf{X})-\overline{\hat{\theta}^{*}}
\end{aligned}
\]</span></p>
<p>Este estimador no debe confundirse con la media bootstrap <span class="math inline">\(\overline{\hat{\theta}^{*}}\)</span> que no es, en general un estimador adecuado de <span class="math inline">\(\theta\)</span>.</p>
<p>La corrección del sesgo no debe realizarse de forma indiscriminada. Puede darse el caso que el estimador corregido para el sesgo tenga una varianza mayor que el estimador original, con lo que el error cuadrático medio aumente en vez de disminuir. Una posibilidad, de cara a decidir si vale la pena o no corregir el sesgo es estimar el error estándar del estimador corregido para el sesgo mediante bootstrap, es decir calcular <span class="math inline">\(\widehat{\sigma_{\tilde{\theta}}^{*}}\)</span>. Si el error estándar bootstrap estimado de <span class="math inline">\(\tilde{\theta}^{*}\)</span> y de <span class="math inline">\(\hat{\theta}^{*}\)</span> es aproximadamente el mismo</p>
<p><span class="math display">\[
\widehat{\sigma_{\tilde{\theta}}^{*}} \approx \widehat{\sigma_{\hat{\theta}}^{*}}
\]</span></p>
<p>dado que el sesgo es menor</p>
<p><span class="math display">\[
\widehat{b_{\hat{F}}\left(\tilde{\theta}^{*}\right)}&lt;\widehat{b_{\hat{F}}\left(\hat{\theta}^{*}\right)}
\]</span></p>
<p>entonces puede ser interesante, en terminos de reduccion del error cuadrático medio, la reducción del sesgo.</p>
</div>
<div id="el-jackknife" class="section level3 hasAnchor" number="14.2.5">
<h3><span class="header-section-number">14.2.5</span> El jackknife<a href="métodos-de-computación-intensiva-el-bootstrap.html#el-jackknife" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>El jackknife fue introducido por Quenouille en 1949 para estudiar el sesgo de un estimador. Es bàsicamente un método de “remuestreo” consistente en eliminar un punto de la muestra original y recalcular el estimador. Esto da lugar a <span class="math inline">\(n\)</span> estimaciones ligeramente distintas a partir de las cuales se estimará el sesgo.</p>
<div id="el-estimador-jackknife-del-sesgo" class="section level4 hasAnchor" number="14.2.5.1">
<h4><span class="header-section-number">14.2.5.1</span> El estimador jackknife del sesgo<a href="métodos-de-computación-intensiva-el-bootstrap.html#el-estimador-jackknife-del-sesgo" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Sea <span class="math inline">\(\left\{X \sim F_{\theta}, \theta \in \Theta\right\}\)</span> y sea <span class="math inline">\(\mathbf{X}\)</span> una muestra aleatoria simple de <span class="math inline">\(X\)</span>. La muestra jackknife <span class="math inline">\(i\)</span>-esima, <span class="math inline">\(\mathbf{X}_{(i)}\)</span> es:</p>
<p><span class="math display">\[
\mathbf{X}_{(i)}=\left(X_{1}, X_{2}, \ldots, X_{i-1}, X_{i+1}, \ldots, X_{n}\right)
\]</span></p>
<p>es decir la muestra original de la que se ha eliminado la observación <span class="math inline">\(X_{i}\)</span>.
Sea ahora <span class="math inline">\(\hat{\theta}=T_{n}(\mathbf{X})\)</span> un estimador del parámetro <span class="math inline">\(\theta\)</span> que puede ser, o no, un estadístico de substitución. Sea</p>
<p><span class="math display">\[
\hat{\theta}_{(i)}=T_{(n-1)}\left(\mathbf{X}_{(i)}\right)=T_{(n-1)}\left(\left(X_{1}, X_{2}, \ldots, X_{i-1}, X_{i+1}, \ldots, X_{n}\right)\right) .
\]</span></p>
<p>El estimador jackknife del sesgo definido por Quenouille es:</p>
<p><span class="math display">\[
\hat{b}_{J}=(n-1)\left(\hat{\theta}_{(\cdot)}-\hat{\theta}_{n}\right), \quad \text { donde: } \hat{\theta}_{(\cdot)}=\frac{1}{n} \sum_{i=1}^{n} \hat{\theta}_{(i)}
\]</span></p>
<p>A partir del estimador jackknife del seso puede definirse el estimador jackknife corregido para el sesgo:</p>
<p><span class="math display">\[
\begin{aligned}
\hat{\theta}_{J}=\hat{\theta}-\hat{b}_{J} &amp; =\hat{\theta}-(n-1)\left(\hat{\theta}_{(\cdot)}-\hat{\theta}_{n}\right) \\
&amp; =n \hat{\theta}-(n-1) \hat{\theta}_{(\cdot)}
\end{aligned}
\]</span></p>
<p>A veces este estimador recibe directamente el nombre de estimador jackknife de <span class="math inline">\(\theta\)</span>.
¿Como se explica que el estimador jackknife tenga menor sesgo que el original? Supongamos que <span class="math inline">\(\hat{b}_{J}\)</span> sea un estimador insesgado del sesgo de <span class="math inline">\(\hat{\theta}\)</span>, es decir</p>
<p><span class="math display">\[
E\left[\hat{b}_{J}\right]=b(\theta)=E\left[T_{n}\right]-\theta
\]</span></p>
<p>Entonces tendremos que:</p>
<p><span class="math display">\[
\begin{aligned}
b\left(\hat{\theta}_{J}\right) &amp; =E\left[\hat{\theta}_{J}\right]-\theta=E\left[\hat{\theta}-\hat{b}_{J}\right]-\theta \\
&amp; =E(\hat{\theta})-E\left(\hat{b}_{J}\right)-\theta= \\
&amp; =\underbrace{\theta+b(\hat{\theta})}_{E(\hat{\theta})}-b(\hat{\theta})-\theta=0 .
\end{aligned}
\]</span></p>
<p>Es decir, si <span class="math inline">\(\hat{b}_{J}\)</span> es un estimador insesgado del sesgo de <span class="math inline">\(\hat{\theta}\)</span> entonces <span class="math inline">\(\hat{\theta}_{J}\)</span> no tendrá sesgo:</p>
<p><span class="math display">\[
E\left(\hat{b}_{J}\right)=b(\hat{\theta}) \Longrightarrow E\left(\hat{\theta}_{J}\right)=\theta
\]</span></p>
<p>Evidentemente el problema reside en que, en general, no es posible establecer analíticamente si <span class="math inline">\(\hat{b}_{J}\)</span> esta centrado, o no, en el sesgo de <span class="math inline">\(\hat{\theta}, b(\theta)\)</span>. Efron ([7]) da un argumento heurístico, no del todo formal, para argumentar que, como mínimo, el sesgo de <span class="math inline">\(\hat{\theta}_{J}\)</span> suele ser inferior al de <span class="math inline">\(\hat{\theta}\)</span>. Para ello se basa en el hecho de que muchos estimadores sesgados son asintóticamente insesgados dado que, a menudo el sesgo es una función del tamaño muestral que tiende a 0 cuando <span class="math inline">\(n \longrightarrow \infty\)</span>, es decir</p>
<p><span class="math display">\[
b(\hat{\theta})=o(1)
\]</span></p>
<p>Ejemplos conocidos de esta propiedad son la variancia muestral o el máximo de una muestra en una distribución uniforme <span class="math inline">\(\mathcal{U}(0, \theta)\)</span>. Para los estimadores de este tipu suele exisatir un desarrollo asintótico de la esperanza en potencias de <span class="math inline">\(n\)</span>, del tipo:</p>
<p><span class="math display">\[
E(\hat{\theta})=\theta+\frac{a_{1}}{n}+\frac{a_{2}}{n^{2}}+\frac{a_{3}}{n^{3}}+\ldots,
\]</span></p>
<p>donde las <span class="math inline">\(a_{i}\)</span> dependen únicamente de <span class="math inline">\(F\)</span> pero no de <span class="math inline">\(n, a_{i}=a_{i}(F)\)</span>, de forma que podemos poner el sesgo de <span class="math inline">\(\hat{\theta}\)</span> como</p>
<p><span class="math display">\[
b(\hat{\theta})=E(\hat{\theta})-\theta=\frac{a_{1}}{n}+\frac{a_{2}}{n^{2}}+O\left(\frac{1}{n^{3}}\right)
\]</span></p>
<p>Suponiendo que el desarrollo (2.7) exista entonces podemos justificar que el sesgo de <span class="math inline">\(\hat{\theta}_{J}\)</span> es inferior al de <span class="math inline">\(\hat{\theta}\)</span>. Veamos como. Si suponemos que</p>
<p><span class="math display">\[
b(\hat{\theta})=\frac{a_{1}}{n}+\frac{a_{2}}{n^{2}}+O\left(\frac{1}{n^{3}}\right)
\]</span></p>
<p>entonces</p>
<p><span class="math display">\[
b\left(\hat{\theta}_{(i)}\right)=\frac{a_{1}}{n-1}+\frac{a_{2}}{(n-1)^{2}}+O\left(\frac{1}{(n-1)^{3}}\right)
\]</span></p>
<p>y por lo tanto, dado que</p>
<p><span class="math display">\[
E\left[\hat{\theta}_{(\cdot)}\right]=E\left(\frac{1}{n} \sum_{i=1}^{n} \hat{\theta}_{(i)}\right)=E\left(\hat{\theta}_{(i)}\right)
\]</span></p>
<p>el sesgo de <span class="math inline">\(\hat{\theta}_{(\cdot)}\)</span> tiene la misma expresión que el de <span class="math inline">\(\hat{\theta}_{(i)}\)</span>. Podemos calcular la esperanza del estimador del sesgo, es decir:</p>
<p><span class="math display">\[
\begin{aligned}
E\left(\hat{b}_{J}\right) &amp; =E\left[(n-1)\left(\hat{\theta}_{(\cdot)}-\hat{\theta}_{n}\right)\right] \\
&amp; =(n-1)\left(E\left(\hat{\theta}_{(\cdot)}\right)-E\left(\hat{\theta}_{n}\right)\right) \\
&amp; =(n-1)\left[\left(\theta+b\left(\hat{\theta}_{(i)}\right)\right)+\left(\theta+b\left(\hat{\theta}_{n}\right)\right)\right] \\
&amp; =(n-1)\left[b\left(\hat{\theta}_{(i)}\right)-b\left(\hat{\theta}_{n}\right]\right. \\
&amp; =(n-1)\left[\left(\frac{1}{n-1}-\frac{1}{n}\right) a_{1}+\left(\frac{1}{(n-1)^{2}}-\frac{1}{n^{2}}\right) a_{2}+O\left(\frac{1}{n^{3}}\right)\right] \\
&amp; =\frac{a_{1}}{n}+\frac{(2 n-1)}{n^{2}(n-1)} a_{2}+O\left(\frac{1}{n^{2}}\right)
\end{aligned}
\]</span></p>
<p>La expresion anterior muestra que, bajo determinadas suposiciones, <span class="math inline">\(\hat{b}_{J}\)</span> es correcto hasta el orden <span class="math inline">\(n^{-2}\)</span> com estimador del sesgo de <span class="math inline">\(\hat{\theta}_{n}\)</span>.</p>
<p>Reuniendo lo anterior para determinar el sesgo de <span class="math inline">\(\hat{\theta}_{J}\)</span> tendremos:</p>
<p><span class="math display">\[
\begin{aligned}
b\left(\hat{\theta}_{J}\right) &amp; =b\left(\hat{\theta}_{n}-\hat{b}_{J}\right)= \\
&amp; =b\left(\hat{\theta}_{n}\right)-E\left(\hat{b}_{J}\right)= \\
&amp; =-\frac{b}{n(n-1)}+O\left(\frac{1}{n^{2}}\right)
\end{aligned}
\]</span></p>
<p>que es de un orden inferior a <span class="math inline">\(b\left(\hat{\theta}_{n}\right)\)</span>, lo que explica porqué, en general, <span class="math inline">\(\hat{\theta}_{J}\)</span> tendrá menos sesgo que <span class="math inline">\(\hat{\theta}_{n}\)</span>.</p>
<p>Aún insistiendo en el hecho de que la aproximación que hemos realizado es heurística su validez es bastante grande. Por ejemplo para los estimadores máximo verosímiles suele existir un desarrollo asintótico de la esperanza con lo que en este caso la afirmación seria cierto.</p>
</div>
</div>
<div id="ejercicios-1" class="section level3 hasAnchor" number="14.2.6">
<h3><span class="header-section-number">14.2.6</span> Ejercicios<a href="métodos-de-computación-intensiva-el-bootstrap.html#ejercicios-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ol style="list-style-type: decimal">
<li>Supongamos que utilizamos la mediana muestral para estimar la media poblacional <span class="math inline">\(\theta=E_{F}(X)\)</span>. Describa el estimador bootstrap del sesgo <span class="math inline">\(\widehat{\operatorname{bias}}_{B}\)</span>.</li>
<li>Sea <span class="math inline">\(\widehat{b i a s}_{\text {JACK }}\)</span> el estimador jackknife del sesgo de un estimador <span class="math inline">\(\hat{\theta}\)</span> :</li>
</ol>
<p><span class="math display">\[
\widehat{\operatorname{bias}}_{J A C K}=(n-1)\left(\hat{\theta}_{(\cdot)}-\hat{\theta}\right),
\]</span></p>
<p>siendo</p>
<p><span class="math display">\[
\begin{array}{r}
\hat{\theta}_{(\cdot)}=\sum_{i=1}^{n} \hat{\theta}_{(i)} / n, \mathrm{y} \\
\hat{\theta}_{(i)}=\hat{\theta}\left(x_{1}, x_{2}, \ldots, x_{i-1}, x_{i+1}, \ldots x_{n}\right)
\end{array}
\]</span></p>
<p>Mostrar que, si <span class="math inline">\(\hat{\theta}=\bar{x}\)</span> entonces <span class="math inline">\(\hat{\theta}_{(\cdot)}-\hat{\theta}=0\)</span>, es decir <span class="math inline">\(\widehat{\operatorname{bias}}_{\text {JACK }}=0\)</span>.
3. Demuestre que si <span class="math inline">\(\hat{\theta}=\bar{x}\)</span> el estimador jackknife del error estandar definido por</p>
<p><span class="math display">\[
\widehat{E E}_{J A C K}=\sqrt{\frac{n-1}{n} \sum_{i=1}^{n}\left(\hat{\theta}_{(i)}-\hat{\theta}_{(\cdot)}\right)^{2}}
\]</span></p>
<p>coincide con el estimador insesgado del error error estándar de la media</p>
<p><span class="math display">\[
\hat{s} / \sqrt{n}=\sqrt{\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2} /\{(n-1) n\}} .
\]</span></p>
</div>
</div>
<div id="intervalos-de-confianza-bootstrap" class="section level2 hasAnchor" number="14.3">
<h2><span class="header-section-number">14.3</span> Intervalos de confianza bootstrap<a href="métodos-de-computación-intensiva-el-bootstrap.html#intervalos-de-confianza-bootstrap" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="introducción-11" class="section level3 hasAnchor" number="14.3.1">
<h3><span class="header-section-number">14.3.1</span> Introducción<a href="métodos-de-computación-intensiva-el-bootstrap.html#introducci%C3%B3n-11" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Como hemos visto en el apartado anterior, un aspecto importante en la estimación de un parámetro es el grado de precisión de dicha estimación. Una forma de incluir la medida de la precisión en la estimación consiste en utilizar estimadores por intervalo, es decir, estimar <span class="math inline">\(\theta\)</span> mediante un subconjunto <span class="math inline">\(T \subset \Theta\)</span> del espacio paramétrico que verifique:</p>
<p><span class="math display">\[
T=\{t \in \Theta \mid P(\theta \in T)=(1-\alpha)\} .
\]</span></p>
<p>Hay muchos métodos para construir intervalos de confianza (IC de aquí en adelante). Bajo ciertas circunstancias es posible obtener IC exactos, entendiendo como tales aquellos en que el recubrimiento real del intervalo, <span class="math inline">\(P(\theta \in T)\)</span> es igual al recubrimiento nominal <span class="math inline">\(1-\alpha\)</span> con que se pretende estimar el parámetro.</p>
<p>El método del pivote o el de Neyman, que se estudian en los cursos introductorios de Estadística Matemática, pueden dar IC exactos en algunos casos, como la media de una población normal o el parámetro de una ley exponencial. Son sin embargo escasas las ocasiones en que se verifican las condiciones de aplicación de estos métodos. En muchas situaciones de interés es preciso recurrir a intervalos de confianza aproximados en los que el recubrimiento real es tan solo aproximadamente igual al nominal es decir <span class="math inline">\(P(\theta \in T) \simeq 1-\alpha\)</span>.</p>
<p>El bootstrap se ha revelado como una técnica de gran utilidad de cara a construir IC aproximados. Como hemos visto en la primera parte de este tema su naturaleza esencialmente no paramétrica permite abordar gran tipo
de situaciones en los que las condiciones de aplicación de otros métodos “clásicos” no se verifican.</p>
<p>Una ventaja adicional de los métodos bootstrap es que, mediante procedimientos sofisticados, en los que aquí no entraremos, es posible conocer qué tan bueno es el grado de aproximación de un IC aproximado, valga la redundancia. Dicho de otra forma, se puede cuantificar la diferencia entre el recubrimiento real y el nominal. Ello ha permitido no tan sólo crear una amplia gama de metodos de construcción de IC sino también determinar en qué grado la estimación que estos ofrecen debe de considerarse aceptable.</p>
<p>El número de técnicas que se basan en la distribución bootstrap del estadístico para, en una forma u otra, obtener intervalos de confianza es muy elevado. Para evitar que, como se suele decir, el gran número de árboles no nos impida ver el bosque, nos centraremos en tres categorías: los intervalos de confianza estándar, los intervalos de confianza estudentizados o bootstrap-t y el grupo de los denominados métodos percentil, todos ellos ideados por Efron que fue introduciendo sofisticaciones en una interesante idea inicial.</p>
</div>
<div id="intervalos-de-confianza-estándar" class="section level3 hasAnchor" number="14.3.2">
<h3><span class="header-section-number">14.3.2</span> Intervalos de confianza estándar<a href="métodos-de-computación-intensiva-el-bootstrap.html#intervalos-de-confianza-est%C3%A1ndar" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>El concepto de IC estándar no va necesariamente ligado al bootstrap . La idea en que se basa consiste en suponer que, dado un parámetro <span class="math inline">\(\theta\)</span>, un estimador centrado <span class="math inline">\(\hat{\theta}\)</span> de <span class="math inline">\(\theta\)</span> y el error estándar de <span class="math inline">\(\hat{\theta}, \sigma_{\hat{\theta}}\)</span>,se verifica de forma aproximada que:</p>
<p><span class="math display">\[
\frac{\hat{\theta}-\theta}{\sigma_{\hat{\theta}}} \xrightarrow{\mathcal{L}} N(0,1) .
\]</span></p>
<p>Bajo esta suposición un IC aproximado al nivel de confianza <span class="math inline">\((1-\alpha)\)</span> será:</p>
<p><span class="math display">\[
\hat{\theta}-z_{1-\alpha / 2} \sigma_{\hat{\theta}}, \hat{\theta}-z_{\alpha / 2} \sigma_{\hat{\theta}}
\]</span></p>
<p>donde <span class="math inline">\(z_{1-\alpha / 2}\)</span> es el percentil <span class="math inline">\(100(1-\alpha / 2)\)</span> de la distribución normal <span class="math inline">\(N(0,1)\)</span>.
En una situación paramétrica, <span class="math inline">\(\sigma_{\hat{\theta}}\)</span> puede ser el error estándar asintótico del estimador máximo-verosímil o incluso una expresión exacta. P.ej. si <span class="math inline">\(\hat{\theta}=\hat{\rho}\)</span>, el coeficiente de correlación muestral, entonces, supuesta la distribución de la población normal bivariante, podemos usar el estimador de la teoría normal:</p>
<p><span class="math display">\[
\sigma_{N O R M}=\left(1-\rho^{2}\right) /(n-3)^{1 / 2}
\]</span></p>
<p>Cuando no se conoce la forma de <span class="math inline">\(\sigma_{\hat{\theta}}\)</span> puede utilizarse el estimador bootstrap del error estándar, <span class="math inline">\(\sigma_{B O O T}\)</span> definido en la sección anterior, que, como hemos visto puede siempre aproximarse mediante el algoritmo de Monte Carlo.</p>
<p>Los IC estándar resultan muy atractivos -han sido y son muy utilizados en la práctica- por el hecho de ser automáticos: es posible escribir un programa que a partir de cualquier conjunto de datos estime el parámetro y los construya.</p>
<p>Su inconveniente principal reside en el hecho de que son aproximados, con lo pueden dar lugar a intervalos inexactos. Vale la pena destacar que, de hecho, suelen ser doblemente aproximados puesto que en la práctica se realizan a menudo no una sino dos aproximaciones:</p>
<ol style="list-style-type: decimal">
<li>La suposición de normalidad, que no es necesariamente cierta, sobre todo en muestras pequeñas,</li>
<li>A menudo el error estándar es funcion del parámetro desconocido, es decir <span class="math inline">\(\sigma_{\hat{\theta}}=\sigma_{\hat{\theta}}(\theta)\)</span> con lo que no podemos basarnos en la aproximación</li>
</ol>
<p><span class="math display">\[
\frac{\hat{\theta}-\theta}{\sigma_{\hat{\theta}}} \xrightarrow{\mathcal{L}} N(0,1),
\]</span></p>
<p>sinó en esta otra, de velocidad de convergencia más lenta, y por lo tanto más errónea en muestras pequeñas:</p>
<p><span class="math display">\[
\frac{\hat{\theta}-\theta}{\hat{\sigma}_{\hat{\theta}}} \xrightarrow{\mathcal{L}} N(0,1) .
\]</span></p>
<p>Un ejemplo conocido de esta doble aproximación es el caso de la proporción muestral que estima la probabilidad de observar un suceso <span class="math inline">\(A, P(A)\)</span> mediante</p>
<p><span class="math display">\[
\hat{p}=\frac{\sum_{i=1}^{n} Y_{i}}{n}, Y=\left\{\begin{array}{l}
1, \text { si se presenta un suceso } A \\
0, \text { si no se presenta el suceso }
\end{array}\right.
\]</span></p>
<p>donde el error estándar de <span class="math inline">\(\hat{p}\)</span> es:</p>
<p><span class="math display">\[
\sigma_{\hat{p}}=\sqrt{\frac{p \cdot q}{n}},
\]</span></p>
<p>que debe aproximarse en la práctica por:</p>
<p><span class="math display">\[
\hat{\sigma}_{\hat{p}}=\sqrt{\frac{\hat{p} \cdot \hat{q}}{n}}
\]</span></p>
<p>Otros ejemplos de sta doble aproximación los tendremos cuando utilicemos el estimador aproximado por montecarlo del estimador bootstrap del error estándar o el estimador del error estándar del coeficiente de correlación,(3.1), en donde substituyamos <span class="math inline">\(\rho\)</span> por su estimador máximo verosímil, <span class="math inline">\(\hat{\rho}\)</span>.</p>
</div>
</div>
<div id="intervalos-de-confianza-basados-en-tablas-bootstrap" class="section level2 hasAnchor" number="14.4">
<h2><span class="header-section-number">14.4</span> Intervalos de confianza basados en tablas bootstrap<a href="métodos-de-computación-intensiva-el-bootstrap.html#intervalos-de-confianza-basados-en-tablas-bootstrap" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="el-caso-de-la-t-de-student" class="section level3 hasAnchor" number="14.4.1">
<h3><span class="header-section-number">14.4.1</span> El caso de la t de Student<a href="métodos-de-computación-intensiva-el-bootstrap.html#el-caso-de-la-t-de-student" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Cuando el parámetro de interés es la media poblacional de una distribución normal <span class="math inline">\(X \sim N(\mu, \sigma)\)</span>, que estimamos mediante la media muestral, <span class="math inline">\(\hat{\theta}=\bar{X}\)</span>, existe una solución conocida para que no sea preciso realizar la doble aproximación</p>
<p><span class="math display">\[
\frac{\bar{X}-\mu}{\hat{\sigma}_{\bar{X}}} \dot{\sim} N(0,1)
\]</span></p>
<p>Tomando <span class="math inline">\(\hat{\sigma}_{\bar{X}}=s / \sqrt{n-1}\)</span>, donde <span class="math inline">\(s\)</span> representa la desviación típica muestra, és decir el estimador maximo verosímil de <span class="math inline">\(\sigma\)</span>, la distribución del estadístico anterior es una <span class="math inline">\(t\)</span> de Student con <span class="math inline">\(n-1\)</span> grados de libertad, es decir:</p>
<p><span class="math display">\[
\frac{\bar{X}-\mu}{s / \sqrt{n-1}} \sim t_{n-1}
\]</span></p>
<p>Basándonos en esta aproximación el intervalo de confianza que se obtiene para <span class="math inline">\(\hat{\theta}\)</span> es:</p>
<p><span class="math display">\[
\hat{\theta}-t_{n-1,1-\alpha / 2} \hat{\sigma}_{\hat{\theta}}, \hat{\theta}-t_{n-1, \alpha / 2} \hat{\sigma}_{\hat{\theta}}
\]</span></p>
<p>donde <span class="math inline">\(t_{n-1,1-\alpha / 2}\)</span> es el percentil <span class="math inline">\(1-\alpha / 2\)</span> de la distribución <span class="math inline">\(t\)</span> de Student con <span class="math inline">\(n-1\)</span> grados de libertad.</p>
<p>Este intervalo es exacto si <span class="math inline">\(X \sim N(\mu, \sigma)\)</span> y el parámetro a estimar es la media, <span class="math inline">\(\mu \mathrm{y}\)</span> tiene el efcto de ensanchar el intervalo para compensar el error cometido al tener que estimar <span class="math inline">\(\sigma_{\bar{X}}\)</span>, con <span class="math inline">\(\hat{\sigma}_{\bar{X}}\)</span>. Esto se debe al hecho de que la distribución <span class="math inline">\(t\)</span> de Student es ligeramente más ancha que la <span class="math inline">\(N(0,1)\)</span> a la cual tiende cuando <span class="math inline">\(n \rightarrow \infty\)</span>.</p>
<p>Si, por ejemplo tomamos los datos de tiempos de supervivencia de los ratones del grupo control, discutidos en el capítulo 1</p>
<p><span class="math display">\[
52,10,40,104,51,27,146,30,46
\]</span></p>
<p>con media y error estándar</p>
<p><span class="math display">\[
\bar{X}=56,22, \hat{\sigma}_{\bar{X}}=s / \sqrt{n-1}=13,33
\]</span></p>
<p>obtenemos el intérvalo de confianza “aproximadísimo” al <span class="math inline">\(95 \%\)</span> basado en la normal:</p>
<p><span class="math display">\[
56,22 \pm 1,645 \times 13,33=[34,29,78,15]
\]</span></p>
<p>y el intervalo de confianza basado en la <span class="math inline">\(t\)</span> de tudent:</p>
<p><span class="math display">\[
56,22 \pm 1,86 \times 13,33=[31,22,81,01] .
\]</span></p>
<p>La corrección que efectua el segundo tipo de intérvalo es válida únicamente para la media. Otros estadísticos como la mediana no pueden aprovecharla por lo que un intervalo de confianza estándar para la mediana seria del tipo:</p>
<p><span class="math display">\[
\widehat{M e d} \pm 1,645 \times \hat{\sigma}_{B O O T}(\widehat{M e d}) .
\]</span></p>
</div>
<div id="intervalos-de-confianza-basados-en-tablas-bootstrap-1" class="section level3 hasAnchor" number="14.4.2">
<h3><span class="header-section-number">14.4.2</span> Intervalos de confianza basados en tablas bootstrap<a href="métodos-de-computación-intensiva-el-bootstrap.html#intervalos-de-confianza-basados-en-tablas-bootstrap-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Hasta el momento hemos podido constatar que la aproximación normal falla, especialmente con muestras pequeñas si tenemos que estimar <span class="math inline">\(\sigma_{\hat{\theta}}\)</span>.</p>
<p>Tambien hemos visto un caso, la media muestral en que este problema se compensa utilizando unas tablas diferentes a las de la <span class="math inline">\(N(0,1)\)</span>, las de la <span class="math inline">\(t\)</span> de Student. Observamos que lo que ha sucedido es que hemos ganado en precisión al precio de perder generalidad ya que hemos pasado de usar unas tablas válidas para todas las muestras a necesitar una distinta para cada tamaño muestral. Encima esta mejora con “coste” solo es válida para el caso de la media muestral. Para la inmensa mayoria de los estadísticos restantes no es válida.</p>
<p>El bootstrap-t fuerza esta idea de ganar precisión perdiendo generalidad y pasa de tener una tabla distinta para cada tamaño muestral a tener una tabla distinta para cada muestra. Es decir lo que hace este procedimiento es construir una tabla a medida para cada muestra. Esta tabla se utilizará para construir intervalos de confianza exactament de la misma forma en que se procedia con la <span class="math inline">\(N(0,1)\)</span> o con la <span class="math inline">\(t_{n-1}\)</span>.</p>
<p>El algoritmo para el cálculo de los intervalos de confianza es el siguiente:</p>
<ol style="list-style-type: decimal">
<li>Sea <span class="math inline">\(\mathbf{X}\)</span> la muestra original y <span class="math inline">\(\hat{\theta}=T(\mathbf{X})\)</span></li>
<li>Extraer <span class="math inline">\(B\)</span> muestras, <span class="math inline">\(\mathbf{X}_{1}^{*}, \mathbf{X}_{2}^{*}, \ldots, \mathbf{X}_{B}^{*}\)</span> de <span class="math inline">\(F_{n}\)</span></li>
<li>Sobre cada una de ellas:</li>
</ol>
<ol style="list-style-type: lower-alpha">
<li>Calcular <span class="math inline">\(\hat{\theta}_{b}^{*}=T\left(\mathbf{X}_{b}^{*}\right), \widehat{\sigma}_{\hat{\theta}}\left(\mathbf{X}_{b}^{*}\right)=\widehat{\sigma}_{b}^{*}\)</span></li>
<li>Calcular</li>
</ol>
<p><span class="math display">\[
\frac{\hat{\theta}_{b}^{*}-\hat{\theta}}{\widehat{\sigma}_{b}^{*}}=z_{b}^{*}
\]</span></p>
<p>Este algoritmo dara lugar a <span class="math inline">\(B\)</span> valores <span class="math inline">\(z_{1}^{*}, z_{2}^{*}, \ldots, z_{B}^{*}\)</span> que, una vez ordenados <span class="math inline">\(z_{(1)}^{*}, z_{(2)}^{*}, \ldots, z_{(B)}^{*}\)</span> representan la aproximación de Monte carlo a la distribución bootstrap del estadístico</p>
<p><span class="math display">\[
Z=\frac{\hat{\theta}-\theta}{\widehat{\sigma}_{\hat{\theta}}}
\]</span></p>
<ol start="4" style="list-style-type: decimal">
<li>El percentil <span class="math inline">\(\alpha\)</span> de de <span class="math inline">\(\mathrm{Z}^{*}\)</span> lo estimaremos por el valor <span class="math inline">\(\hat{t}^{(\alpha)}\)</span> que verifica que</li>
</ol>
<p><span class="math display">\[
\frac{\#\left\{z_{b}^{*} \leq \hat{t}^{(\alpha)}\right\}}{B}=\alpha,
\]</span></p>
<ol start="5" style="list-style-type: decimal">
<li>El intervalo de confianza basado en el bootstrap- <span class="math inline">\(t\)</span> será finalmente:</li>
</ol>
<p><span class="math display">\[
\hat{\theta}-\hat{t}^{(1-\alpha / 2)} \hat{\sigma}_{\hat{\theta}}, \hat{\theta}-\hat{t}^{(\alpha / 2)} \hat{\sigma}_{\hat{\theta}} .
\]</span></p>
<p>Aplicando este método a la media del grupo control se obtiene</p>
<p><span class="math display">\[
56,22-1,53 \times 13,33,56,22+4,53 \times 13,33=[35,82,116,74] .
\]</span></p>
<p>Si comparamos los tres intervalos desarrollados hastra el momento observamos lo siguiente:</p>
<table>
<thead>
<tr class="header">
<th align="left">Intervalo</th>
<th align="left">Extremos</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Normal</td>
<td align="left"><span class="math inline">\([34,29,78,15]\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(t\)</span> de Student</td>
<td align="left"><span class="math inline">\([31,22,81,01]\)</span></td>
</tr>
<tr class="odd">
<td align="left">Bootstrap- <span class="math inline">\(t\)</span></td>
<td align="left"><span class="math inline">\([35,8,116,7]\)</span>.</td>
</tr>
</tbody>
</table>
<p>Pdemos observar que, mientras el extremo inferior és muy parecido en los tres casos el IC basado en el bootstrap tiene un extremo superior más alargado, como consecuencia de que en 1 muestra aparecen 2 valores muy grandes.</p>
<p>El estadístico <span class="math inline">\(Z=\frac{\hat{\theta}-\theta}{\widehat{\sigma}_{\hat{\theta}}}\)</span>, cuya distribución aproximamos doblemente mediante bootstrap recibe el nombre de pivote aproximado, lo que da a entender
que su distribución no depende de <span class="math inline">\(\theta\)</span>. Esta propiedad es la que permite utilizarlo para construir el intervalo de confianza basado en el bootstrap-t.</p>
<p>La clave del buen funcionamiento del bootsrap- <span class="math inline">\(t\)</span> está precisamente en qué tan buena aproximación a la distribución del pivote podemos lograr mediante el remuestreo. Estudios teóricos bastante complejos demuestran que, en muestras grandes el recubrimiento del bootstrap-t tiende a aproximarse más al recubrimiento nominal, es decir el recubrimiento que se desea que tengan los intervalos de confianza en el momento de construirlos, que los intervalos estándar o basados en la <span class="math inline">\(t\)</span> de Student. El bootstrap- <span class="math inline">\(t\)</span> es una generalización de la <span class="math inline">\(t\)</span> de Student que extiende su aplicabilidad de la media muestral a estadísticos de posición como la media, la mediana, la media recortada o el percentil de la muestra.</p>
<p>Resumiendo:</p>
<table>
<colgroup>
<col width="25%" />
<col width="25%" />
<col width="25%" />
<col width="25%" />
</colgroup>
<thead>
<tr class="header">
<th align="left">Intervalos</th>
<th align="left">IC estándar <br> (Normales)</th>
<th align="left">IC estandar <br> (t de Student)</th>
<th align="left">Bootstrap-t</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Propiedades</td>
<td align="left">Simétricos</td>
<td align="left">Simétricos</td>
<td align="left">No simétricos</td>
</tr>
<tr class="even">
<td align="left">Tablas</td>
<td align="left">Muy inexactos</td>
<td align="left">Poca exactitud</td>
<td align="left">Los más exactos</td>
</tr>
<tr class="odd">
<td align="left">Una sola</td>
<td align="left">Una para cada <span class="math inline">\(n\)</span></td>
<td align="left">Una por muestra</td>
<td align="left"></td>
</tr>
</tbody>
</table>
</div>
<div id="el-bootstrap-t-y-las-transformaciones" class="section level3 hasAnchor" number="14.4.3">
<h3><span class="header-section-number">14.4.3</span> El bootstrap-t y las transformaciones<a href="métodos-de-computación-intensiva-el-bootstrap.html#el-bootstrap-t-y-las-transformaciones" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>A pesar de sus buenas propiedades el bootstrap- <span class="math inline">\(t\)</span> no es la “panacea” en cuanto a intervalos de confianza, puesto que presenta algunas dificultades.</p>
<ul>
<li>El cálculo de los intervalos basados en el bootstrap-t puede ser muy costoso computacionalmente. Como hemos visto se basa en calcular o aproximar los percentiles de <span class="math inline">\(Z_{b}^{*}\)</span> definido como</li>
</ul>
<p><span class="math display">\[
Z_{b}^{*}=\frac{\hat{\theta}_{b}^{*}-\hat{\theta}}{\widehat{\sigma}_{b}^{*}}=\frac{T\left(\mathbf{X}_{b}^{*}\right)-T(\mathbf{X})}{\widehat{\sigma}_{\hat{\theta}}\left(\mathbf{X}_{b}^{*}\right)}
\]</span></p>
<p>pero, dado que, en general, no se dispone de una fórmula cerrada con la que calcular <span class="math inline">\(\widehat{\sigma}_{\hat{\theta}}\left(\mathbf{X}_{b}^{*}\right)\)</span> suele recurrirse de nuevo al bootstrap (una alternativa es el jackknife) es decir debemos remuestrear cada remuestra para estimar el error estándar, lo que conlleva de nuevo un error de aproximación:</p>
<p><span class="math display">\[
\widehat{\sigma}_{b}^{*} \approx \widehat{\sigma}_{b, B_{2}}^{* *}
\]</span></p>
<p>Teniendo en cuenta que para construir intervalos de confianza suelen considerarse necesarias unas <span class="math inline">\(1000-2000\)</span> remuestras, si cada una tiene que remuestrearse entre 100 y 200 veces estamos hablando de
un número total de remuestras que oscila entre 100.000 y 400.000 , lo que, segun para que modelos puede resultar bastante costoso.</p>
<ul>
<li>Para que el bootstrap-t funcione correctamente el estimador del error estándar debe ser estable, es decir no debe tener una gran variación entre muestras. Algunos estimadores del error estándar varian mucho entre muestras, y, en estos casos el bootstrap- <span class="math inline">\(t\)</span> puede dar resultados erráticos. Para acabar de empeorar la situación el estimador bootstrap del error estándar no es estable, es decir no tan sólo da lugar a que el proceso sea costoso sino que puede hacer que no sea bueno.</li>
</ul>
<p>Un caso conocido, en donde el bootstrap- <span class="math inline">\(t\)</span> no funciona correctamente es el coeficiente de correlación. Si aplicamos sin más el bootsrap- <span class="math inline">\(t\)</span> a los datos de las notas de los 15 alumnos presentados en el capítulo 1 , tomando <span class="math inline">\(B_{1}=1000\)</span>, <span class="math inline">\(B_{2}=25\)</span> y <span class="math inline">\(\alpha=0.10\)</span> obtenemos</p>
<p><span class="math display">\[
\left[\hat{\rho}_{i n f}, \hat{\rho}_{s u p}\right]=[-0,026,0,90]
\]</span></p>
<p>que, para un valor de <span class="math inline">\(\hat{\rho}\)</span> de 0.776 parece excesivamente ancho (Un test basado en estos intervalos nos llevaria a considerar no significativa la correlación entre ambas notas, lo cual contradice la intuición de los datos).</p>
<div id="transformación-normalizante-para-el-coeficiente-de-correlación" class="section level4 hasAnchor" number="14.4.3.1">
<h4><span class="header-section-number">14.4.3.1</span> Transformación normalizante para el coeficiente de correlación<a href="métodos-de-computación-intensiva-el-bootstrap.html#transformaci%C3%B3n-normalizante-para-el-coeficiente-de-correlaci%C3%B3n" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Los textos de estadística matemática sugieren que para hacer inferencia sobre el coeficiente de correlación, si los datos siguen una distribución normal bivariante puede aplicarse lo que se conoce con el nombre de transformación normalizante de Fisher para el coeficiente de correlación. Ésta permite normalizar el coeficiente de correlación muestral, <span class="math inline">\(\hat{\rho}\)</span> aplicándole la transformación arco tangente hiperbólica, <span class="math inline">\(\tanh ^{-1}\)</span>. Dado que se trata de una transformación monótona ello permite la construcción de intervalos de confianza por el procedimiento siguiente:</p>
<ol style="list-style-type: decimal">
<li>Dado <span class="math inline">\(\hat{\rho}\)</span> el coeficiente de correlación calculado sobre una muestra la transformación <span class="math inline">\(\tanh ^{-1}(\hat{\rho})\)</span> da lugar a un estadístico aproximadamente normal:</li>
</ol>
<p><span class="math display">\[
\tanh ^{-1}(\hat{\rho})=\hat{\phi} \simeq N\left(\mu_{\phi}, \sigma_{\phi}\right), \quad \mu_{\phi}=\phi+\frac{\rho}{2(n-1)}, \quad \sigma_{\phi}=\frac{1}{(n-3)}
\]</span></p>
<ol start="2" style="list-style-type: decimal">
<li>A continuación se construye un intervalo de confianza “normal” para <span class="math inline">\(\phi\)</span></li>
</ol>
<p><span class="math display">\[
\left[\hat{\phi}_{1}, \hat{\phi}_{2}\right], \quad \hat{\phi}_{1}=\mu_{\hat{\phi}}-z_{\alpha / 2} \sigma_{\hat{\phi}}, \hat{\phi}_{2}=\mu_{\hat{\phi}}+z_{\alpha / 2} \sigma_{\hat{\phi}}
\]</span></p>
<ol start="3" style="list-style-type: decimal">
<li>Invirtiendo la transformación se obtienen intervalos de confianza para <span class="math inline">\(\rho\)</span>.</li>
</ol>
<p><span class="math display">\[
\begin{aligned}
&amp; \hat{\rho}_{1}=\tanh \left[\left(\phi+\frac{\hat{\rho}}{2(n-1)}\right)-\frac{z_{\alpha / 2}}{\sqrt{n-3}}\right] \\
&amp; \hat{\rho}_{2}=\tanh \left[\left(\phi+\frac{\hat{\rho}}{2(n-1)}\right)+\frac{z_{\alpha / 2}}{\sqrt{n-3}}\right] .
\end{aligned}
\]</span></p>
<p>Por ejemplo, con los datos de las notas de 15 alumnos si</p>
<ul>
<li><span class="math inline">\(\alpha=0,05\)</span> el intervalo de confianza obtenido es [0,592, 0,926]</li>
<li><span class="math inline">\(\alpha=0,10\)</span> el intervalo de confianza obtenido es <span class="math inline">\([0,4487,0,9495]\)</span>.</li>
</ul>
</div>
<div id="aplicación-del-bootstrap--t-sobre-la-escala-transformada" class="section level4 hasAnchor" number="14.4.3.2">
<h4><span class="header-section-number">14.4.3.2</span> Aplicación del bootstrap- <span class="math inline">\(t\)</span> sobre la escala transformada<a href="métodos-de-computación-intensiva-el-bootstrap.html#aplicaci%C3%B3n-del-bootstrap--t-sobre-la-escala-transformada" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>La idea de transformar los datos para construir los intervalos de confianza puede expresarse, esquemáticamente:</p>
<p><span class="math display">\[
\hat{\rho} \xrightarrow{\text { tanh }} \hat{\phi}^{\text {IC estandar }}\left[\hat{\phi}_{1}, \hat{\phi}_{2}\right] \xrightarrow{\text { tanh }^{-1}}\left[\hat{\rho}_{1}, \hat{\rho}_{2}\right] \quad(\stackrel{\alpha=0,10}{=}[0,4487,0,9495])
\]</span></p>
<p>Si procedemos de forma similar pero una vez en la escala transformada calculamos intervalos de confianza basados en el bootstrap- <span class="math inline">\(t\)</span></p>
<p><span class="math display">\[
\hat{\rho} \xrightarrow{\text { tanh }} \hat{\phi} \xrightarrow{\text { Bootstrap- } t}\left[\hat{\phi}_{1}^{*}, \hat{\phi}_{2}^{*}\right] \xrightarrow{\tanh ^{-1}}\left[\hat{\rho}_{1}^{*}, \hat{\rho}_{2}^{*}\right] \quad(\stackrel{\alpha=0,10}{=}[0,45,0,93])
\]</span></p>
<p>obtenemos unos intervalos distintos, aparentemente mejores, que los que obteníamos para el coeficiente de correlación al trabajar sobre la escala sin transformar. Si el coeficiente de confianza es menor la diferencia resulta aún más exagerada. Un intervalo bootstrap- <span class="math inline">\(t\)</span> al <span class="math inline">\(98 \%\)</span> es <span class="math inline">\([-0.66,1.03]\)</span> en la escala original y [0.17,0-95] en la transformada.</p>
<p>Resumiendo: El bootstrap-t no da lugar a intervalos que se conserven al transformar los datos. En el ejemplo anterior los mejores intervalos son los construidos en la escala transformada ( <span class="math inline">\(\hat{\phi}\)</span> ) puesto que sabemos que la transformación es realmente normalizante. De hecho si calculamos los intervalos de confianza estándar a partir de <span class="math inline">\(\hat{\phi}\)</span> y los invertimos obtenemos valores similares a los obtenidos con el bootstrap- <span class="math inline">\(t\)</span> basado calculado sobre <span class="math inline">\(\hat{\phi}\)</span> e invirtiendo.</p>
<table>
<colgroup>
<col width="20%" />
<col width="20%" />
<col width="20%" />
<col width="20%" />
<col width="20%" />
</colgroup>
<thead>
<tr class="header">
<th align="left"></th>
<th align="left"><span class="math inline">\(90 \%\)</span></th>
<th align="left"></th>
<th align="left"><span class="math inline">\(98 \%\)</span></th>
<th align="left"></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"></td>
<td align="left">Ext. inf</td>
<td align="left">Ext. sup</td>
<td align="left">Ext. inf</td>
<td align="left">Ext. sup.</td>
</tr>
<tr class="even">
<td align="left">bootstrap-t directo</td>
<td align="left">-0.026</td>
<td align="left">0.90</td>
<td align="left">-0.66</td>
<td align="left">1.03</td>
</tr>
<tr class="odd">
<td align="left">bootstrap-t para <span class="math inline">\(\hat{\phi}\)</span> e inversión</td>
<td align="left">0.45</td>
<td align="left">0.903</td>
<td align="left">0.17</td>
<td align="left">0.95</td>
</tr>
<tr class="even">
<td align="left">IC estándar para <span class="math inline">\(\hat{\phi}\)</span> e inversión</td>
<td align="left">0.591</td>
<td align="left">0.926</td>
<td align="left">0.449</td>
<td align="left">0.949</td>
</tr>
</tbody>
</table>
<p>El bootsrap- <span class="math inline">\(t\)</span> funciona “tan bien como lo correcto” es decir el intervalo estándar en la escala normal, cuando se conoce la transformación normalizante.</p>
<p>En general no conoceremos esta transformación y tendremos que considerar distintas alternativas:</p>
<ul>
<li>Suponer que existe aunque no la conozcamos. Esto nos llevará a los métodos percentil desarrollados por Efron</li>
<li>Estimar la transformación utilizando el bootstrap.</li>
</ul>
</div>
</div>
</div>
<div id="intervalos-de-confianza-basados-en-percentiles-bootstrap" class="section level2 hasAnchor" number="14.5">
<h2><span class="header-section-number">14.5</span> Intervalos de confianza basados en percentiles bootstrap<a href="métodos-de-computación-intensiva-el-bootstrap.html#intervalos-de-confianza-basados-en-percentiles-bootstrap" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="intervalos-basados-en-percentiles-de-una-distribución-normal" class="section level3 hasAnchor" number="14.5.1">
<h3><span class="header-section-number">14.5.1</span> Intervalos basados en percentiles de una distribución normal<a href="métodos-de-computación-intensiva-el-bootstrap.html#intervalos-basados-en-percentiles-de-una-distribuci%C3%B3n-normal" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Los métodos percentil fueron ideados por Efron entre 1979 y 1987 ([7, 9, 10, 11]) quien en sucesivos trabajos fue refinando la idea original en que se basan.</p>
<p>Una forma de motivar el método es observar que los intervalos de confianza estándar son intervalos basados en los percentiles de una distribución. Consideremos <span class="math inline">\(X \sim F_{\theta}\)</span> un modelo estadístico, <span class="math inline">\(\hat{\theta}\)</span> un estimador de <span class="math inline">\(\theta\)</span>, y <span class="math inline">\(\hat{\sigma}_{\hat{\theta}}\)</span> un estimador del error estándar de <span class="math inline">\(\hat{\theta}\)</span>. El intervalo de confianza estándar que hemos presentado en el capítulo anterior era de la forma:</p>
<p><span class="math display">\[
\left[\hat{\theta}-z^{(1-\alpha)} \hat{\sigma}_{\hat{\theta}}, \hat{\theta}-z^{(\alpha)} \hat{\sigma}_{\hat{\theta}}\right]=\left[\hat{\theta}_{\mathrm{inf}}, \hat{\theta}_{\mathrm{sup}}\right]
\]</span></p>
<p>Consideremos una variable aleatoria <span class="math inline">\(\tilde{\theta}\)</span> distribuida normalmente con parámet<span class="math inline">\(\operatorname{ros} \hat{\theta}, \hat{\sigma}_{\hat{\theta}}\)</span> es decir</p>
<p><span class="math display">\[
\tilde{\theta} \sim N\left(\hat{\theta}, \hat{\sigma}_{\hat{\theta}}\right)
\]</span></p>
<p>El intervalo de confianza estándar <span class="math inline">\(\left[\hat{\theta}_{\text {inf }}, \hat{\theta}_{\text {sup }}\right]\)</span> consiste precisamente en los percentiles <span class="math inline">\(\alpha \mathrm{y}(1-\alpha)\)</span> de esta distribución, es decir</p>
<p><span class="math display">\[
\begin{aligned}
&amp; \hat{\theta}_{\text {inf }}=\tilde{\theta}^{\alpha} \\
&amp; \hat{\theta}_{\text {sup }}=\tilde{\theta}^{1-\alpha} .
\end{aligned}
\]</span></p>
<p>Ejemplo 1 Consideremos el caso de los tiempos de supervivencia de los 9 ratones del grupo. Poniendo <span class="math inline">\(\hat{\theta}=\bar{x}=86,85, \hat{\sigma}_{\hat{\theta}}=s / \sqrt{n}=25,23\)</span> y tomando <span class="math inline">\(\alpha=0,05\)</span> obtenemos el intervalo de confianza estándar:</p>
<p><span class="math display">\[
\left[\hat{\theta}_{\mathrm{inf}}, \hat{\theta}_{\mathrm{sup}}\right]=[45,3,128,4]
\]</span></p>
<p>Si hacemos 1000 replicas bootstrap y representamos su histograma vemos que se aproxima bastante a una distribución normal, es decir llamando <span class="math inline">\(\hat{\theta}^{*}=\tilde{\theta}\)</span> tenemos</p>
<p><span class="math display">\[
\hat{\theta}^{*} \dot{\sim} N\left(\hat{\theta}=86,85, \hat{\sigma}_{\hat{\theta}}=25,23\right) .
\]</span></p>
<p>El razonamiento anterior junto con una aparente normalidad sugeriría que los percentiles 5% y 95% se asemejen a los intervalos estándar. Estos valores valen:</p>
<p><span class="math display">\[
\left[\hat{\theta}_{\mathrm{inf}}^{*}, \hat{\theta}_{\mathrm{sup}}^{*}\right]=[47,3,126,4] .
\]</span></p>
<p>Es decir, si la distribución bootstrap es aproximadamente normal entonces los percentiles de ésta coinciden bastante aproximada con los extremos de los intervalos estándar. En este paralelismo se basan los métodos percentil desarrollados por Efron</p>
</div>
<div id="los-intervalos-percentil" class="section level3 hasAnchor" number="14.5.2">
<h3><span class="header-section-number">14.5.2</span> Los intervalos percentil<a href="métodos-de-computación-intensiva-el-bootstrap.html#los-intervalos-percentil" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>El ejemplo de la sección anterior sugiere como utilizar el bootstrap para construir intervalos de confianza.</p>
<p>El método percentil para construir un intervalo de confianza de coeficiente de confianza <span class="math inline">\(1-2 \alpha\)</span> consiste en tomar com extremos inferior y superior respectivamente los percentiles <span class="math inline">\(\alpha\)</span> i <span class="math inline">\(1-\alpha\)</span> de la distribución bootstrap del estimador. En general deberemos aproximar dicha distribución por Monte Carlo por lo que el intervalo percentil aproximado estará formado por los valores consistirá en los valores</p>
<p><span class="math display">\[
\left[\hat{\theta}_{\mathrm{inf}}^{*}, \hat{\theta}_{\mathrm{sup}}^{*}\right]=\left[\hat{\theta}_{(B \cdot \alpha)}^{*}, \hat{\theta}_{(B \cdot(1-\alpha))}^{*}\right]
\]</span></p>
<p>de la distribución ordenada de estadísticos bootstrap <span class="math inline">\(\hat{\theta}_{(1)}^{*}, \ldots, \hat{\theta}_{(B)}^{*}\)</span>.
Si la distribución bootstrap es normal el intervalo percentil y el estándar seran muy similares.</p>
<p>Si el estimador es asintóticamente normal, entonces para muestras grandes tambien se obtendran valores parecidos.</p>
<p>El problema se presenta cuando la muestra es pequeña o el estimador no es asintóticamente normal. Si ambos intervalos difieren ¿cual debemos utilizar?</p>
<div id="una-situación-en-la-que-podemos-saber-qué-intervalo-es-mejor" class="section level4 hasAnchor" number="14.5.2.1">
<h4><span class="header-section-number">14.5.2.1</span> Una situación en la que podemos saber qué intervalo es mejor<a href="métodos-de-computación-intensiva-el-bootstrap.html#una-situaci%C3%B3n-en-la-que-podemos-saber-qu%C3%A9-intervalo-es-mejor" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Vamos a considerar un caso en el que conocemos el intervalo exacto por lo que podremos decidir si se ajusta mejor el intervalo de confianza estándar o el percentil.</p>
<p>Sea <span class="math inline">\(X \sim N(\mu=0, \sigma=1)\)</span>, sea <span class="math inline">\(\theta=e^{\mu}\)</span>, es decir en este ejemplo <span class="math inline">\(\theta= e^{0}=1\)</span>, y el estimador máximo verosímil es <span class="math inline">\(\hat{\theta}=e^{\bar{x}}\)</span>. La distribución de <span class="math inline">\(e^{\bar{x}}\)</span> es asimé trica. Si generamos una muestra de 10 observaciones podemos comprobar como los intervalos difieren mas que en el caso anterior.</p>
<table>
<thead>
<tr class="header">
<th align="left"><span class="math inline">\(e^{\bar{x}}\)</span></th>
<th align="center">Error estándar</th>
<th align="center">I.C. estándar</th>
<th align="center">I.C. percentil</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">1.25</td>
<td align="center">0.34</td>
<td align="center"><span class="math inline">\([0,59,1,92]\)</span></td>
<td align="center"><span class="math inline">\([0,75,2,07]\)</span></td>
</tr>
</tbody>
</table>
<p>Hay una buena razón para pensar que los intervalos percentil son preferibles a los estándar: la necesaria suposición de normalidad no está en absoluto clara para <span class="math inline">\(e^{\bar{x}}\)</span> ni tan sólo si la distribución de <span class="math inline">\(X\)</span>, como en este caso es normal. Los IC estándar, que son simétricos, recubriran desigualmente la distribución.</p>
<p>Una forma de solucionar esta dificultad consiste en transformar los datos mediante una transformación normalizante, de forma que <span class="math inline">\(\hat{\phi}=g(\hat{\theta})\)</span> sea aproximadamente normal. En este caso conocemos la transformación normalizante: <span class="math inline">\(g(\theta)=\log (\theta)\)</span>. Si llevamos a cabo la transformación construimos los intervalos de confianza estándar y percentil en la escala normalizada veremos que resultan muy similares. Esto es razonable puesto que ahora trabajamos de nuevo con una distribución normal.</p>
<p>Resumiendo una forma de soslayar el problema de la falta de normalidad de <span class="math inline">\(\hat{\theta}\)</span> consiste en:</p>
<ol style="list-style-type: decimal">
<li>Transformar los datos mediante una transformación normalizadora.</li>
<li>Construir los intervalos de confianza en la escala normalizada</li>
<li>Invertir la transformación.</li>
</ol>
<p>Ejemplo 1 Simulando 10 valores como en la introducción de la sección tendremos:</p>
<table>
<thead>
<tr class="header">
<th align="center">Escala</th>
<th align="center">IC Estándar</th>
<th align="center">IC Percentil</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">Escala Original</td>
<td align="center"><span class="math inline">\([0,65,1,53]\)</span></td>
<td align="center"><span class="math inline">\([0,75,1,71]\)</span></td>
</tr>
<tr class="even">
<td align="center">Escal Normalizada</td>
<td align="center"><span class="math inline">\([-0,35,0,53]\)</span></td>
<td align="center"><span class="math inline">\([-0,28,0,54]\)</span></td>
</tr>
<tr class="odd">
<td align="center">Escala original</td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr class="even">
<td align="center">(inversión de la</td>
<td align="center"><span class="math inline">\([0,71,1,69]\)</span></td>
<td align="center"><span class="math inline">\([0,75,1,71]\)</span></td>
</tr>
<tr class="odd">
<td align="center">transformación)</td>
<td align="center"></td>
<td align="center"></td>
</tr>
</tbody>
</table>
<p>La tabla nos muestra cómo los intervalos estándar “correctos” construidos por el método de
“transformación → Intervalo → Intervalo invertido”
son mucho más parecidos a los intervalos percentil que los intervalos estándar “incorrectos” en la escala original inicial. Dicho al revés los intervalos percentil dan la solución correcta directamente sin necesidad de realizar la transformación. Además al hacer la transformación e invertir los intervalos se obtienen los mismos extremos que en la escala original. Podemos considerar que la bondad de los intervalos percentil proviene de que funcionan como si incorporasen automáticamente la transformación normalizante, dando de salida los intervalos que de otra forma requieren tres pasos para su obtención.</p>
<p>Además y aquí está la clave del interés del método, mientras que con el método general’es preciso conocer la transfoprmación no pasa así con el percentil. De hecho basta con que exista la transformación normalizante para que el método funcione.</p>
<p>Ejemplo 2 Un estudio de simulación en el que se repitió 100 veces el proceso del ejemplo anterior, tomando <span class="math inline">\(B=1000\)</span> da lugar a los siguientes intervalos “promedio”</p>
<table>
<thead>
<tr class="header">
<th align="center">Escala</th>
<th align="center">IC Estándar</th>
<th align="center">IC Percentil</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">Escala Original</td>
<td align="center"><span class="math inline">\([0,452,1,677]\)</span></td>
<td align="center"><span class="math inline">\([0,599,1,932]\)</span></td>
</tr>
<tr class="even">
<td align="center">Escal Normalizada</td>
<td align="center"><span class="math inline">\([-0,606,0,619]\)</span></td>
<td align="center"><span class="math inline">\([-0,5742,0,585]\)</span></td>
</tr>
<tr class="odd">
<td align="center">Escala original</td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr class="even">
<td align="center">(inversión de la</td>
<td align="center"><span class="math inline">\([0,580,1,995]\)</span></td>
<td align="center"><span class="math inline">\([0,599,1,932]\)</span></td>
</tr>
<tr class="odd">
<td align="center">transformación)</td>
<td align="center"></td>
<td align="center"></td>
</tr>
</tbody>
</table>
<p>Queda de nuevo claro que los intervalos percentil dan sobre la escala original unos valores similares a los que se obtienen de los intervalos estándar transformando e invirtiendo la transformación.</p>
<p>Los resultados anteriores se formalizan en lo que Efron denomina “Lema percentil”
Lema 1 Supóngase que la transformación <span class="math inline">\(\hat{\phi}=g(\hat{\theta})\)</span> normaliza perfectamente la distribución de <span class="math inline">\(\hat{\theta}\)</span>, es decir que</p>
<p><span class="math display">\[
\hat{\phi} \sim N\left(\phi, \tau^{2}\right)
\]</span></p>
<p>siendo <span class="math inline">\(\phi=g(\theta)\)</span> y <span class="math inline">\(\tau\)</span> una constante (es decir además de normalizar, la transformación estabiliza la varianza). Entonces, si <span class="math inline">\(\hat{G}()\)</span> es la distribución bootstrap de <span class="math inline">\(\hat{\theta}\)</span>, el intervalo percentil basado en los percentiles de ésta:</p>
<p><span class="math display">\[
\left[\hat{\theta}_{\mathrm{inf}}^{*}, \hat{\theta}_{\mathrm{sup}}^{*}\right]=\left[\hat{G}^{-1}(\alpha), \hat{G}^{-1}(1-\alpha)\right]
\]</span></p>
<p>coincide con el intervalo resultante de transformar la estimación e invertirla de nuevo</p>
<p><span class="math display">\[
\left[\hat{\theta}_{\text {inf }}, \hat{\theta}_{\text {sup }}\right]=\left[g^{-1}\left(\hat{\phi}-z^{(1-\alpha)} \tau\right), g^{-1}\left(\hat{\phi}-z^{(\alpha)} \tau\right)\right]
\]</span></p>
<p>con la ventaja adicional de que no es preciso conocer la transformación sino tan sólo (suponer) que existe.</p>
<p>Podemos ver los intervalos percentil como un algoritmo computacional para extender la efectividad y la utilización universal de los IC estándar:</p>
<ul>
<li>Si los IC estándar son adecuados entonces los IC percentil coincidiran con ellos.</li>
<li>Si los IC estándar no son apropiados, es decir lo serian si pudiéramos aplicar una transformación normalizante, entonces los IC percentil realizan la transformación automáticamente.</li>
</ul>
<p>La ventaja principal del método es que no es preciso conocer la forma de la transformación.</p>
</div>
<div id="la-propiedad-de-respetar-las-transformaciones" class="section level4 hasAnchor" number="14.5.2.2">
<h4><span class="header-section-number">14.5.2.2</span> La propiedad de respetar las transformaciones<a href="métodos-de-computación-intensiva-el-bootstrap.html#la-propiedad-de-respetar-las-transformaciones" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>La tabla anterior ilustra también una propiedad importante de los IC percentil que, como vimos en el capítulo anterior, no poseen los intervalos basados en el bootstrap- <span class="math inline">\(t\)</span> : Los intervalos percentil són invariantes bajo transformaciones o dicho de otra manera el método “respeta” las transformaciones: El intervalo percentil para cualquier transformación monótona del parámetro <span class="math inline">\(\phi=g(\theta)\)</span> es simplemente el intervalo percentil para <span class="math inline">\(\theta\)</span> transformado mediante <span class="math inline">\(g()\)</span> :</p>
<p><span class="math display">\[
\left[\hat{\phi}_{\mathrm{inf}}, \hat{\phi}_{\mathrm{sup}}\right]=g\left(\hat{\theta}_{\mathrm{inf}}\right), g\left(\hat{\theta}_{\mathrm{sup}}\right)
\]</span></p>
<p>Esta propiedad, interesante donde las haya no es compartida ni por el bootstrap<span class="math inline">\(t\)</span> ni por los IC estándar.</p>
<p>En la figura núm 1. puede verse de forma esquemática el funcionamiento del método percentil.</p>
<p>Este método, a pesar de su automatismo, no resulta adecuado en todas las situaciones. Es por ello que Efron fue realizando sucesivos refinamientos que dieron lugar al método percentil corregido para el sesgo o <span class="math inline">\(B C\)</span> y al método percentil corregido para el sesgo de forma acelerada, <span class="math inline">\(B C_{a}\)</span>.</p>
<p>Los tres métodos tienen en común que utilizan para calcular los intervalos de confianza los percentiles de la distribución bootstrap del estimador, <span class="math inline">\(G(\hat{\theta})\)</span>.</p>
<p>Figura 5.1: Funcionamiento del método percentil</p>
<p>Utilizaremos <span class="math inline">\(\hat{G}(\hat{\theta})\)</span> para referirnos a la aproximación por Monte Carlo de dicha distribución.</p>
<p>El método percentil incorpora de forma automática las transformación normalizante <span class="math inline">\(g\)</span>. Sucede sin embargo que hay situaciones en que no existe una transformación normalizante que transforme <span class="math inline">\(\hat{\theta}\)</span> en <span class="math inline">\(\hat{\phi} \sim N\left(\phi, \tau^{2}\right)\)</span>.</p>
<p>Una de tales situaciones se da cuando el estimador <span class="math inline">\(\hat{\theta}\)</span> presenta un cierto sesgo, como es el caso del coeficiente de correlación muestral.</p>
<p>Consideremos, p.ej., <span class="math inline">\(f_{\theta}(\hat{\theta})\)</span> la familia de densidades del coeficiente de correlación muestral en muestras de tamaño <span class="math inline">\(n=15\)</span> de una distribución normal bivariante. En este caso no existe la transformación, puesto que si así fuera se tendría que</p>
<p><span class="math display">\[
\operatorname{Prob}_{\theta}(\hat{\theta}&lt;\theta)=\operatorname{Prob}_{\phi}(\hat{\phi}&lt;\phi)=0,50,
\]</span></p>
<p>pero si para el valor <span class="math inline">\(\hat{\theta}=0,776\)</span> de nuestro ejemplo integramos la función de densidad <span class="math inline">\(f_{, 776}(\hat{\theta})\)</span> se obtiene <span class="math inline">\(\operatorname{Prob}_{\theta=, 776}(\hat{\theta}&lt;\theta)=0,431\)</span>.</p>
<p>Una explicación de este malfuncionamiento se encuentra en el hecho de que aunque existe una transformación normalizadora frecuentemente utilizada, el inverso de la tangente hiperbólica, la media del parámetro en la escala transformada, <span class="math inline">\(\mu_{\phi}\)</span> tiene la forma <span class="math inline">\(\phi+\)</span> cte</p>
<p>Para solucionar este problema Efron propuso el método del percentil corregido para el sesgo, <span class="math inline">\(B C\)</span>. Este método, como el anterior, presupone la existencia de una transformación normalizadora <span class="math inline">\(g\)</span> del parámetro <span class="math inline">\(\theta\)</span>, pero no requiere que la distribución normal resultante esté centrada en <span class="math inline">\(g(\theta)\)</span>.
theorem 1 Sea <span class="math inline">\(\theta\)</span> un parámetro que se desea estimar, <span class="math inline">\(\theta=\theta(F)\)</span> y <span class="math inline">\(\hat{\theta}=\theta\left(F_{n}\right)\)</span> un estimador de <span class="math inline">\(\theta\)</span>. Supongamos que existe una transformación monótona <span class="math inline">\(y\)</span> creciente, <span class="math inline">\(g\)</span> tal que si <span class="math inline">\(\phi=g(\theta), \hat{\phi}=g(\hat{\theta})\)</span> y <span class="math inline">\(\hat{\phi}^{*}=g\left(\hat{\theta}^{*}\right)\)</span> se verifica:</p>
<p><span class="math display">\[
\hat{\phi} \sim N\left(\phi-z_{o} \tau, \tau^{2}\right), \quad \hat{\phi}^{*} \sim N\left(\hat{\phi}-z_{o} \tau, \tau^{2}\right),
\]</span></p>
<p><span class="math inline">\(\forall \theta y\)</span> para cierta constante <span class="math inline">\(z_{o}\)</span>. Entonces para cualquier <span class="math inline">\(\alpha, 0&lt;\alpha&lt;1\)</span> el intervalo</p>
<p><span class="math display">\[
\left[\hat{G}^{-1}\left(\Phi\left\{2 z_{o}-z_{\alpha / 2}\right\}\right), \hat{G}^{-1}\left(\Phi\left\{2 z_{o}+z_{\alpha / 2}\right\}\right)\right]
\]</span></p>
<p>es un intervalo para <span class="math inline">\(\theta\)</span> al coeficiente <span class="math inline">\(1-\alpha\)</span>, siendo</p>
<p><span class="math display">\[
z_{o}=\Phi^{-1}(\hat{G}(\hat{\theta})
\]</span></p>
<p>y <span class="math inline">\(\hat{G}\)</span> la distribución bootstrap del estadístico.
Con los datos del ejemplo 2, del coeficiente de correlación podemos construir los intervalos de confianza que hemos discutido hasta el momento, obteniendose la tabla siguiente:</p>
<p>Tabla 3
Intervalos de confianza exactos y aproximados al 90 % para el coeficiente de correlación
| 1. Exactos (teoría normal) | <span class="math inline">\([.496, .898]\)</span> | <span class="math inline">\(\mathrm{D} / \mathrm{I}=0.44\)</span> |
| :— | :— | :— |
| 2. Estándar | <span class="math inline">\([.587, .965]\)</span> | <span class="math inline">\(\mathrm{D} / \mathrm{I}=1.00\)</span> |
| 3. Estándar transformados | <span class="math inline">\([.508, .907]\)</span> | <span class="math inline">\(\mathrm{D} / \mathrm{I}=.49\)</span> |
| 4. Percentil ordinario | <span class="math inline">\([.536, .911]\)</span> | <span class="math inline">\(\mathrm{D} / \mathrm{I}=.56\)</span> |
| 5. Percentil corregido <span class="math inline">\((B C)\)</span> | <span class="math inline">\([.488, .900]\)</span> | <span class="math inline">\(\mathrm{D} / \mathrm{I}=.43\)</span> |</p>
<p>El valor <span class="math inline">\(D / I\)</span> mide la simetría de los intervalos mediante el cociente entre la distancia del extremo superior hasta <span class="math inline">\(\hat{\theta}\)</span> (D) respecto a la distancia desde el extremo inferior. Como puede verse los intervalos de confianza obtenidos por el método del percentil corregido por el sesgo se asemejan mucho más a los intervalos exactos, que los obtenidos por el método percentil ordinario, o los intervalos de confianza estándar, o estándar transformados.</p>
<p>Los intervalos <span class="math inline">\(B C\)</span> efectivamente corrigen el problema anterior. Sin embargo Schenker (1985, [19]) demostró que este método funcionaba mal para la varianza de una población normal. Esto se debía a la no existencia en este caso de una transformación normalizadora con <span class="math inline">\(a=0\)</span>, es decir, que sea a la vez normalizadora y estabilizadora de la varianza. Como indica Efron (1982, [9]), estos dos objetivos son antagónicos en algunas de las familias de distribuciones más usuales (como p.ejemplo la binomial o la Poisson).</p>
<p>Esto hizo replantearse el problema a Efron quien en 1987 ([11]) construyo el método <span class="math inline">\(B C a\)</span> corregido para el sesgo y acelerado, que con la corrección introducida por una nueva constante <span class="math inline">\(a\)</span>, requiere únicamente la existencia de una transformación normalizante <span class="math inline">\(g\)</span>.
theorem 2 Sea <span class="math inline">\(\theta\)</span> un parámetro que se desea estimar, <span class="math inline">\(\theta=\theta(F)\)</span> y <span class="math inline">\(\hat{\theta}=\theta\left(F_{n}\right)\)</span> un estimador de <span class="math inline">\(\theta\)</span>. Supongamos que existe una transformación monótona <span class="math inline">\(y\)</span> creciente, <span class="math inline">\(g\)</span> tal que si <span class="math inline">\(\phi=g(\theta), \hat{\phi}=g(\hat{\theta})\)</span> y se verifica:</p>
<p><span class="math display">\[
\frac{\hat{\phi}-\phi}{\tau_{\phi}} \sim N\left(-z_{o}, 1\right), \quad \tau_{\phi}=1+a \phi
\]</span></p>
<p>para ciertas constantes <span class="math inline">\(z_{o}\)</span> y a tales que <span class="math inline">\(\tau_{\phi}&gt;0\)</span>. Entonces para cualquier <span class="math inline">\(\alpha\)</span>, <span class="math inline">\(0&lt;\alpha&lt;1\)</span> el intervalo</p>
<p><span class="math display">\[
\left[\hat{G}^{-1}\left(\Phi\left\{z_{B C_{a}}\left[\frac{\alpha}{2}\right]\right\}\right), \quad \hat{G}^{-1}\left(\Phi\left\{z_{B C_{a}}\left[1-\frac{\alpha}{2}\right]\right\}\right)\right]
\]</span></p>
<p>es un intervalo para <span class="math inline">\(\theta\)</span> al coeficiente <span class="math inline">\(1-\alpha\)</span>, siendo</p>
<p><span class="math display">\[
z_{B C_{a}}[\alpha]=z_{o}+\frac{z_{o}+z_{1-\alpha}}{1-a\left(z_{o}+z_{1-\alpha}\right)}, \quad y z_{1-\alpha}=\Phi^{-1}(\alpha)
\]</span></p>
<p>La demostración de este teorema, que no se incluye aquí, puede verse en Efron (1987, [11]) o en Cristóbal (1992, [4]).</p>
<p>Como en los casos anteriores, el intervalo de confianza 5.3 puede construirse sin el conocimiento de la transformación normalizante <span class="math inline">\(g\)</span>, únicamente aceptando que existe. Lo que sí es necesario conocer son los valores de las constantes <span class="math inline">\(z_{o}\)</span> y <span class="math inline">\(a\)</span> que se discutirán más adelante.</p>
<p>En el caso particular en que <span class="math inline">\(z_{o}=a=0\)</span> el intervalo queda reducido a</p>
<p><span class="math display">\[
\left[\hat{G}^{-1}(\alpha / 2), \hat{G}^{-1}(1-\alpha / 2)\right]
\]</span></p>
<p>es decir el percentil ordinario.
Si <span class="math inline">\(a=0\)</span> el método del <span class="math inline">\(B C_{a}\)</span> se reduce al del percentil corregido para el sesgo.</p>
<p>El método <span class="math inline">\(B C\)</span> al igual que el del percentil ordinario es automático de calcular, mientras que el método <span class="math inline">\(B C_{a}\)</span> requiere un cierto trabajo analítico para determinar una constante, denominada de aceleración. En compensación al inconveniente que esto representa el método <span class="math inline">\(B C_{a}\)</span> es el que resulta más exacto de los tres.</p>
<p>En la tabla siguiente se presentan de forma resumida los distintos intervalos. Como puede observarse al pasar de los IC estándar a los IC percentil, percentil <span class="math inline">\(B C\)</span> o percentil <span class="math inline">\(B C_{a}\)</span> aumenta la complejidad en la forma de los extremos, pero simultáneamente cada método resulta correcto bajo condiciones más generales que sus predecesores, lo cual los hace, supuestas calculables todas las constantes necesarias, de mayor aplicabilidad.</p>
<p>Tabla 4
Cuatro métodos de construcción de IC aproximados para un parámetro <span class="math inline">\(\theta\)</span>
| Cuatro métodos de construcción de IC aproximados para un parámetro <span class="math inline">\(\theta\)</span> | | | | |
| :— | :— | :— | :— | :— |
| Método | Abreviación | Extremo de nivel <span class="math inline">\(\alpha\)</span> | Es correcto si | |
| 1. Estándar | <span class="math inline">\(\theta_{E}[\alpha]\)</span> | <span class="math inline">\(\hat{\theta}+\hat{\sigma}_{\hat{\theta}} z_{\alpha}\)</span> | <span class="math inline">\(\hat{\theta} \sim N\left(\theta, \sigma^{2}\right)\)</span> | <span class="math inline">\(\sigma\)</span> constante |
| | | | Existe una transformación monótona <br> <span class="math inline">\(\hat{\phi}=g(\hat{\theta}), \phi=g(\theta)\)</span>, tal que: | |
| 2. Percentil | <span class="math inline">\(\theta_{P}[\alpha]\)</span> | <span class="math inline">\(\hat{G}^{-1}(\alpha)\)</span> | <span class="math inline">\(\hat{\phi} \sim N\left(\phi, \tau^{2}\right)\)</span> | <span class="math inline">\(\tau\)</span> constante |
| 3. <span class="math inline">\(B C\)</span> | <span class="math inline">\(\theta_{B C}[\alpha]\)</span> | <span class="math inline">\(\hat{G}^{-1}\left(\Phi\left[2 z_{o}+z_{\alpha}\right]\right)\)</span> | <span class="math inline">\(\hat{\phi} \sim N\left(\phi-z_{o} \tau, \tau^{2}\right)\)</span> | <span class="math inline">\(z_{o}, \tau\)</span> constante |
| 4. <span class="math inline">\(B C_{a}\)</span> | <span class="math inline">\(\theta_{B C_{a}}[\alpha]\)</span> | <span class="math inline">\(\hat{G}^{-1}\left(\Phi\left[z_{o}+\frac{z_{o}+z_{\alpha}}{1-a\left(z_{o}+z_{\alpha}\right)}\right]\right)\)</span> | <span class="math inline">\(\hat{\phi} \sim N\left(\phi-z_{o} \tau_{\phi}, \tau_{\phi}^{2}\right)\)</span> | <span class="math inline">\(\tau_{\phi}=1+a \phi\)</span> |
| | | | | |</p>
<p>El cálculo de la constante de aceleración <span class="math inline">\(a\)</span> es el punto débil de este método, en tanto que al tener que realizarse previo al cálculo de los intervalos, les resta automaticidad. Dicho cálculo se basa en los resultados que se presentan a continuación.
theorem 3 Sea <span class="math inline">\(\theta\)</span> un parámetro sobre el cual queremos encontrar intervalos de confianza basados en un estimador <span class="math inline">\(\hat{\theta}\)</span>, cuya función de densidad (o de
probabilidad, según el caso), es <span class="math inline">\(f_{\theta}\)</span>, derivable en <span class="math inline">\(\hat{\theta}\)</span>. Supongamos que existen transformaciones <span class="math inline">\(h_{1}, h_{2}\)</span> biyectivas y derivables tal que si <span class="math inline">\(\phi=h_{1}(\theta)\)</span> y <span class="math inline">\(\hat{\phi}=h_{2}(\hat{\theta})\)</span> se verifica</p>
<p><span class="math display">\[
\frac{\hat{\phi}-\phi}{\tau_{\phi}} \sim N\left(-z_{o}, 1\right), \quad \tau_{\phi}=1+a \phi
\]</span></p>
<p>Entonces la constante de aceleración a puede aproximarse por:</p>
<p><span class="math display">\[
\left.a \simeq \frac{1}{6} \gamma_{3}\left[\frac{\partial}{\partial \theta} \log f_{\theta}(\hat{\theta})\right]\right|_{\theta=\hat{\theta}}
\]</span></p>
<p>siendo <span class="math inline">\(\gamma_{3}(\mathbf{X})=\mu_{3}(\mathbf{X}) \mu_{2}^{-3 / 2}(\mathbf{X})\)</span> el coeficiente de simetría de <span class="math inline">\(\mathbf{X}\)</span>.
El teorema anterior resuelve el problema del cálculo de <span class="math inline">\(a\)</span> en el caso paramétrico. En el caso no paramétrico el cálculo de <span class="math inline">\(a\)</span> requiere la determinación de la función de influencia empírica de <span class="math inline">\(\hat{\theta}=T\left(F_{n}\right)\)</span> definida como:</p>
<p><span class="math display">\[
U_{i}=\lim _{\Delta \rightarrow 0} \frac{T\left((1-\Delta) F_{n}+\Delta \delta_{i}\right)-T\left(F_{n}\right)}{\Delta}, \quad i=1,2, \ldots n
\]</span></p>
<p>siendo <span class="math inline">\(\delta_{i}\)</span> una delta de Dirac en <span class="math inline">\(x_{i}\)</span>, es decir una variable aleatoria degenerada cuya función de masa de probabilidad vale 1 en <span class="math inline">\(x_{i}\)</span> y 0 en <span class="math inline">\(\mathbf{R}-\mathbf{x}_{\mathbf{i}}\)</span>.
theorem 4 En las hipótesis del teorema 1, la constante de aceleración puede aproximarse por</p>
<p><span class="math display">\[
a \simeq \frac{1}{6} \frac{\sum_{i=1}^{n} U_{i}^{3}}{\left[\sum_{i=1}^{n} U_{i}^{2}\right]^{3 / 2}}
\]</span></p>
<p>La constante <span class="math inline">\(a\)</span> se denomina de aceleración porque tiene el efecto de cambiar constantemente las unidades naturales de medida al movernos a lo largo del eje <span class="math inline">\(\phi\)</span>. Efectivamente por las hipótesis del teorema 1 podemos poner:</p>
<p><span class="math display">\[
\tau_{\phi}=\tau_{\phi_{0}}\left[1+a\left(\phi-\phi_{0}\right) / \tau_{\phi_{0}}\right]
\]</span></p>
<p>con lo que se obtiene</p>
<p><span class="math display">\[
a=\frac{d\left(\tau_{\phi} / \tau_{\phi_{0}}\right)}{d\left(\left(\phi-\phi_{0}\right) / \tau_{\phi_{0}}\right)}
\]</span></p>
<p>para cualquier valor fijo de <span class="math inline">\(\phi_{0}\)</span>. Esto muestra que <span class="math inline">\(a\)</span> mide el cambio <span class="math inline">\(\tau_{\phi}\)</span> relativo al cambio en en unidades de desviación estándar en <span class="math inline">\(\phi\)</span>.</p>
</div>
<div id="ejemplo-ic-para-la-varianza-de-una-población-normal" class="section level4 hasAnchor" number="14.5.2.3">
<h4><span class="header-section-number">14.5.2.3</span> Ejemplo: IC para la varianza de una población normal<a href="métodos-de-computación-intensiva-el-bootstrap.html#ejemplo-ic-para-la-varianza-de-una-poblaci%C3%B3n-normal" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Schenker (1985, [19]) cita un ejemplo tomado de una población normal de parámetros conocidos en donde se ve como el método <span class="math inline">\(B C\)</span> no funciona correctamente. Si en este caso se calcula el intervalo <span class="math inline">\(B C_{a}\)</span> con valores <span class="math inline">\(z_{o}=\)</span> 0,1082 y <span class="math inline">\(a=0,1077\)</span> se obtiene una aproximación mucho mejor a los intervalos exactos, que podemos calcular aquí por el hecho de encontrarnos en una situación paramétrica conocida.</p>
<p>Tabla 5
| Intervalos de confianza exactos y aproximados al 90 % para la varianza de una población normal | |
| :— | :— |
| 1. Exactos (teoría normal) | <span class="math inline">\(\left[, 631 S^{2}, 1,88 S^{2}\right]\)</span> |
| 2. Basados en la verosimilitud | <span class="math inline">\(\left[, 466 S^{2}, 1,53 S^{2}\right]\)</span> |
| 3. Percentil corregido ( <span class="math inline">\(B C\)</span> ) | [,580S <span class="math inline">\({ }^{2}\)</span>, <span class="math inline">\(1,69 S^{2}\)</span> ] |
| 4. Percentil acelerado ( <span class="math inline">\(B C_{a}\)</span> ) | , <span class="math inline">\(630 S^{2}\)</span>, <span class="math inline">\(1,88 S^{2}\)</span> ] |</p>
</div>
</div>
<div id="ejercicios-2" class="section level3 hasAnchor" number="14.5.3">
<h3><span class="header-section-number">14.5.3</span> Ejercicios<a href="métodos-de-computación-intensiva-el-bootstrap.html#ejercicios-2" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ol style="list-style-type: decimal">
<li>Sea <span class="math inline">\(X\)</span> una v.a. con media <span class="math inline">\(\theta\)</span> i desviación estándar <span class="math inline">\(s(\theta)\)</span>. Sea <span class="math inline">\(g(x)\)</span> una transformación aplicada sobre <span class="math inline">\(X\)</span>.</li>
</ol>
<ol style="list-style-type: lower-alpha">
<li>Expanda <span class="math inline">\(g(X)\)</span> en serie de Taylor para mostrar que:</li>
</ol>
<p><span class="math display">\[
\operatorname{var}(g(X)) \simeq g^{\prime}(\theta)^{2} \operatorname{var}(X)
\]</span></p>
<ol start="2" style="list-style-type: lower-alpha">
<li>Mostrar que la transformación</li>
</ol>
<p><span class="math display">\[
g(x)=\int^{x} \frac{1}{s(u)} d u
\]</span></p>
<p>tiene la propiedad <span class="math inline">\(\operatorname{var}(g(X)) \simeq\)</span> constante.
2. Supóngase <span class="math inline">\(X_{i} / \theta\)</span> son i.i.d. <span class="math inline">\(\simeq \chi_{1}^{2}, i=1, \ldots, n\)</span> y que <span class="math inline">\(\hat{\theta}=\bar{X}\)</span>. Mostrar que la transformación estabilizadora de la varianza para <span class="math inline">\(\hat{\theta}\)</span> es:</p>
<p><span class="math display">\[
g(\hat{\theta})=(n / 2)^{1 / 2} \log \hat{\theta}
\]</span></p>
<ol start="3" style="list-style-type: decimal">
<li>Supóngase <span class="math inline">\(X_{i} / \theta\)</span> son i.i.d. <span class="math inline">\(\simeq \chi_{1}^{2}, i=1, \ldots, 20\)</span>. Realizar un estudio de simulación para comparar los siguientes intervalos para <span class="math inline">\(\theta\)</span> basados en <span class="math inline">\(\bar{X}\)</span> suponiendo que el auténtico valor de <span class="math inline">\(\theta\)</span> és 1 .</li>
</ol>
</div>
<div id="prácticas" class="section level3 hasAnchor" number="14.5.4">
<h3><span class="header-section-number">14.5.4</span> Prácticas<a href="métodos-de-computación-intensiva-el-bootstrap.html#pr%C3%A1cticas" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>El objetivo de las prácticas siguientes es estudiar el comportamiento de los distintos métodos de construcción de intervalos de confianza bajo condiciones diferentes.</p>
<p>Para facilitar el cálculo conviene disponer de una función que construya los IC estándar y percentil de forma automática. Por ejemplo las funciones ICStd() y ICPerc()realizan estos cálculos</p>
<ol style="list-style-type: decimal">
<li>Comparación de los IC estándar y percentil cuando la distribución del estimador es normal: Tomamos como parámetro la media, <span class="math inline">\(\mu, \mathrm{y}\)</span> como estimador la media aritmética, <span class="math inline">\(\bar{X}\)</span></li>
<li><ol style="list-style-type: lower-alpha">
<li>Generar muestras de tamaño 10 de una distribución normal <span class="math inline">\(\mathrm{N}(10,2)\)</span></li>
</ol></li>
</ol>
<ol start="2" style="list-style-type: lower-alpha">
<li>Estimar el EE como <span class="math inline">\(\operatorname{sqrt}(\operatorname{var}(\mathrm{x}) / \mathrm{n})\)</span></li>
<li>Remuestrear con <span class="math inline">\(\mathrm{B}=1000\)</span></li>
<li>Representar la distribución bootstrap. ¿Parece normal?
<span class="math inline">\(e\)</span> ) Construir el IC estándar y el IC percentil</li>
</ol>
<ol start="3" style="list-style-type: decimal">
<li>Comparación de los IC estándar y percentil cuando la distribución del estimador no es normal: Tomamos como parámetro <span class="math inline">\(\exp \{\mu\}\)</span> y como estimador (el estimador máximo verosímil) <span class="math inline">\(\exp \{\bar{X}\}\)</span>. El error estándar asintótico de este estimador es la raíz cuadrada de la inversa de la información de Fisher del modelo y vale <span class="math inline">\(\sigma_{\hat{\theta}}=\exp \{\bar{X}\} \sigma / \sqrt{n}\)</span> y puede estimarse como <span class="math inline">\(\hat{\sigma}_{\hat{\theta}}=\exp \{\bar{X}\} s / \sqrt{n}\)</span></li>
<li><ol style="list-style-type: lower-alpha">
<li>Generar muestras de tamaño 10 de una distribución normal <span class="math inline">\(\mathrm{N}(0,1)\)</span></li>
</ol></li>
</ol>
<ol start="2" style="list-style-type: lower-alpha">
<li>Estimar el EE como <span class="math inline">\(\exp \{\bar{X}\} s / \sqrt{n}\)</span></li>
<li>Remuestrear con <span class="math inline">\(\mathrm{B}=1000\)</span></li>
<li>Representar la distribución bootstrap. ¿Parece normal?
<span class="math inline">\(e\)</span> ) Construir el IC estándar y el IC percentil. ¿Cual de los 2 resulta más fiable?.</li>
</ol>
<ol start="5" style="list-style-type: decimal">
<li>Para responder a la pregunta anterior podemos normalizar la distribución del estimador a través de la transformación <span class="math inline">\(\lg (\exp (\bar{X}))\)</span></li>
<li><ol style="list-style-type: lower-alpha">
<li>Generar muestras de tamaño 10 de una distribución normal <span class="math inline">\(\mathrm{N}(0,1)\)</span></li>
</ol></li>
</ol>
<ol start="2" style="list-style-type: lower-alpha">
<li>Estimar el EE como <span class="math inline">\(\exp \{\bar{X}\} s / \sqrt{n}\)</span></li>
<li>Remuestrear con <span class="math inline">\(\mathrm{B}=1000\)</span></li>
<li>Construir el IC estándar y el IC percentil.
<span class="math inline">\(e\)</span> ) Transformar la distribución bootstrap tomando logaritmos neperianos. Representar la distribución transformada. Parece normal?
<span class="math inline">\(f)\)</span> Construir el IC estándar y el IC percentil. Para el IC estándar tómese el error estándar usual de la media, o, si se prefiere, tómese dicho error con <span class="math inline">\(\mathrm{s}=1\)</span>.</li>
<li>Inviértase la transformación. Los intervalos resultantes de invertir los IC estándar deberian ser ahora correctos puesto que provienen de una distribución normal. Que ha sucedido ahora con los IC percentil?.</li>
</ol>
<ol start="7" style="list-style-type: decimal">
<li>Repita un estudio similar al anterior para el coeficiente de correlación tomando como transformación normalizante la transformación <span class="math inline">\(z\)</span> de Fisher: <span class="math inline">\(\arctan h(\rho)\)</span></li>
<li>Para ver que tan bien funciona un intervalo de confianza podemos plantear un estudio de simulación en donde se compare el recubrimiento real del IC con el nominal. Para ello debemos comparar el% de veces que el IC contiene al parámetro “real”, conocido por tratarse de una simulación, con el nivel de confianza nominal, con el que se ha construido teóricamente el intervalo. Otra característica interesante de estudiar es la forma del IC. Para ello podemos calcular el cociente “Brazo Derecho/Brazo Izquierdo” definido como :
“(Extremo superior-Valor del parámetro)/(Valor del parámetro-Extremo inferior)”
Realizar un programa que compare el funcionamiento de dos métodos de construcción de intervalos de confianza basandose en el algoritmo siguiente:</li>
<li><ol style="list-style-type: lower-alpha">
<li>Repetir <span class="math inline">\(1 \ldots \mathrm{~S}\)</span> veces:</li>
</ol></li>
</ol>
<ol start="2" style="list-style-type: lower-alpha">
<li>Generar una muestra de una población de distribución conocida</li>
<li>Para cada intervalo de confianza que se estudia:</li>
</ol>
<ol style="list-style-type: decimal">
<li>Construir el IC</li>
<li>Determinar si cubre (1) o no (0) el parámetro y acumular el resultado</li>
<li>Calcular la forma (D/I) y acumular el resultado</li>
<li>Calcular la longitud (D-I) y acumular el resultado</li>
</ol>
<ol start="4" style="list-style-type: lower-alpha">
<li>Estimar el recubrimiento, la longitud y la potencia de los intervalos mediante el promedio de las cantidades acumuladas anteriores</li>
</ol>
<p>Los estudios de simulación ayudan a decidir qué método resulta más adecuado aún cuando no se conozca el resultado exacto (no se disponga de resultados teóricos con los que comparar). Tomando los datos de 20 pacientes afectados de SIDA (variable CD4 del dataframe cd4 en la libreria) queremos estudiar las propiedades del estimador del mayor valor propio de la matriz de covarianzas. Deseamos:</p>
<ol style="list-style-type: decimal">
<li>Una aproximación de su distribución muestral. Es normal?</li>
<li>Una estimación del error estándar</li>
<li>Un intervalo de confianza. ¿Qué metodo podemos elegir?. ¿Que argumentos estan a favor de los IC estandar?, del bootstrap-t, de los métodos percentil?. Realice un estudio de simulación para comparar los métodos competitivos y decida cual resultará más adecuado.</li>
</ol>
</div>
</div>
<div id="contraste-de-hipótesis-mediante-bootstrap" class="section level2 hasAnchor" number="14.6">
<h2><span class="header-section-number">14.6</span> Contraste de hipótesis mediante bootstrap<a href="métodos-de-computación-intensiva-el-bootstrap.html#contraste-de-hip%C3%B3tesis-mediante-bootstrap" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="introducción-12" class="section level3 hasAnchor" number="14.6.1">
<h3><span class="header-section-number">14.6.1</span> Introducción<a href="métodos-de-computación-intensiva-el-bootstrap.html#introducci%C3%B3n-12" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Muchas aplicaciones estadísticas implican la realización de contrastes o pruebas de significación para establecer la plausibilidad de una hipótesis científica. Distintos métodos de remuestreo, como los tests de permutaciones o los tests de Monte Carlo ya se utilizaban para la elaboración de contrastes antes de la aparición del bootstrap. En primer lugar se revisan algunos de los conceptos básicos sobre contraste de hipótesis</p>
<div id="revisión-de-conceptos-básicos-de-contraste-de-hipótesis" class="section level4 hasAnchor" number="14.6.1.1">
<h4><span class="header-section-number">14.6.1.1</span> Revisión de conceptos básicos de contraste de hipótesis<a href="métodos-de-computación-intensiva-el-bootstrap.html#revisi%C3%B3n-de-conceptos-b%C3%A1sicos-de-contraste-de-hip%C3%B3tesis" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>En un contraste de hipótesis se desea aceptar o rechazar una hipótesis nula que especifica cierta clase de distribuciones para los datos <span class="math inline">\(\mathcal{F}_{\mathcal{H}}\)</span>. Un contraste de hipótesis se basa en un estadístico de test <span class="math inline">\(T=T\left(X_{1}, X_{2}, \ldots, X_{n}\right)\)</span> que mide la discrepancia entre los datos, <span class="math inline">\(\left(X_{1}, X_{2}, \ldots, X_{n}\right)\)</span>, y la hipótesis nula. En general valores grandes de <span class="math inline">\(T\)</span> son evidencia en contra de <span class="math inline">\(H_{0}\)</span>. Si el valor observado del estadístico de test es <span class="math inline">\(t\)</span> el nivel de evidencia en contra de <span class="math inline">\(H_{0}\)</span> se mide por la probabilidad de significación:</p>
<p><span class="math display">\[
p=P_{H_{0}}\{T \geq t\}=P\left\{T \geq t \mid H_{0}\right\}=P\left\{T \geq t \mid F \in \mathcal{F}_{\mathcal{H}_{1}}\right\}
\]</span></p>
<p>también llamada <span class="math inline">\(p\)</span>-valor o NSA por Nivel de Significación Alcanzado. Cuanto menor sea el p-valor mayor será pués la evidencia en contra de <span class="math inline">\(H_{0}\)</span>. El test de hipótesis consiste en calcular el p-valor y ver si es demasiado pequeño según ciertos criterios “usuales”. En general si <span class="math inline">\(N S A&gt;0,10\)</span> se aceptará <span class="math inline">\(H_{0}\)</span>, si
<span class="math inline">\(N S A&lt;0,01\)</span> se rechazará y para valores intermedios la decisión dependerá del decisor. El punto de corte entre aceptar o rechazar la hipótesis recibe el nombre de nivel de significación del test y por lo tanto podemos resumir diciendo que se rechazará la hipótesis nula si <span class="math inline">\(N S A&lt;\alpha\)</span>.</p>
<p>En la elaboración de un test de hipótesis hay dos aspectos a tener en cuenta:</p>
<ul>
<li>la elección del estadístico de test y</li>
<li>el calculo del p-valor</li>
</ul>
</div>
</div>
</div>
<div id="elección-del-estadístico-de-test" class="section level2 hasAnchor" number="14.7">
<h2><span class="header-section-number">14.7</span> Elección del estadístico de test<a href="métodos-de-computación-intensiva-el-bootstrap.html#elecci%C3%B3n-del-estad%C3%ADstico-de-test" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>En problemas paramétricos la distribución de los datos está completamente especificada, salvo por un número finito de parámetros desconocidos. Hay una hipótesis alternativa <span class="math inline">\(H_{1}\)</span> que describe qué alternativas a la hipótesis nula se pretenden detectar o se suponen más probables, en el caso de que ésta sea falsa. Esta hipótesis alternativa suele guiar la elección del estadístico de test, generalmente a través de la función de verosimilitud</p>
<p><span class="math display">\[
L(\underset{\sim}{X} ; \theta)=f_{X_{1}, X_{2}, \ldots, X_{n}}\left(x_{1}, x_{2}, \ldots, x_{n} \mid \theta\right) .
\]</span></p>
<p>Por ejemplo si ambas hipótesis son simples, <span class="math inline">\(H_{1}: \theta=\theta_{0}, \quad H_{1}: \theta=\theta_{1}\)</span>, entonces el mejor estadístico de test se obtiene a través del lema de NeymannPearson, de la razón de verosimilitudes:</p>
<p><span class="math display">\[
T=L\left(\underset{\sim}{X} ; \theta_{1}\right) / L\left(\underset{\sim}{X} ; \theta_{0}\right) .
\]</span></p>
<p>Si las hipótesis són compuestas la solución puede todavía basarse en la verosimilitud, a través, por ejemplo, del test de razón de verosimilitud.</p>
<p>En el caso no paramétrico la elección del estadístico de test es menos obvia pero también debería basarse en mayor o menor grado en lo que ocurre cuando la hipótesis nula es falsa</p>
</div>
<div id="cálculo-del-p-valor-1" class="section level2 hasAnchor" number="14.8">
<h2><span class="header-section-number">14.8</span> Cálculo del p-valor<a href="métodos-de-computación-intensiva-el-bootstrap.html#c%C3%A1lculo-del-p-valor-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>En muchos casos la hipótesis nula es compuesta. Por ejemplo <span class="math inline">\(H_{0}: X\)</span> sigue una distribución Normal, <span class="math inline">\(\left(\mathcal{F}_{\mathcal{H},}=\{\mathcal{F} \mid \mathcal{F} \sim \mathcal{N}(\mu, \sigma)\}\right.\)</span>. Esto deja parámetros desconocidos y por lo tanto <span class="math inline">\(F\)</span> no está completamente especificada, con lo cual el p-valor no estará bien definido, dado que <span class="math inline">\(P\left\{T \geq t \mid F \in \mathcal{F}_{\mathcal{H}}\right\}\)</span> puede depender de cual <span class="math inline">\(F\)</span> de entre todas las que satisfacen la hipótesis nula se tome. Hay distintas soluciones posibles:</p>
<ol style="list-style-type: decimal">
<li>Podemos escoger <span class="math inline">\(T\)</span> de forma que tenga la misma distribución <span class="math inline">\(\forall F \in \mathcal{F}_{\mathcal{H}}\)</span>. Un ejemplo és el test de comparación de medias en poblaciones normales con varianza, común, desconocida. Bajo la hipótesis nula <span class="math inline">\(\mu_{1}= \mu_{2}\)</span> estadístico de test tiene una distribución t-Student que no depende de los valores de <span class="math inline">\(\mu_{1}\)</span> ni <span class="math inline">\(\mu_{2}\)</span>.</li>
<li>Otra posibilidad es eliminar los parámetros desconocidos cuando <span class="math inline">\(H_{0}\)</span> es cierta. En algúnos casos esto puede hacerse condicionando por un estadístico suficiente bajo <span class="math inline">\(H_{0}\)</span>. Un ejemplo de es el test exacto de Fisher para contrastar la igualdad entre dos distribuciones. Dadas 2 muestras</li>
</ol>
<p><span class="math display">\[
Y_{1}, \ldots, Y_{n} \sim F_{Y}, Z_{1}, \ldots, Z_{m} \sim G_{Z}
\]</span></p>
<p>y la muestra conjunta resultante de combinarlas:</p>
<p><span class="math display">\[
X_{1}, X_{2}, \ldots, X_{N}, N=n+m
\]</span></p>
<p>Fisher demostró que bajo la hipótesis nula <span class="math inline">\(H_{0}: F_{Y}=G_{Z}\)</span> el estadístico de orden</p>
<p><span class="math display">\[
X_{(1)}, X_{(2)}, \ldots, X_{(N)}
\]</span></p>
<p>es suficiente para <span class="math inline">\(F, G\)</span> y la probabilidad de una muestra dado este estadístico es</p>
<p><span class="math display">\[
P_{F_{Y}=G_{Z}}\left\{X_{1}, X_{2}, \ldots, X_{N} \mid X_{(1)}=x_{(1)}, X_{(2)}=x_{(2)}, \ldots, X_{(N)}=x_{(N)}\right\}=\frac{1}{n!} .
\]</span></p>
<p>A partir de este resultado, dado un estadístico de test <span class="math inline">\(T\)</span>, plantea un test condicional que permite estimar el p -valor enumerando todas las muestras y contando en cuáles el valor del estadístico de test es superior al valor observado en la muestra, <span class="math inline">\(t_{o b s}\)</span> :</p>
<p><span class="math display">\[
\begin{aligned}
P_{F_{Y}=G_{Z}}\left\{T \geq t_{o b s} \mid x_{(1)}, x_{(2)}, \ldots, x_{(N)}\right\} &amp; =\sum_{\left\{x_{1}, x_{2}, \ldots, x_{N} \mid T&gt;t\right\}} \frac{1}{n!}= \\
&amp; =\frac{\left\{\# \text { ordenaciones de } x_{1}, x_{2}, \ldots, x_{N}: T \geq t_{o b s}\right\}}{n!} .
\end{aligned}
\]</span></p>
<p>Si, por ejemplo, tomamos como estadístico de test la diferencia (en valor absoluto) entre las medias de las muestras el test consiste en determinar en cuántas de ellas la diferencia es mayor que la diferencia observada sobre la muestra. Si la hipótesis nula es cierta es de esperar que un elevado número de muestras lo cumplan, mientras que si no lo es habra pocas, tantas menos cuanto mas distintas sean ambas muestras.</p>
<div id="contrastes-de-hipótesis-bootstrap" class="section level3 hasAnchor" number="14.8.1">
<h3><span class="header-section-number">14.8.1</span> Contrastes de hipótesis bootstrap<a href="métodos-de-computación-intensiva-el-bootstrap.html#contrastes-de-hip%C3%B3tesis-bootstrap" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Del planteamiento realizado en el apartado anterior queda claro que al realizar un contraste de hipótesis el p-valor debe calcularse bajo la hipótesis nula.</p>
<p>Las dos opciones presentadas son soluciones clásicas aplicadas en gran número de contrastes, especialmente cuando la distribución de la poblaciones es normal. Los tests bootstrap no suelen basarse ni en la primera opción ni en segunda sinó que explotan una tercera posibilidad consistente en estimar <span class="math inline">\(F\)</span> mediante una función de distribución <span class="math inline">\(\hat{F}_{0}\)</span> que satisface la hipótesis nula, es decir una función de distribución que verifique <span class="math inline">\(\hat{F}_{0} \in \mathcal{F}_{\mathcal{H}}\)</span>. Si llamamos <span class="math inline">\(t_{o b s}\)</span> al valor observado del estadístico de test sobre la muestra original, el p-valor bootstrap se define como:</p>
<p><span class="math display">\[
p^{*}=P\left\{T\left({\underset{\sim}{X}}^{*}\right) \geq t_{o b s} \mid \hat{F}_{0}\right\}=P_{\hat{F}_{0}}\left\{T\left({\underset{\sim}{X}}^{*}\right) \geq t_{o b s}\right\}
\]</span></p>
<p>donde <span class="math inline">\({\underset{\sim}{X}}^{*}\)</span> indica que las <span class="math inline">\(X_{i}^{*}\)</span> son remuestras independientes de <span class="math inline">\(\hat{F}_{0}\)</span>.</p>
<div id="ejemplos" class="section level4 hasAnchor" number="14.8.1.1">
<h4><span class="header-section-number">14.8.1.1</span> Ejemplos<a href="métodos-de-computación-intensiva-el-bootstrap.html#ejemplos" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Presentamos algunos ejemplos con problemas básicos como hipótesis sobre medias o varianzas que permiten la comparación de los métodos bootstrap con procedimientos familiares.</p>
</div>
</div>
</div>
<div id="ejemplo-1-comparación-de-medias" class="section level2 hasAnchor" number="14.9">
<h2><span class="header-section-number">14.9</span> Ejemplo 1: Comparación de medias<a href="métodos-de-computación-intensiva-el-bootstrap.html#ejemplo-1-comparaci%C3%B3n-de-medias" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Supongamos que tenemos 2 muestras independientes que indicamos por <span class="math inline">\(X_{11}, X_{12}, \ldots, X_{1 n}\)</span> y <span class="math inline">\(X_{21}, X_{22}, \ldots, X_{2 m}\)</span> y que deseamos contrastar la hipótesis <span class="math inline">\(H_{0}: \mu_{1}=\mu_{2}\)</span>. Aquí intervienen dos distribuciones, <span class="math inline">\(F, G\)</span> con sus correspondientes aproximaciones empíricas, <span class="math inline">\(\hat{F}, \hat{G}\)</span>. Hay distintas elecciones possibles para <span class="math inline">\(\hat{F}_{0}=\left(\hat{F}_{H_{0}}, \hat{G}_{H_{0}}\right):\)</span></p>
<ul>
<li>Una posibilidad es “forzar” que las medias de <span class="math inline">\(\hat{F}_{H_{0}}\)</span> y <span class="math inline">\(\hat{G}_{H_{0}}\)</span> sean iguales. Esta posibilidad se muestra con más detalle a continuación.</li>
<li>Otra posibilidad es igualar ambas distribuciones empíricas a la de la muestra combinada es decir <span class="math inline">\(\hat{F}_{0}=\left(\hat{F}_{\text {comb }}, \hat{F}_{\text {comb }}\right)\)</span>, donde <span class="math inline">\(\hat{F}_{\text {comb }}\)</span> asigna probabilidad <span class="math inline">\(1 / N, N=n+m\)</span> a cada valor observado.</li>
</ul>
<p>Como estadístico de test puede utilizarse la diferencia de medias, <span class="math inline">\(\bar{X}_{1}-\bar{X}_{2}\)</span>, o un estadístico estudentizado como el estadístico <span class="math inline">\(t\)</span> de Student habitual para la comparación de dos medias:</p>
<p><span class="math display">\[
T(\underset{\sim}{X})=\frac{\bar{X}_{1}-\bar{X}_{2}}{\tilde{\sigma} \sqrt{1 / n+1 / m}},
\]</span></p>
<p><span class="math inline">\(\operatorname{con} \tilde{\sigma}=\left\{\left[n \cdot s_{1}^{2}+m \cdot s_{2}^{2}\right] /[n+m-2]\right\}^{1 / 2}\)</span>.
El p-valor a calcular es:</p>
<p><span class="math display">\[
p=\operatorname{Prob}\left\{T(\underset{\sim}{X}) \geq t_{o b s} \mid \mu_{1}=\mu_{2}\right\} .
\]</span></p>
<p>El planteamiento bootstrap de este problema consistirá en estimar este p -valor, <span class="math inline">\(p\)</span>, mediante el p -valor bootstrap, <span class="math inline">\(p^{*}\)</span></p>
<p><span class="math display">\[
p^{*}=\operatorname{Prob}_{*}\left\{T\left(\mathbf{Y}^{*}\right) \geq t_{o b s} \mid \bar{Y}_{1}=\bar{Y}_{2}\right\}
\]</span></p>
<p>en donde el requisito de estimar la probabilidad bajo la hipótesis nula, es decir como sobre una “población” (“mundo real”) en donde <span class="math inline">\(\mu_{1}=\mu_{2}\)</span> se realiza substituyendo dicha población por la “muestra” (“mundo bootstrap”), <span class="math inline">\(\tilde{\mathbf{Y}}\)</span> que se ajusta a la hipótesis <span class="math inline">\(\bar{Y}_{1}=\bar{Y}_{2}(=\bar{X})\)</span> donde</p>
<p><span class="math display">\[
\bar{X}=\frac{m \bar{X}_{1}+n \bar{X}_{2}}{m+n}
\]</span></p>
<p>y donde las remuestras <span class="math inline">\(X^{*}\)</span> pueden generarse a partir de <span class="math inline">\(\widehat{F}_{H_{0}}\)</span> que asigna probabilidad <span class="math inline">\(\frac{1}{m}\)</span> y <span class="math inline">\(\frac{1}{n}\)</span> respectivamente a los valores</p>
<p><span class="math display">\[
\left(Y_{11} \ldots Y_{1 m} ; Y_{21} \ldots Y_{2 n}\right)
\]</span></p>
<p>obtenidos de <span class="math inline">\(Y_{i j}=X_{i j}-\bar{X}_{i}+\bar{X}\)</span>. Como puede comprobarse en la tabla siguiente esta transformación de los datos da lugar a dos nuevas muestras cuyas varianzas muestrales son iguales a las de la muestra inicial y cuyas medias aritméticas coinciden y son iguales a <span class="math inline">\(\bar{X}\)</span>.</p>
<p>El p-valor bootstrap <span class="math inline">\(p^{*}\)</span> se puede aproximar mediante el siguiente algoritmo de Monte Carlo, en donde se observa que aquí, el remuestreo bajo la hipótesisi nula consiste en extraer muestras de <span class="math inline">\(\widehat{F}_{H_{0}}\)</span> por separado en la primera y la segunda submuestras.</p>
<ol style="list-style-type: decimal">
<li>Generar <span class="math inline">\(b=1, \ldots, B\)</span> pares de remuestras a partir de <span class="math inline">\(\widehat{F}_{H_{0}}\)</span> definida en el párrafo anterior y calcular sobre cada una de ellas el estadístico de test
<span class="math inline">\(T^{*}\)</span></li>
</ol>
<p><span class="math display">\[
\begin{array}{cccc}
\left(Y_{11} \ldots Y_{1 m} ; Y_{21} \ldots Y_{2 n}\right) &amp; &amp; \\
\Downarrow &amp; \Downarrow &amp; &amp; \\
&amp; &amp; &amp; \\
\left(Y_{11}^{*} \ldots Y_{1 m}^{*} ; Y_{21}^{*} \ldots Y_{2 n}^{*}\right)_{1} &amp; \rightarrow &amp; T_{1}^{*} \\
\left(Y_{11}^{*} \ldots Y_{1 m}^{*}\right. &amp; \left.; Y_{21}^{*} \ldots Y_{2 n}^{*}\right)_{2} &amp; \rightarrow &amp; T_{2}^{*} \\
\ldots &amp; \ldots &amp; \ldots &amp; \\
\left(Y_{11}^{*} \ldots Y_{1 m}^{*}\right. &amp; \left.; Y_{21}^{*} \ldots Y_{2 n}^{*}\right)_{B} &amp; \rightarrow &amp; T_{B}^{*}
\end{array}
\]</span></p>
<ol start="2" style="list-style-type: decimal">
<li>Aproximar el p -valor bootstrap <span class="math inline">\(p^{*}\)</span> mediante</li>
</ol>
<p><span class="math display">\[
\hat{p}^{*}=\frac{\#\left\{T_{b}^{*} \geq t_{o b s}\right\}}{B} \approx p^{*}
\]</span></p>
<ol start="3" style="list-style-type: decimal">
<li>Si <span class="math inline">\(\hat{p}^{*} \leq \alpha\)</span> se rechaza la hiopótesis nula.</li>
</ol>
</div>
<div id="ejemplo-2-prueba-de-independencia" class="section level2 hasAnchor" number="14.10">
<h2><span class="header-section-number">14.10</span> Ejemplo 2: Prueba de independencia<a href="métodos-de-computación-intensiva-el-bootstrap.html#ejemplo-2-prueba-de-independencia" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Supongase que observamos <span class="math inline">\(n\)</span> pares de valores</p>
<p><span class="math display">\[
(\underset{\sim}{X} ; \underset{\sim}{Y})=\left(\begin{array}{cc}
X_{1} &amp; Y_{1} \\
X_{2} &amp; Y_{2} \\
\ldots &amp; \ldots \\
X_{n} &amp; Y_{n}
\end{array}\right)
\]</span></p>
<p>y que deseamos contrastar la independencia entre <span class="math inline">\(X\)</span> e <span class="math inline">\(Y\)</span> :</p>
<p><span class="math display">\[
H_{0}: X \quad \text { e } \quad Y \text { son estocásticamente independientes. }
\]</span></p>
<p>Es razonable utilizar un estadístico de test que no tan sólo permita calcular el p-valor sino que cuando la hipótesis nula no se verifique también lo indique. Una posibilidad es basar el estadístico de test en el coeficiente de correlación muestral. Algunas elecciones posibles para <span class="math inline">\(T\)</span> son:</p>
<ul>
<li><span class="math inline">\(T(\underset{\sim}{X} ; \underset{\sim}{Y})=|r(\underset{\sim}{X} ; \underset{\sim}{Y})|\)</span>, el coeficiente de correlación muestral.</li>
<li><span class="math inline">\(T(\underset{\sim}{X} ; \underset{\sim}{Y})=\left|\frac{r}{\sqrt{1-r^{2}}} \sqrt{n-2}\right|\)</span>, una versión estudentizada del anterior, cuya distribución -si las poblaciones son normales- no depende de <span class="math inline">\(\rho\)</span>, es decir que es pivotal.</li>
<li><span class="math inline">\(T(\underset{\sim}{X} ; \underset{\sim}{Y})=\sup _{\left(X_{i}, Y_{i}\right)}\left|\hat{F}-\hat{G}_{1} \hat{G}_{2}\right|\)</span> basado en la diferencia entre la la función de distribucion empírica de la muestra original y la función de distribución bajo la hipótesis nula que seria el producto de la funciones de distribución marginales.</li>
</ul>
<p>El planteamiento bootstrap del problema se basa en la idea utilizada en el tercer estadístico. Si indicamos por <span class="math inline">\(\hat{F}\)</span> la función de distribución empírica de la muestra original:</p>
<table>
<colgroup>
<col width="11%" />
<col width="13%" />
<col width="13%" />
<col width="11%" />
<col width="11%" />
<col width="11%" />
<col width="13%" />
<col width="13%" />
</colgroup>
<thead>
<tr class="header">
<th align="left"><span class="math inline">\(\hat{F}\)</span></th>
<th align="center"><span class="math inline">\(Y_{(1)}\)</span></th>
<th align="center"><span class="math inline">\(Y_{(2)}\)</span></th>
<th align="left">.</th>
<th align="left">.</th>
<th align="left">.</th>
<th align="center"><span class="math inline">\(Y_{(n)}\)</span></th>
<th align="center"><span class="math inline">\(\hat{G}_{1}\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><span class="math inline">\(X_{(1)}\)</span></td>
<td align="center">0</td>
<td align="center"><span class="math inline">\(\frac{1}{n}\)</span></td>
<td align="left">.</td>
<td align="left">.</td>
<td align="left">.</td>
<td align="center">0</td>
<td align="center"><span class="math inline">\(\frac{1}{n}\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(X_{(2)}\)</span></td>
<td align="center">0</td>
<td align="center">0</td>
<td align="left">.</td>
<td align="left">.</td>
<td align="left">.</td>
<td align="center"><span class="math inline">\(\frac{1}{n}\)</span></td>
<td align="center"><span class="math inline">\(\frac{1}{n}\)</span></td>
</tr>
<tr class="odd">
<td align="left">.</td>
<td align="center">.</td>
<td align="center">.</td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="center">.</td>
<td align="center">.</td>
</tr>
<tr class="even">
<td align="left">.</td>
<td align="center">.</td>
<td align="center">.</td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="center">.</td>
<td align="center">.</td>
</tr>
<tr class="odd">
<td align="left">.</td>
<td align="center">.</td>
<td align="center">.</td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="center">.</td>
<td align="center">.</td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(X_{(n)}\)</span></td>
<td align="center"><span class="math inline">\(\frac{1}{n}\)</span></td>
<td align="center">0</td>
<td align="left">.</td>
<td align="left">.</td>
<td align="left">.</td>
<td align="center">0</td>
<td align="center"><span class="math inline">\(\frac{1}{n}\)</span></td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(\hat{G}_{2}\)</span></td>
<td align="center"><span class="math inline">\(\frac{1}{n}\)</span></td>
<td align="center"><span class="math inline">\(\frac{1}{n}\)</span></td>
<td align="left">.</td>
<td align="left">.</td>
<td align="left">.</td>
<td align="center"><span class="math inline">\(\frac{1}{n}\)</span></td>
<td align="center"></td>
</tr>
</tbody>
</table>
<p>la distribución empírica bajo la hipótesis nula asignará a cada par de valores el producto de sus funciones de distribución marginales <span class="math inline">\(\widehat{F}_{H_{0}}=\hat{G}_{1} \hat{G}_{2}\)</span> :</p>
<table>
<colgroup>
<col width="11%" />
<col width="13%" />
<col width="13%" />
<col width="11%" />
<col width="11%" />
<col width="11%" />
<col width="13%" />
<col width="13%" />
</colgroup>
<thead>
<tr class="header">
<th align="left"><span class="math inline">\(\widehat{F}_{H}\)</span></th>
<th align="center"><span class="math inline">\(Y_{(1)}\)</span></th>
<th align="center"><span class="math inline">\(Y_{(2)}\)</span></th>
<th align="left"><span class="math inline">\(\cdot\)</span></th>
<th align="left"><span class="math inline">\(\cdot\)</span></th>
<th align="left"><span class="math inline">\(\cdot\)</span></th>
<th align="center"><span class="math inline">\(Y_{(n)}\)</span></th>
<th align="center"><span class="math inline">\(\hat{G}_{1}\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><span class="math inline">\(X_{(1)}\)</span></td>
<td align="center"><span class="math inline">\(\frac{1}{n^{2}}\)</span></td>
<td align="center"><span class="math inline">\(\frac{1}{n^{2}}\)</span></td>
<td align="left"><span class="math inline">\(\cdot\)</span></td>
<td align="left"><span class="math inline">\(\cdot\)</span></td>
<td align="left"><span class="math inline">\(\cdot\)</span></td>
<td align="center"><span class="math inline">\(\frac{1}{n^{2}}\)</span></td>
<td align="center"><span class="math inline">\(\frac{1}{n}\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(X_{(2)}\)</span></td>
<td align="center"><span class="math inline">\(\frac{1}{n^{2}}\)</span></td>
<td align="center"><span class="math inline">\(\frac{1}{n^{2}}\)</span></td>
<td align="left"><span class="math inline">\(\cdot\)</span></td>
<td align="left"><span class="math inline">\(\cdot\)</span></td>
<td align="left"><span class="math inline">\(\cdot\)</span></td>
<td align="center"><span class="math inline">\(\frac{1}{n^{2}}\)</span></td>
<td align="center"><span class="math inline">\(\frac{1}{n}\)</span></td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(\cdot\)</span></td>
<td align="center"><span class="math inline">\(\cdot\)</span></td>
<td align="center"><span class="math inline">\(\cdot\)</span></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="center"><span class="math inline">\(\cdot\)</span></td>
<td align="center"><span class="math inline">\(\cdot\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(\cdot\)</span></td>
<td align="center"><span class="math inline">\(\cdot\)</span></td>
<td align="center"><span class="math inline">\(\cdot\)</span></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="center"><span class="math inline">\(\cdot\)</span></td>
<td align="center"><span class="math inline">\(\cdot\)</span></td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(\cdot\)</span></td>
<td align="center"><span class="math inline">\(\cdot\)</span></td>
<td align="center"><span class="math inline">\(\cdot\)</span></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="center"><span class="math inline">\(\cdot\)</span></td>
<td align="center"><span class="math inline">\(\cdot\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(X_{(n)}\)</span></td>
<td align="center"><span class="math inline">\(\frac{1}{n^{2}}\)</span></td>
<td align="center"><span class="math inline">\(\frac{1}{n^{2}}\)</span></td>
<td align="left"><span class="math inline">\(\cdot\)</span></td>
<td align="left"><span class="math inline">\(\cdot\)</span></td>
<td align="left"><span class="math inline">\(\cdot\)</span></td>
<td align="center"><span class="math inline">\(\frac{1}{n^{2}}\)</span></td>
<td align="center"><span class="math inline">\(\frac{1}{n}\)</span></td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(\hat{G}_{2}\)</span></td>
<td align="center"><span class="math inline">\(\frac{1}{n}\)</span></td>
<td align="center"><span class="math inline">\(\frac{1}{n}\)</span></td>
<td align="left"><span class="math inline">\(\cdot\)</span></td>
<td align="left"><span class="math inline">\(\cdot\)</span></td>
<td align="left"><span class="math inline">\(\cdot\)</span></td>
<td align="center"><span class="math inline">\(\frac{1}{n}\)</span></td>
<td align="center"></td>
</tr>
</tbody>
</table>
<p>Para realizar un test de independencia es preciso estimar:</p>
<p><span class="math display">\[
p=\operatorname{Prob}\{T(\underset{\sim}{X} ; \underset{\sim}{Y}) \geq t_{o b s} \mid \underbrace{F=G_{1} G_{2}}_{H_{0}}\} .
\]</span></p>
<p>El planteamiento bootstrap de este problema consistirá en estimar este pvalor, <span class="math inline">\(p\)</span>, mediante el p -valor bootstrap, <span class="math inline">\(p^{*}\)</span> :</p>
<p><span class="math display">\[
p^{*}=\operatorname{Prob}_{*}\left\{T\left(\mathbf{X}^{*} ; \mathbf{Y}^{*}\right) \geq t_{o b s} \mid \widehat{F}_{H_{0}}=\hat{G}_{1} \hat{G}_{2}\right\}
\]</span></p>
<p>Este p-valor bootstrap se aproximará por Monte Carlo mediante el siguiente algoritmo:</p>
<ol style="list-style-type: decimal">
<li>Generar <span class="math inline">\(b=1 \ldots B\)</span> remuestras a partir de <span class="math inline">\(\widehat{F}_{H_{0}}\)</span> y calcular sobre cada una de ellas el estadístico de test escogido</li>
</ol>
<p><span class="math display">\[
\begin{array}{lllll}
\left(\left(X_{1}^{*}, Y_{1}^{*}\right),\right. &amp; \ldots &amp; \left.\left(X_{n}^{*}, Y_{n}^{*}\right)\right)_{1} &amp; \rightarrow &amp; T_{1}^{*} \\
\left(\left(X_{1}^{*}, Y_{1}^{*}\right),\right. &amp; \ldots &amp; \left.\left(X_{n}^{*}, Y_{n}^{*}\right)\right)_{2} &amp; \rightarrow &amp; T_{2}^{*} \\
&amp; \ldots &amp; &amp; \ldots &amp; \\
\left(\left(X_{1}^{*}, Y_{1}^{*}\right),\right. &amp; \ldots &amp; \left.\left(X_{n}^{*}, Y_{n}^{*}\right)\right)_{B} &amp; \rightarrow &amp; T_{B}^{*}
\end{array}
\]</span></p>
<ol start="2" style="list-style-type: decimal">
<li>Aproximar el p -valor bootstrap <span class="math inline">\(p^{*}\)</span> mediante</li>
</ol>
<p><span class="math display">\[
\hat{p}^{*}=\frac{\#\left\{T_{b}^{*} \geq t_{o b s}\right\}}{B} \approx p^{*}
\]</span></p>
<ol start="3" style="list-style-type: decimal">
<li>Si <span class="math inline">\(\hat{p}^{*} \leq \alpha\)</span> se rechaza la hipótesis nula.</li>
</ol>
<div id="resumiendo-estimación-de-f-bajo-h_0" class="section level4 hasAnchor" number="14.10.0.1">
<h4><span class="header-section-number">14.10.0.1</span> Resumiendo: Estimación de <span class="math inline">\(F\)</span> bajo <span class="math inline">\(H_{0}\)</span><a href="métodos-de-computación-intensiva-el-bootstrap.html#resumiendo-estimaci%C3%B3n-de-f-bajo-h_0" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>A pesar de que existen diversas estrategias para la estimación de <span class="math inline">\(F\)</span> bajo la hipótesis nula la idea básica es procurar realizar la estimación bajo las restricciones impuestas por la hipótesis nula. En terminos prácticos esto significa que, cuando ello sea posible, se mire de adaptar el proceso de remuestreo para generar las remuestras directamente a partir de la hipótesis nula.</p>
<p>De forma más general si <span class="math inline">\(\hat{F}\)</span> es la estimación que se considera más apropiada para <span class="math inline">\(F\)</span> en condiciones generales, y <span class="math inline">\(\delta\)</span> es una medida de distancia entre distribuciones escogeremos <span class="math inline">\(\hat{F}_{H_{0}}\)</span> de forma que minimice <span class="math inline">\(\delta(\hat{F}, G)\)</span> entre todas aquellas distribuciones <span class="math inline">\(G\)</span> que verifiquen <span class="math inline">\(H_{0}\)</span>. Posteriormente se remuestreará a partir de <span class="math inline">\(\hat{F}_{H_{0}}\)</span>. La distancia <span class="math inline">\(\delta(\)</span>,<span class="math inline">\() puede no considerarse explicitamente. De he-\)</span> cho puede tomarse el mínimo valor de <span class="math inline">\(\delta\)</span> como estadístico de test.</p>
<p>Los ejemplos introductorias nos han mostrado distintos enfoques para la estimación de la distribución bajo la hipótesis nula, y por lo tanto para el remuestreo bajo la hipótesis nula</p>
<ul>
<li>En el primer ejemplo (comparación de medias) se ha modificado el soporte de la distribución empírica, puesto que se ha remuestreado a partir de una nueva muestra adaptada a la hipótesis nula. Se ha realizado un desplazamiento de la muestra y de su distribución empírica pero no de la distribución bootstrap del estadístico de test que no se ha visto afectada.</li>
<li>En el segundo ejemplo (independencia entre variables) podemos considerar que la muestra original era toda la tabla cruzada <span class="math inline">\(\left(X_{i}, Y_{j}\right), i=\)</span>
<span class="math inline">\(1 \ldots n, j=1 \ldots n\)</span> y que la distribución empírica asigna masa de probabilidad <span class="math inline">\(1 / n\)</span> a todos los pares tales que <span class="math inline">\(i=j\)</span>, es decir <span class="math inline">\(\left(X_{i}, Y_{i}\right)\)</span> y 0 a los restantes. En este caso el proceso de remuestreo de <span class="math inline">\(\hat{F}_{H_{0}}\)</span> solo ha modificado los pesos o probabilidades de cada par ( <span class="math inline">\(X_{i}, Y_{j}\)</span> ), asignandoles un peso1 <span class="math inline">\(/ n^{2}\)</span> pero sin modificar sus valores, es decir sin cambiar el soporte.</li>
</ul>
</div>
<div id="relación-entre-contrastes-de-hipótesis-intervalos-de-confianza-y-el-bootstrap" class="section level3 hasAnchor" number="14.10.1">
<h3><span class="header-section-number">14.10.1</span> Relación entre contrastes de hipótesis, intervalos de confianza y el bootstrap<a href="métodos-de-computación-intensiva-el-bootstrap.html#relaci%C3%B3n-entre-contrastes-de-hip%C3%B3tesis-intervalos-de-confianza-y-el-bootstrap" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Cuando la hipótesis nula se refiere a un valor concreto de un parámetro, <span class="math inline">\(H_{0}: \theta=\theta_{0}\)</span>, puede utilizarse la relación entre pruebas de hipótesis e intervalos de confianza para estimar el p-valor o para aceptar o rechazar la hipótesis nula a partir de un intervalo de confianza.</p>
<div id="conceptos-básicos-de-intervalos-de-confianza" class="section level4 hasAnchor" number="14.10.1.1">
<h4><span class="header-section-number">14.10.1.1</span> Conceptos básicos de intervalos de confianza<a href="métodos-de-computación-intensiva-el-bootstrap.html#conceptos-b%C3%A1sicos-de-intervalos-de-confianza" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Supongamos que disponemos de un estimador, <span class="math inline">\(\hat{\theta}\)</span>, centrado en <span class="math inline">\(\theta\)</span> y con distribución normal</p>
<p><span class="math display">\[
\hat{\theta} \sim N\left(\theta, \sigma_{\hat{\theta}}\right)
\]</span></p>
<p>Sea <span class="math inline">\(\left(\hat{\theta}_{\text {inf }}, \hat{\theta}_{\text {sup }}\right)\)</span> un intervalo de confianza de nivel <span class="math inline">\(1-2 \alpha\)</span> para <span class="math inline">\(\theta\)</span>, es decir si sobre una muestra dada el valor de <span class="math inline">\(\hat{\theta}\)</span> ha sido <span class="math inline">\(\hat{\theta}_{\text {obs }}\)</span> entonces</p>
<p><span class="math display">\[
\hat{\theta}_{\mathrm{inf}}=\hat{\theta}_{o b s}-z^{1-\alpha} \cdot \sigma_{\hat{\theta}}, \quad \hat{\theta}_{\mathrm{sup}}=\hat{\theta}_{o b s}-z^{\alpha} \cdot \sigma_{\hat{\theta}}
\]</span></p>
<p>donde</p>
<p><span class="math display">\[
P_{\theta}\left\{\theta \in\left(\hat{\theta}_{\mathrm{inf}}, \hat{\theta}_{\mathrm{sup}}\right)\right\}=1-2 \alpha
\]</span></p>
<p>en el sentido habitual de los contrastes de hipótesis. Este intervalo no tiene tan sólo un recubrimiento global de <span class="math inline">\(1-2 \alpha\)</span> sinó que unilateralmente los recubrimientos son de <span class="math inline">\(\alpha\)</span> a cada lado, es decir</p>
<p><span class="math display">\[
P_{\theta}\left\{\theta&lt;\hat{\theta}_{\text {inf }}\right\}=\alpha, \text { y } P_{\theta}\left\{\theta&gt;\hat{\theta}_{\text {sup }}\right\}=\alpha
\]</span></p>
<p>Esta segunda propiedad es más fuerte que la anterior puesto que la implica mientras que la implicación contraria no es necesariamente cierta.</p>
</div>
<div id="relación-entre-los-intérvalos-de-confianza-y-las-pruebas-de-hipótesis" class="section level4 hasAnchor" number="14.10.1.2">
<h4><span class="header-section-number">14.10.1.2</span> Relación entre los intérvalos de confianza y las pruebas de hipótesis<a href="métodos-de-computación-intensiva-el-bootstrap.html#relaci%C3%B3n-entre-los-int%C3%A9rvalos-de-confianza-y-las-pruebas-de-hip%C3%B3tesis" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Supongamos ahora que el auténtico valor de <span class="math inline">\(\theta\)</span> fuera <span class="math inline">\(\hat{\theta}_{\text {ínf }}\)</span>. Tendríamos:</p>
<p><span class="math display">\[
\hat{\theta} \sim N\left(\hat{\theta}_{\mathrm{inf}}, \sigma_{\hat{\theta}}\right)
\]</span></p>
<p>y es fácil ver que</p>
<p><span class="math display">\[
P_{\hat{\theta}_{\text {inf }}}\left\{\hat{\theta} \geq \hat{\theta}_{o b s}\right\}=\alpha
\]</span></p>
<p>Así para cualquier valor de <span class="math inline">\(\theta\)</span> menor que <span class="math inline">\(\hat{\theta}_{\text {inf }}\)</span> tendremos</p>
<p><span class="math display">\[
P_{\theta}\left\{\hat{\theta} \geq \hat{\theta}_{o b s}\right\}&lt;\alpha \quad\left[\forall \theta&lt;\hat{\theta}_{\mathrm{inf}}\right]
\]</span></p>
<p>Similarmente, para cualquier valor de <span class="math inline">\(\theta\)</span> mayor que <span class="math inline">\(\hat{\theta}_{\text {sup }}\)</span> tendremos</p>
<p><span class="math display">\[
P_{\theta}\left\{\hat{\theta} \leq \hat{\theta}_{o b s}\right\}&lt;\alpha \quad\left[\forall \theta&gt;\hat{\theta}_{\mathrm{inf}}\right]
\]</span></p>
<p>La lógica del intervalo de confianza ( <span class="math inline">\(\hat{\theta}_{\text {inf }}, \hat{\theta}_{\text {sup }}\)</span> ) puede explicarse en terminos de 6.5 y 6.6 :</p>
<ul>
<li>Escogemos una probabilidad pequeña <span class="math inline">\(\alpha\)</span></li>
<li>Decidimos qué valores del parámetro inferiores a <span class="math inline">\(\hat{\theta}_{\text {inf }}\)</span> son implausibles porque dan una probabilidad menor que <span class="math inline">\(\alpha\)</span> de observar una estimación tan grande cómo (o mayor que) la que hemos observado</li>
<li>Decidimos qué valores del parámetro inferiores a <span class="math inline">\(\hat{\theta}_{\text {sup }}\)</span> son implausibles porque dan una probabilidad menor que <span class="math inline">\(\alpha\)</span> de observar una estimación tan pequeña cómo (o menor que) la que hemos observado</li>
</ul>
<p>En resumen:</p>
<ol style="list-style-type: decimal">
<li>el intervalo de confianza de nivel <span class="math inline">\(1-2 \alpha\)</span>, ( <span class="math inline">\(\left.\hat{\theta}_{\text {inf }}, \hat{\theta}_{\text {sup }}\right)\)</span> es el conjunto de los valores plausibles de <span class="math inline">\(\theta\)</span>, habiendo observado <span class="math inline">\(\hat{\theta}_{\text {obs }}\)</span>, cuyos valores no son rechazados por los tests de plausibilidad 6.5 y 6.6.</li>
<li>los tests de plausibilidad 6.5 y 6.6 ’on tambien los niveles de significación para los contrastes de hipótesis asociados</li>
</ol>
<ul>
<li>El valor en los tests de plausibilidad 6.5 es el nivel de significación para el test de alternativa unilateral de que el verdadero valor del parámetro es mayor que <span class="math inline">\(\theta\)</span> y</li>
<li>El valor en los tests de plausibilidad 6.6 es el nivel de significación para el test de alternativa unilateral de que el verdadero valor del parámetro es menor que <span class="math inline">\(\theta\)</span>.</li>
</ul>
<p>Los resultados anteriores permiten que, en muchas situaciones, sea posible llevar a cabo una prueba de hipótesis construyendo un intervalo de confianza y comprobandoi si el valor de la hipótesis nula se encuentra en el intérvalo.</p>
</div>
<div id="pruebas-de-hipótesis-intervalos-de-confianza-y-bootstrap" class="section level4 hasAnchor" number="14.10.1.3">
<h4><span class="header-section-number">14.10.1.3</span> Pruebas de hipótesis, intervalos de confianza y bootstrap<a href="métodos-de-computación-intensiva-el-bootstrap.html#pruebas-de-hip%C3%B3tesis-intervalos-de-confianza-y-bootstrap" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Como acabamos de ver la relación entre intervalos de confianza y pruebas de hipótesis es muy fuerte. De hecho lo es tanto que no tan sólo podemos utilizar el intervalo de confianza para aceptar o rechazar la hipótesis nula sinó que también podemos utilizarlo para calcular el p -valor asociado al test.</p>
<p>Supongamos que <span class="math inline">\(\hat{\theta}_{o b s}\)</span>, el valor observado del estadístico de interés es mayor que 0 y escojamos <span class="math inline">\(\alpha\)</span> de forma que el extremo inferior del intervalo <span class="math inline">\(\hat{\theta}_{\text {inf }}\)</span> de nivel <span class="math inline">\(1-2 \alpha\)</span> sea exactamente igual a 0 . Entonces según 6.4</p>
<p><span class="math display">\[
P_{\theta=0}\left\{\hat{\theta} \geq \hat{\theta}_{o b s}\right\}=\alpha
\]</span></p>
<p>Ejemplo 1 Con los datos de dos grupos de ratones, control y tratamiento tenemos, suponiendo normalidad:</p>
<p><span class="math display">\[
\begin{aligned}
\bar{X}_{T} &amp; =86,9, \quad S_{T} / \sqrt{7}=25,2 \\
\bar{X}_{C} &amp; =56,3, \quad S_{C} / \sqrt{7}=14,3 \\
\bar{X}_{D} &amp; =\bar{X}_{T}-\bar{X}_{C}=30,6, \quad S_{D}=28,8
\end{aligned}
\]</span></p>
<p>Un intervalo de confianza al 95% para la diferencia, es</p>
<p><span class="math display">\[
\bar{X}_{D} \mp S_{D} \cdot 1,96,
\]</span></p>
<p>de donde el extremo inferior del intervalo es</p>
<p><span class="math display">\[
\bar{X}_{\mathrm{inf}}=-26,6 .
\]</span></p>
<p>Tomando un valor <span class="math inline">\(\alpha\)</span> tal que: <span class="math inline">\(z_{\alpha / 2}=1,06\)</span> conseguiremos que el extremo inferior valga 0 .</p>
<p>Cuando como en el ejemplo anterior ( <span class="math inline">\(\theta=\mu_{1}-\mu_{2}\)</span>, ) la hipótesis nula es <span class="math inline">\(\theta=0\)</span>, el p-valor, tal como se ha definido en 6.1:</p>
<p><span class="math display">\[
N S A=P_{H_{0}}\left\{\hat{\theta} \geq \hat{\theta}_{o b s}\right\}
\]</span></p>
<p>coincide con el valor de <span class="math inline">\(\alpha\)</span> que ha sido preciso fijar para que el extremo inferior del intervalo de nivel <span class="math inline">\(1-2 \alpha, \hat{\theta}_{\text {inf }}\)</span>, valga 0 . Es decir a través del intervalo de confianza (en concreto a traves de fijar el valor para <span class="math inline">\(\hat{\theta}_{\text {inf }}\)</span> ) hemos obtenido el p-valor del test correspondiente, tal como se habia indicado que er posible realizar.</p>
<p>Veamos un ejemplo:</p>
<p>La figura muestra la distribución bootstrap de la diferencia de medias con los datos de los tiempos de supervivencia para dos grupos de ratones.</p>
<p>Podemos utilizar esta distribución para formar intervalos de confianza para la diferencia entre el grupo tratamiento y el grupo control con una diferencia media observada de <span class="math inline">\(\hat{\theta}_{\text {obs }}=30,23\)</span>.</p>
<p>Qué valor de <span class="math inline">\(\alpha\)</span> hará que el extremo inferior del intervalo sea igual a cero?. Si consideramos el método del percentil aplicado a un estadístico <span class="math inline">\(\hat{\theta}_{b}^{*}\)</span> la respuesta es</p>
<p><span class="math display">\[
\alpha_{0}=\frac{\left\{\# \hat{\theta}_{b}^{*}&lt;0\right\}}{B},
\]</span></p>
<p>es decir la proporción de réplicas bootstrap en donde <span class="math inline">\(\hat{\theta}_{b}^{*}\)</span> es menor que 0 , con lo que, por la definición del método percentil, <span class="math inline">\(\hat{\theta}_{\text {ínf }}=\hat{\theta}^{\left(\alpha_{0}\right)}=0\)</span>. El valor <span class="math inline">\(\alpha_{0}\)</span> coincide, como hemos indicado en el párrafo anterior, con el NDS de <span class="math inline">\(\hat{\theta}\)</span> es decir:</p>
<p><span class="math display">\[
\widehat{N S A}=\frac{\left\{\# \hat{\theta}_{b}^{*}&lt;0\right\}}{B}
\]</span></p>
<p>Las <span class="math inline">\(B=1000\)</span> réplicas bootstrap de la figura anterior dieron unos valores de</p>
<p><span class="math display">\[
\widehat{N S A}\left(\hat{\theta}_{o b s}\right)=\frac{\left\{\# \hat{\theta}_{b}^{*}&lt;0\right\}}{B}=0,132
\]</span></p>
<p>Este es un valor similar al que se obtendria remuestreando bajo la hipó tesis nula o mediante el test de permutaciones comentado en el apartado anterior que, en este ejemplo concreto da un valor de 0.137</p>
<p>Los cálculos anteriores se basan en el intervalo percentil ordinario. Efron (1993) describe cómo hacerlo de forma general basándose en los intervalos percentil <span class="math inline">\(\mathrm{BC}_{a}\)</span>, de los que los intervalos percentil son un caso particular.</p>
<p>Es importante contrastar las dos aproximaciones a la resolución del problema del contraste de hipótesis: El histograma de la distribución bootstrap de la diferencia está centrado entorno del <span class="math inline">\(\hat{\theta}_{o b s}\)</span>, mientras que el del test bootstrap estás centrado entorno del 0 . En este sentido <span class="math inline">\(N D S_{\text {boot }}\)</span> mide qué tan lejos está <span class="math inline">\(\hat{\theta}_{o b s}\)</span> del 0 mientras que <span class="math inline">\(N D S\left(\hat{\theta}_{o b s}\right)\)</span> mide qué tan lejos está el 0 de <span class="math inline">\(\hat{\theta}_{\text {obs }}\)</span>. Efron (1993) sugiere que los ajustes que realiza el método del <span class="math inline">\(\mathrm{BC}_{a}\)</span> pueden verse como una forma de reconciliar estas dos aproximaciones.</p>
</div>
</div>
</div>
<div id="validez-de-los-métodos-bootstrap" class="section level2 hasAnchor" number="14.11">
<h2><span class="header-section-number">14.11</span> Validez de los métodos bootstrap<a href="métodos-de-computación-intensiva-el-bootstrap.html#validez-de-los-m%C3%A9todos-bootstrap" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="introducción-13" class="section level3 hasAnchor" number="14.11.1">
<h3><span class="header-section-number">14.11.1</span> Introducción<a href="métodos-de-computación-intensiva-el-bootstrap.html#introducci%C3%B3n-13" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>A la vista de las técnicas que acabamos de discutir, los métodos bootstrap aparecen como una valiosa herramienta, utilizable en muchas situaciones en las que no se dispone de una solución exacta. Debemos fijarnos sin embargo que la justificación que se ha dado para su utilización era, de entrada, de tipo intuitivo: dado que en general los estimadores del error estándar de un estimador son de la forma</p>
<p><span class="math display">\[
\sigma_{\hat{\theta}}=\sigma_{\hat{\theta}}(F)
\]</span></p>
<p>entonces, cuando no se conoce la forma explícita de <span class="math inline">\(\sigma_{\hat{\theta}}(F)\)</span> se puede calcular, mediante bootstrap, directamente <span class="math inline">\(\hat{\sigma}_{\hat{\theta}}\left(F_{n}\right)\)</span> y utilizarlo para estimar <span class="math inline">\(\sigma_{\hat{\theta}}\)</span>.</p>
<p>La idea anterior parece correcta, pero “qué garantía tenemos de que, en cada caso concreto, esta aproximación será correcta?</p>
<p>Yendo más allá del error estándar, la idea que hay detrás de los métodos bootstrap consiste en utilizar la distribución bootstrap del estadístico -aproximable por Monte Carlo- en substitución de su distribución muestral (habitualmente desconocida). De nuevo debemos preguntarnos si esta aproximación será correcta en cada situación en que la utilicemos.</p>
<p>No se trata ahora de descubrir que todo lo anterior era una falacia, sino de remarcar el hecho de que, para utilizar un método determinado, por muy intuitivo o correcto que este nos parezca, debemos de justificar de alguna forma su validez en una situación dada.</p>
<p>En el caso del bootstrap esto se puede hacer de diversas formas. Sin embargo la mayoría de ellas requieren resultados cuya complejidad escapa por completo al nivel al que se realiza nuestra discusión. Por lo tanto, y simple-
mente a modo de ilustración, nos limitaremos a enunciar un resultado que justifica asintóticamente la validez del bootstrap, en ciertas situaciones, y a discutir como puede probarse la validez del método mediante simulación.</p>
</div>
<div id="teorema-central-del-límite-bootstrap" class="section level3 hasAnchor" number="14.11.2">
<h3><span class="header-section-number">14.11.2</span> Teorema central del límite bootstrap<a href="métodos-de-computación-intensiva-el-bootstrap.html#teorema-central-del-l%C3%ADmite-bootstrap" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>El siguiente teorema, enunciado de forma casi simultánea por P. Bickel de la universidad de Berkeley y K. Singh de Stanford a principios de los 80, establece la normalidad asintótica de la media bootstrap y la convergencia del error estándar bootstrapen probabilidad condicional hacia el error estándar de la media.</p>
<p>Aún cuando parezca que “para este viaje no hacían falta alforjas”, dado que la distribución asintótica de la media muestral es conocida, este resultado es sugerente cómo mínimo por dos motivos:</p>
<ul>
<li>En un caso conocido se obtienen mediante el bootstrap los mismos resultados que con la teoría clásica.</li>
<li>La mayor parte de los resultados asintóticos relativos a los métodos bootstrap para situaciones diversas son parecidas a este teorema de Bickel: Establecen que la distribución bootstrapconverge hacia la misma distribución asintótica que la distribución del estadístico con lo que la primera - y las características calculadas sobre ella- es una buena aproximación de la última.</li>
</ul>
<p>El enunciado del teorema es el siguiente:
theorem 1 Sean <span class="math inline">\(\left(X_{1}, X_{2}, \ldots, X_{n}\right) \stackrel{\text { iiid }}{\sim} F\)</span> con varianza finita, <span class="math inline">\(\sigma^{2}&gt;0\)</span>. Sea <span class="math inline">\(F_{n}\)</span> la función de distribución empírica de <span class="math inline">\(\left(X_{1}, X_{2}, \ldots, X_{n}\right)\)</span> y <span class="math inline">\(\left(X_{1}^{*}, X_{2}^{*}, \ldots, X_{n}^{*}\right)\)</span> una muestra obtenida de <span class="math inline">\(F_{n}\)</span>. Sea:</p>
<p><span class="math display">\[
T_{n}^{*}=\sqrt{n}\left(\bar{X}_{b}^{*}-\bar{X}\right)
\]</span></p>
<p>donde <span class="math inline">\(\bar{X}=n^{-1} \sum_{i=1}^{n} X_{i}\)</span> y <span class="math inline">\(\bar{X}_{b}^{*}=n^{-1} \sum_{i=1}^{n} X_{i}^{*}\)</span>. Entonces:</p>
<ol style="list-style-type: decimal">
<li>(Bickel y Freedman (1981, [2] ), K. Singh (1981, [22]) y Shie Shie Yang (1988, [21]): La distribución condicional de <span class="math inline">\(T_{n}^{*}\)</span>, dada <span class="math inline">\(\left(X_{1}, X_{2} \ldots, X_{n}\right)\)</span>, converge débilmente hacia una distribución normal <span class="math inline">\(N\left(0, \sigma^{2}\right)\)</span>, cuando <span class="math inline">\(n \rightarrow \infty\)</span>.</li>
<li>(Unicamente en Bickel y Freedman (1981, [2])). Sea</li>
</ol>
<p><span class="math display">\[
\hat{S}^{2 *}=\frac{1}{n} \sum_{i=1}^{n}\left(X_{i}^{*}-\bar{X}_{b}^{*}\right)^{2}
\]</span></p>
<p>Entonces:</p>
<p><span class="math display">\[
\hat{S}^{*} \rightarrow \sigma
\]</span></p>
<p>en probabilidad condicional es decir que <span class="math inline">\(\forall \epsilon&gt;0\)</span> :</p>
<p><span class="math display">\[
P\left\{\left|\hat{S}^{*}-\sigma\right|&gt;\epsilon \mid\left(X_{1}, X_{2}, \ldots, X_{n}\right)\right\} \rightarrow 0 \text {, casi seguramente. }
\]</span></p>
</div>
<div id="validación-del-bootstrap-mediante-simulación" class="section level3 hasAnchor" number="14.11.3">
<h3><span class="header-section-number">14.11.3</span> Validación del bootstrap mediante simulación<a href="métodos-de-computación-intensiva-el-bootstrap.html#validaci%C3%B3n-del-bootstrap-mediante-simulaci%C3%B3n" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Como hemos comentado, la teoría asintótica para el bootstrap es muy compleja. Una forma alternativa de determinar si un método bootstrap funciona es mediante simulación de Monte Carlo (este método es de aplicabilidad general aunque aquí se considera tan sólo en el contexto de los métodos bootstrap. Puede consultarse Lewis (1988[16]) para una exposición general del tema).</p>
<p>La validación del bootstrap mediante métodos de Monte Carlo consiste en simular la población original de forma que los estimadores calculados sobre el conjunto de la muestra nos den una buena aproximación de los valores poblacionales. Una vez estimados éstos podremos compararlos con los estimadores que deseamos validar, mediante su valor medio a lo largo de todas las simulaciones.</p>
<p>Por ejemplo, para realizar esta validación en el caso del error estándar bootstrapdel coeficiente de correlación ejecutaremos el siguiente algoritmo:</p>
<ol style="list-style-type: decimal">
<li>Repetir un elevado número de veces <span class="math inline">\(s=1, \ldots, S\)</span> (p.ej. <span class="math inline">\(S=10000\)</span> ) el siguiente proceso:</li>
</ol>
<ol style="list-style-type: lower-alpha">
<li>Generar una muestra de una distribución bivariante, p.ej. una normal, <span class="math inline">\(N(\mu, \Sigma)\)</span>.</li>
<li>Calcular sobre ella el coeficiente de correlación <span class="math inline">\(\hat{\rho}_{s}\)</span>, y el error estándar bootstrap <span class="math inline">\(\hat{\sigma}_{B}\)</span></li>
</ol>
<ol start="2" style="list-style-type: decimal">
<li>La desviación típica muestral, <span class="math inline">\(s_{\hat{\rho}}\)</span> de los coeficientes de correlación <span class="math inline">\(\hat{\rho}_{1}, \ldots, \hat{\rho}_{s}\)</span> es una buena estimación del error estándar de <span class="math inline">\(\hat{\rho}\)</span> y la media muestral de los errores estándar bootstrap, <span class="math inline">\(\overline{\hat{\sigma}}_{B}\)</span> es una buena estimación del error estándar bootstrap del coeficiente de correlación.</li>
<li>Cuanto más similares sean <span class="math inline">\(s_{\hat{\rho}}\)</span> y <span class="math inline">\(\overline{\hat{\sigma}_{B}}\)</span> mejor será la calidad de <span class="math inline">\(\hat{\sigma}_{B}\)</span> cómo estimador de <span class="math inline">\(\sigma_{\hat{\rho}}\)</span>.</li>
</ol>
<p>Una simulación realizada con el programa EMSS en donde se han generado se generaron <span class="math inline">\(S=10000\)</span> muestras de una población normal bivariante con auténtico valor del coeficiente de correlación <span class="math inline">\(\rho=0,5\)</span> y <span class="math inline">\(n=15\)</span> da los resultados siguientes:</p>
<p>Tabla 6
Comparación de estimadores del error estándar
<span class="math inline">\(X \sim N(\mu, \boldsymbol{\Sigma}), S=10000\)</span>
| Bootstrap <span class="math inline">\((\mathrm{B}=128)\)</span> | 0.197 |
| :— | :— |
| Teoría normal | 0.207 |
| Verdadero valor | 0.208 |</p>
<p>Cómo se ve en la tabla 6 el estimador bootstrap del error estándar es una buiena aproximación, pero el estimador de la teoría normal funciona aún mejor, dado que la situación en que se ha realizado la simulación es precisamente una normal bivariante.</p>
</div>
</div>
<div id="bibliografía" class="section level2 hasAnchor" number="14.12">
<h2><span class="header-section-number">14.12</span> Bibliografía<a href="métodos-de-computación-intensiva-el-bootstrap.html#bibliograf%C3%ADa" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>[2] Bickel, P. and Freedman, J. (1981). Some assymptotic theory for the bootstrap. The annals of Statistics, (9), 6, 1196-1217.</p>
<p>[3] Booth, J., G. and Sarkar, S. (1998). Monte Carlo Approximation of Bootstrap Variances. The American Statistician, (52), 4, 354-357.</p>
<p>[4] Cristóbal Cristobal, J. Antonio (1992).Inferencia Estadística. Servicio de Publicaciones. Universidad de Zaragoza.</p>
<p>[5] Dudewicz, Edward J., Mishra, S. (1988).Modern mathematical statistics. Wiley. New York.</p>
<p>[6] Di Ciccio, T.J. and Romano, J.P. (1988). A review of bootstrap confidence intervals. J.R. Statist. Soc. B. 50, 338-354.</p>
<p>[7] Efron, B., 1979 The bootstrap, another look at the jackniffe. Annals of Statistics 7 (1), 1-26</p>
<p>[8] Efron, B. 1982 Transformation theory: how normal is a family of distributions?. Annals of Statistics 10 (2), 323-339</p>
<p>[9] Efron, B., 1982 The jackniffe, the bootstrap and other resampling plans. S.I.A.M. CBMS-Natl. Sci. Found. Monogr. 38</p>
<p>[10] Efron, B.(1985) Bootstrap confidence intervals for a class of parametric problems. Biometrika 72 (1) 45-58</p>
<p>[11] Efron, B.(1987) Better Bootstrap Confidence Intervals. Journal of the American Statistical Association 82 (397) 171-200</p>
<p>[12] Efron, B. Tibishirani, R. (1986) Bootstrap methods for standard errors, confidence intervals and other measures of statistical accuracy.Statistical Science 1 54-77</p>
<p>[13] Hall, P. (1988) Theoretical comparison of Bootstrap confidence intervals. Annals of Statistics 16 927-953.</p>
<p>[14] Holmes, Susan (1999) Introduction to the bootstrap. Curso en internet en la dirección URL:
<a href="http://www-stat.stanford.edu/~susan/courses/s208/web1.html" class="uri">http://www-stat.stanford.edu/~susan/courses/s208/web1.html</a></p>
<p>[15] Hampel, F., Ronchetti, E., Rouseeuw, P. and Stahel, W. (1986). Robust Statistics, The Approach Based on Influence Functions. Wiley, New York.</p>
<p>[16] (1989) Simulation methodology for Statisticians, Operations Analysts, and Engineers. Volume I. Wadsworth &amp; Brooks/Cole. Pacific Grove, California.</p>
<p>[17] Peña, Daniel (1988). Estadística modelos y metodos 1. Fundamentos. Alianza editorial, Madrid.</p>
<p>[18] Sánchez, A., Ocaña, J., Ruiz de Villa, C. (1992) An Environment for Monte Carlo Simulation Studies (EMSS). 10th Symposium on Computational Statistics (COMPSTAT).Physica-Verlag. Springer Verlag.</p>
<p>[19] Schenker,N. (1985) Qualms about bootstrap confidence intervals. Journal of the American Statistical Association 80 360-361.</p>
<p>[20] Serfling, R. J., (1980). Approximation Theorems of Mathematical Statistics. John Wiley &amp; Sons. New York.</p>
<p>[21] Shie Shie Yang (1988). A Central Limit Theorem for the Bootstrap Mean. The American Statistician. (42), 3, 202-203.</p>
<p>[22] Singh,Kesar. (1981). On the assymptotic accuracy of Efron’s Bootstrap. The annals of Statistics, (9), 6, 1187-1195.</p>
<p>[23] Velez, R., García, A. (1993). Principios de inferencia estadística. Publicaciones de la U.N.E.D. Madrid</p>

<div style="page-break-after: always;"></div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="estadística-no-paramétrica.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="bibliografia.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
  "sharing": {
    "github": false,
    "facebook": true,
    "twitter": true,
    "linkedin": false,
    "weibo": false,
    "instapaper": false,
    "vk": false,
    "whatsapp": false,
    "all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
  },
  "fontsettings": {
    "theme": "white",
    "family": "sans",
    "size": 2
  },
  "edit": {
    "link": null,
    "text": null
  },
  "history": {
    "link": null,
    "text": null
  },
  "view": {
    "link": null,
    "text": null
  },
  "download": null,
  "search": {
    "engine": "fuse",
    "options": null
  },
  "toc": {
    "collapse": "section",
    "scroll_highlight": true
  }
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
