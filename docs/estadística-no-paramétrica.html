<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Capítulo 13 Estadística no paramétrica | Fundamentos de Inferencia Estadistica</title>
  <meta name="description" content="Capítulo 13 Estadística no paramétrica | Fundamentos de Inferencia Estadistica" />
  <meta name="generator" content="bookdown 0.45 and GitBook 2.6.7" />

  <meta property="og:title" content="Capítulo 13 Estadística no paramétrica | Fundamentos de Inferencia Estadistica" />
  <meta property="og:type" content="book" />
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Capítulo 13 Estadística no paramétrica | Fundamentos de Inferencia Estadistica" />
  
  
  

<meta name="author" content="Alex Sanchez Pla y Santiago Pérez Hoyos" />


<meta name="date" content="2026-01-05" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="las-pruebas-chi-cuadrado.html"/>
<link rel="next" href="bibliografia.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="blocks.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<div style="margin:0 0 10px 15px;">
  <a href="FundamentosInferenciaEstadistica.pdf" target="_blank"
     title="Descargar PDF"
     style="display:flex;align-items:center;gap:6px;text-decoration:none;">
    <img src="images/aPDF.png" alt="PDF" width="16" height="20">
    <span style="font-weight:bold;">Descargar versión PDF</span>
  </a>
</div>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Presentación</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#objetivo"><i class="fa fa-check"></i>Objetivo</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#prerequisitos-y-organizaci%C3%B3n-del-material"><i class="fa fa-check"></i>Prerequisitos y organización del material</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="agradecimiento-y-fuentes-utilizadas.html"><a href="agradecimiento-y-fuentes-utilizadas.html"><i class="fa fa-check"></i>Agradecimiento y fuentes utilizadas</a>
<ul>
<li class="chapter" data-level="" data-path="agradecimiento-y-fuentes-utilizadas.html"><a href="agradecimiento-y-fuentes-utilizadas.html#el-proyecto-statmedia"><i class="fa fa-check"></i>El proyecto Statmedia</a></li>
<li class="chapter" data-level="" data-path="agradecimiento-y-fuentes-utilizadas.html"><a href="agradecimiento-y-fuentes-utilizadas.html#otros-materiales-utilizados"><i class="fa fa-check"></i>Otros materiales utilizados</a></li>
<li class="chapter" data-level="" data-path="agradecimiento-y-fuentes-utilizadas.html"><a href="agradecimiento-y-fuentes-utilizadas.html#materiales-complementarios"><i class="fa fa-check"></i>Materiales complementarios</a>
<ul>
<li class="chapter" data-level="" data-path="agradecimiento-y-fuentes-utilizadas.html"><a href="agradecimiento-y-fuentes-utilizadas.html#complementos-matem%C3%A1ticos"><i class="fa fa-check"></i>Complementos matemáticos</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="1" data-path="probabilidad-y-experimentos-aleatorios.html"><a href="probabilidad-y-experimentos-aleatorios.html"><i class="fa fa-check"></i><b>1</b> Probabilidad y Experimentos aleatorios</a>
<ul>
<li class="chapter" data-level="1.1" data-path="probabilidad-y-experimentos-aleatorios.html"><a href="probabilidad-y-experimentos-aleatorios.html#introducci%C3%B3n"><i class="fa fa-check"></i><b>1.1</b> Introducción</a>
<ul>
<li class="chapter" data-level="1.1.1" data-path="probabilidad-y-experimentos-aleatorios.html"><a href="probabilidad-y-experimentos-aleatorios.html#fen%C3%B3menos-deterministas-y-fen%C3%B3menos-aleatorios"><i class="fa fa-check"></i><b>1.1.1</b> Fenómenos deterministas y fenómenos aleatorios</a></li>
<li class="chapter" data-level="1.1.2" data-path="probabilidad-y-experimentos-aleatorios.html"><a href="probabilidad-y-experimentos-aleatorios.html#sucesos"><i class="fa fa-check"></i><b>1.1.2</b> Sucesos</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="probabilidad-y-experimentos-aleatorios.html"><a href="probabilidad-y-experimentos-aleatorios.html#funci%C3%B3n-de-probabilidad"><i class="fa fa-check"></i><b>1.2</b> Función de probabilidad</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="probabilidad-y-experimentos-aleatorios.html"><a href="probabilidad-y-experimentos-aleatorios.html#diferentes-funciones-de-probabilidad-para-una-misma-experiencia-aleatoria"><i class="fa fa-check"></i><b>1.2.1</b> ¿Diferentes funciones de probabilidad para una misma experiencia aleatoria?</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="probabilidad-y-experimentos-aleatorios.html"><a href="probabilidad-y-experimentos-aleatorios.html#c%C3%B3mo-se-calculan-las-probabilidades"><i class="fa fa-check"></i><b>1.3</b> ¿Cómo se calculan las probabilidades?</a></li>
<li class="chapter" data-level="1.4" data-path="probabilidad-y-experimentos-aleatorios.html"><a href="probabilidad-y-experimentos-aleatorios.html#sucesos-elementales-y-sucesos-observables"><i class="fa fa-check"></i><b>1.4</b> Sucesos elementales y sucesos observables</a></li>
<li class="chapter" data-level="1.5" data-path="probabilidad-y-experimentos-aleatorios.html"><a href="probabilidad-y-experimentos-aleatorios.html#propiedades-inmediatas-de-la-probabilidad"><i class="fa fa-check"></i><b>1.5</b> Propiedades inmediatas de la probabilidad</a>
<ul>
<li class="chapter" data-level="1.5.1" data-path="probabilidad-y-experimentos-aleatorios.html"><a href="probabilidad-y-experimentos-aleatorios.html#succeso-imposible"><i class="fa fa-check"></i><b>1.5.1</b> Succeso imposible</a></li>
<li class="chapter" data-level="1.5.2" data-path="probabilidad-y-experimentos-aleatorios.html"><a href="probabilidad-y-experimentos-aleatorios.html#suceso-implicado"><i class="fa fa-check"></i><b>1.5.2</b> Suceso implicado</a></li>
<li class="chapter" data-level="1.5.3" data-path="probabilidad-y-experimentos-aleatorios.html"><a href="probabilidad-y-experimentos-aleatorios.html#complementario-de-un-suceso"><i class="fa fa-check"></i><b>1.5.3</b> Complementario de un suceso</a></li>
<li class="chapter" data-level="1.5.4" data-path="probabilidad-y-experimentos-aleatorios.html"><a href="probabilidad-y-experimentos-aleatorios.html#ocurrencia-de-algun-suceso"><i class="fa fa-check"></i><b>1.5.4</b> Ocurrencia de algun suceso</a></li>
<li class="chapter" data-level="1.5.5" data-path="probabilidad-y-experimentos-aleatorios.html"><a href="probabilidad-y-experimentos-aleatorios.html#probabilidad-de-que-ocurra-algun-suceso"><i class="fa fa-check"></i><b>1.5.5</b> Probabilidad de que ocurra algun suceso</a></li>
<li class="chapter" data-level="1.5.6" data-path="probabilidad-y-experimentos-aleatorios.html"><a href="probabilidad-y-experimentos-aleatorios.html#probabilidad-de-que-ocurran-dos-o-m%C3%A1s-sucesos-a-la-vez"><i class="fa fa-check"></i><b>1.5.6</b> Probabilidad de que ocurran dos (o más) sucesos a la vez</a></li>
</ul></li>
<li class="chapter" data-level="1.6" data-path="probabilidad-y-experimentos-aleatorios.html"><a href="probabilidad-y-experimentos-aleatorios.html#espacios-de-probabilidad"><i class="fa fa-check"></i><b>1.6</b> Espacios de probabilidad</a></li>
<li class="chapter" data-level="1.7" data-path="probabilidad-y-experimentos-aleatorios.html"><a href="probabilidad-y-experimentos-aleatorios.html#probabilidad-condicionada"><i class="fa fa-check"></i><b>1.7</b> Probabilidad condicionada</a>
<ul>
<li class="chapter" data-level="1.7.1" data-path="probabilidad-y-experimentos-aleatorios.html"><a href="probabilidad-y-experimentos-aleatorios.html#la-probabilidad-condicionada-es-una-medida-de-probabilidad"><i class="fa fa-check"></i><b>1.7.1</b> La probabilidad condicionada es una medida de probabilidad</a></li>
<li class="chapter" data-level="1.7.2" data-path="probabilidad-y-experimentos-aleatorios.html"><a href="probabilidad-y-experimentos-aleatorios.html#sucesos-dependientes-y-sucesos-independientes"><i class="fa fa-check"></i><b>1.7.2</b> Sucesos dependientes y sucesos independientes</a></li>
<li class="chapter" data-level="1.7.3" data-path="probabilidad-y-experimentos-aleatorios.html"><a href="probabilidad-y-experimentos-aleatorios.html#incompatibilidad-e-independencia"><i class="fa fa-check"></i><b>1.7.3</b> Incompatibilidad e independencia</a></li>
</ul></li>
<li class="chapter" data-level="1.8" data-path="probabilidad-y-experimentos-aleatorios.html"><a href="probabilidad-y-experimentos-aleatorios.html#dos-teoremas-importantes"><i class="fa fa-check"></i><b>1.8</b> Dos Teoremas importantes</a>
<ul>
<li class="chapter" data-level="1.8.1" data-path="probabilidad-y-experimentos-aleatorios.html"><a href="probabilidad-y-experimentos-aleatorios.html#teorema-de-las-probabilidades-totales"><i class="fa fa-check"></i><b>1.8.1</b> Teorema de las probabilidades totales</a></li>
<li class="chapter" data-level="1.8.2" data-path="probabilidad-y-experimentos-aleatorios.html"><a href="probabilidad-y-experimentos-aleatorios.html#teorema-de-bayes"><i class="fa fa-check"></i><b>1.8.2</b> Teorema de Bayes</a></li>
</ul></li>
<li class="chapter" data-level="1.9" data-path="probabilidad-y-experimentos-aleatorios.html"><a href="probabilidad-y-experimentos-aleatorios.html#introducci%C3%B3n-a-los-experimentos-m%C3%BAltiples"><i class="fa fa-check"></i><b>1.9</b> Introducción a los experimentos múltiples</a></li>
<li class="chapter" data-level="1.10" data-path="probabilidad-y-experimentos-aleatorios.html"><a href="probabilidad-y-experimentos-aleatorios.html#combinatoria"><i class="fa fa-check"></i><b>1.10</b> Combinatoria</a>
<ul>
<li class="chapter" data-level="1.10.1" data-path="probabilidad-y-experimentos-aleatorios.html"><a href="probabilidad-y-experimentos-aleatorios.html#permutaciones"><i class="fa fa-check"></i><b>1.10.1</b> Permutaciones</a></li>
<li class="chapter" data-level="1.10.2" data-path="probabilidad-y-experimentos-aleatorios.html"><a href="probabilidad-y-experimentos-aleatorios.html#variaciones"><i class="fa fa-check"></i><b>1.10.2</b> Variaciones</a></li>
<li class="chapter" data-level="1.10.3" data-path="probabilidad-y-experimentos-aleatorios.html"><a href="probabilidad-y-experimentos-aleatorios.html#variaciones-con-repetici%C3%B3n"><i class="fa fa-check"></i><b>1.10.3</b> Variaciones con repetición</a></li>
<li class="chapter" data-level="1.10.4" data-path="probabilidad-y-experimentos-aleatorios.html"><a href="probabilidad-y-experimentos-aleatorios.html#combinaciones"><i class="fa fa-check"></i><b>1.10.4</b> Combinaciones</a></li>
<li class="chapter" data-level="1.10.5" data-path="probabilidad-y-experimentos-aleatorios.html"><a href="probabilidad-y-experimentos-aleatorios.html#permutaciones-con-repetici%C3%B3n"><i class="fa fa-check"></i><b>1.10.5</b> Permutaciones con repetición</a></li>
</ul></li>
<li class="chapter" data-level="1.11" data-path="probabilidad-y-experimentos-aleatorios.html"><a href="probabilidad-y-experimentos-aleatorios.html#frecuencia-relativa-y-probabilidad"><i class="fa fa-check"></i><b>1.11</b> Frecuencia relativa y probabilidad</a>
<ul>
<li class="chapter" data-level="1.11.1" data-path="probabilidad-y-experimentos-aleatorios.html"><a href="probabilidad-y-experimentos-aleatorios.html#ilustraci%C3%B3n-por-simulaci%C3%B3n"><i class="fa fa-check"></i><b>1.11.1</b> Ilustración por simulación</a></li>
</ul></li>
<li class="chapter" data-level="1.12" data-path="probabilidad-y-experimentos-aleatorios.html"><a href="probabilidad-y-experimentos-aleatorios.html#caso-de-estudio-eficacia-de-una-prueba-diagn%C3%B3stica"><i class="fa fa-check"></i><b>1.12</b> Caso de Estudio: Eficacia de una prueba diagnóstica</a>
<ul>
<li class="chapter" data-level="1.12.1" data-path="probabilidad-y-experimentos-aleatorios.html"><a href="probabilidad-y-experimentos-aleatorios.html#aplicaci%C3%B3n-del-teorema-de-bayes"><i class="fa fa-check"></i><b>1.12.1</b> Aplicación del Teorema de Bayes</a></li>
<li class="chapter" data-level="1.12.2" data-path="probabilidad-y-experimentos-aleatorios.html"><a href="probabilidad-y-experimentos-aleatorios.html#ejemplo-num%C3%A9rico"><i class="fa fa-check"></i><b>1.12.2</b> Ejemplo numérico</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="variables-aleatorias-y-distribuciones-de-probabilidad.html"><a href="variables-aleatorias-y-distribuciones-de-probabilidad.html"><i class="fa fa-check"></i><b>2</b> Variables aleatorias y Distribuciones de probabilidad</a>
<ul>
<li class="chapter" data-level="2.1" data-path="variables-aleatorias-y-distribuciones-de-probabilidad.html"><a href="variables-aleatorias-y-distribuciones-de-probabilidad.html#el-espacio-muestral-y-sus-elementos"><i class="fa fa-check"></i><b>2.1</b> El espacio muestral y sus elementos</a></li>
<li class="chapter" data-level="2.2" data-path="variables-aleatorias-y-distribuciones-de-probabilidad.html"><a href="variables-aleatorias-y-distribuciones-de-probabilidad.html#representaci%C3%B3n-num%C3%A9rica-de-los-sucesos-elementales.-variables-aleatorias"><i class="fa fa-check"></i><b>2.2</b> Representación numérica de los sucesos elementales. Variables aleatorias</a></li>
<li class="chapter" data-level="2.3" data-path="variables-aleatorias-y-distribuciones-de-probabilidad.html"><a href="variables-aleatorias-y-distribuciones-de-probabilidad.html#caracterizaci%C3%B3n-de-una-variable-aleatoria-a-trav%C3%A9s-de-la-probabilidad.-funci%C3%B3n-de-distribuci%C3%B3n"><i class="fa fa-check"></i><b>2.3</b> Caracterización de una variable aleatoria a través de la probabilidad. Función de distribución</a></li>
<li class="chapter" data-level="2.4" data-path="variables-aleatorias-y-distribuciones-de-probabilidad.html"><a href="variables-aleatorias-y-distribuciones-de-probabilidad.html#propiedades-de-la-funci%C3%B3n-de-distribuci%C3%B3n"><i class="fa fa-check"></i><b>2.4</b> Propiedades de la función de distribución</a></li>
<li class="chapter" data-level="2.5" data-path="variables-aleatorias-y-distribuciones-de-probabilidad.html"><a href="variables-aleatorias-y-distribuciones-de-probabilidad.html#clasificaci%C3%B3n-de-las-variables-aleatorias"><i class="fa fa-check"></i><b>2.5</b> Clasificación de las variables aleatorias</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="variables-aleatorias-y-distribuciones-de-probabilidad.html"><a href="variables-aleatorias-y-distribuciones-de-probabilidad.html#variables-aleatorias-discretas"><i class="fa fa-check"></i><b>2.5.1</b> Variables aleatorias discretas</a></li>
<li class="chapter" data-level="2.5.2" data-path="variables-aleatorias-y-distribuciones-de-probabilidad.html"><a href="variables-aleatorias-y-distribuciones-de-probabilidad.html#variables-aleatorias-continuas"><i class="fa fa-check"></i><b>2.5.2</b> Variables aleatorias continuas</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="variables-aleatorias-y-distribuciones-de-probabilidad.html"><a href="variables-aleatorias-y-distribuciones-de-probabilidad.html#variable-aleatoria-discretas"><i class="fa fa-check"></i><b>2.6</b> Variable aleatoria discretas</a>
<ul>
<li class="chapter" data-level="2.6.1" data-path="variables-aleatorias-y-distribuciones-de-probabilidad.html"><a href="variables-aleatorias-y-distribuciones-de-probabilidad.html#caracterizaci%C3%B3n-de-las-v.a.-discretas"><i class="fa fa-check"></i><b>2.6.1</b> Caracterización de las v.a. discretas</a></li>
<li class="chapter" data-level="2.6.2" data-path="variables-aleatorias-y-distribuciones-de-probabilidad.html"><a href="variables-aleatorias-y-distribuciones-de-probabilidad.html#propiedades-de-la-funci%C3%B3n-de-densidad-discreta"><i class="fa fa-check"></i><b>2.6.2</b> Propiedades de la función de densidad discreta</a></li>
<li class="chapter" data-level="2.6.3" data-path="variables-aleatorias-y-distribuciones-de-probabilidad.html"><a href="variables-aleatorias-y-distribuciones-de-probabilidad.html#relaciones-entre-la-funci%C3%B3n-de-distribuci%C3%B3n-y-la-funci%C3%B3n-de-densidad-discreta.-probabilidad-de-intervalos."><i class="fa fa-check"></i><b>2.6.3</b> Relaciones entre la función de distribución y la función de densidad discreta. <br> Probabilidad de intervalos.</a></li>
</ul></li>
<li class="chapter" data-level="2.7" data-path="variables-aleatorias-y-distribuciones-de-probabilidad.html"><a href="variables-aleatorias-y-distribuciones-de-probabilidad.html#variables-aleatorias-continuas-1"><i class="fa fa-check"></i><b>2.7</b> Variables aleatorias continuas</a>
<ul>
<li class="chapter" data-level="2.7.1" data-path="variables-aleatorias-y-distribuciones-de-probabilidad.html"><a href="variables-aleatorias-y-distribuciones-de-probabilidad.html#funci%C3%B3n-de-densidad-continua"><i class="fa fa-check"></i><b>2.7.1</b> Función de densidad continua</a></li>
<li class="chapter" data-level="2.7.2" data-path="variables-aleatorias-y-distribuciones-de-probabilidad.html"><a href="variables-aleatorias-y-distribuciones-de-probabilidad.html#relaciones-entre-la-funci%C3%B3n-de-distribuci%C3%B3n-y-la-funci%C3%B3n-de-densidad."><i class="fa fa-check"></i><b>2.7.2</b> Relaciones entre la función de distribución y la función de densidad.</a></li>
</ul></li>
<li class="chapter" data-level="2.8" data-path="variables-aleatorias-y-distribuciones-de-probabilidad.html"><a href="variables-aleatorias-y-distribuciones-de-probabilidad.html#caracterizaci%C3%B3n-de-una-variable-aleatoria-a-trav%C3%A9s-de-par%C3%A1metros"><i class="fa fa-check"></i><b>2.8</b> Caracterización de una variable aleatoria a través de parámetros</a></li>
<li class="chapter" data-level="2.9" data-path="variables-aleatorias-y-distribuciones-de-probabilidad.html"><a href="variables-aleatorias-y-distribuciones-de-probabilidad.html#esperanza-de-una-variable-aleatoria-discreta"><i class="fa fa-check"></i><b>2.9</b> Esperanza de una variable aleatoria discreta</a></li>
<li class="chapter" data-level="2.10" data-path="variables-aleatorias-y-distribuciones-de-probabilidad.html"><a href="variables-aleatorias-y-distribuciones-de-probabilidad.html#esperanza-de-una-variable-aleatoria-continua"><i class="fa fa-check"></i><b>2.10</b> Esperanza de una variable aleatoria continua</a></li>
<li class="chapter" data-level="2.11" data-path="variables-aleatorias-y-distribuciones-de-probabilidad.html"><a href="variables-aleatorias-y-distribuciones-de-probabilidad.html#propiedades-de-la-esperanza-matem%C3%A1tica"><i class="fa fa-check"></i><b>2.11</b> Propiedades de la esperanza matemática</a>
<ul>
<li class="chapter" data-level="2.11.1" data-path="variables-aleatorias-y-distribuciones-de-probabilidad.html"><a href="variables-aleatorias-y-distribuciones-de-probabilidad.html#linealidad-de-la-esperanza-matem%C3%A1tica"><i class="fa fa-check"></i><b>2.11.1</b> Linealidad de la esperanza matemática</a></li>
<li class="chapter" data-level="2.11.2" data-path="variables-aleatorias-y-distribuciones-de-probabilidad.html"><a href="variables-aleatorias-y-distribuciones-de-probabilidad.html#esperanza-del-producto"><i class="fa fa-check"></i><b>2.11.2</b> Esperanza del producto</a></li>
</ul></li>
<li class="chapter" data-level="2.12" data-path="variables-aleatorias-y-distribuciones-de-probabilidad.html"><a href="variables-aleatorias-y-distribuciones-de-probabilidad.html#varianza-de-una-variable-aleatoria"><i class="fa fa-check"></i><b>2.12</b> Varianza de una variable aleatoria</a>
<ul>
<li class="chapter" data-level="2.12.1" data-path="variables-aleatorias-y-distribuciones-de-probabilidad.html"><a href="variables-aleatorias-y-distribuciones-de-probabilidad.html#propiedades-de-la-varianza"><i class="fa fa-check"></i><b>2.12.1</b> Propiedades de la varianza</a></li>
</ul></li>
<li class="chapter" data-level="2.13" data-path="variables-aleatorias-y-distribuciones-de-probabilidad.html"><a href="variables-aleatorias-y-distribuciones-de-probabilidad.html#momentos-de-orden-k-de-una-variable-aleatoria"><i class="fa fa-check"></i><b>2.13</b> Momentos (de orden <span class="math inline">\(k\)</span>) de una variable aleatoria</a></li>
<li class="chapter" data-level="2.14" data-path="variables-aleatorias-y-distribuciones-de-probabilidad.html"><a href="variables-aleatorias-y-distribuciones-de-probabilidad.html#definici%C3%B3n-formal-de-variable-aleatoria"><i class="fa fa-check"></i><b>2.14</b> Definición formal de variable aleatoria</a></li>
<li class="chapter" data-level="2.15" data-path="variables-aleatorias-y-distribuciones-de-probabilidad.html"><a href="variables-aleatorias-y-distribuciones-de-probabilidad.html#caso-pr%C3%A1ctico-lanzamiento-de-dos-dados"><i class="fa fa-check"></i><b>2.15</b> Caso práctico: Lanzamiento de dos dados</a>
<ul>
<li class="chapter" data-level="2.15.1" data-path="variables-aleatorias-y-distribuciones-de-probabilidad.html"><a href="variables-aleatorias-y-distribuciones-de-probabilidad.html#espacio-muestral"><i class="fa fa-check"></i><b>2.15.1</b> Espacio muestral</a></li>
<li class="chapter" data-level="2.15.2" data-path="variables-aleatorias-y-distribuciones-de-probabilidad.html"><a href="variables-aleatorias-y-distribuciones-de-probabilidad.html#representaci%C3%B3n-num%C3%A9rica"><i class="fa fa-check"></i><b>2.15.2</b> Representación numérica</a></li>
<li class="chapter" data-level="2.15.3" data-path="variables-aleatorias-y-distribuciones-de-probabilidad.html"><a href="variables-aleatorias-y-distribuciones-de-probabilidad.html#algunas-probabilidades"><i class="fa fa-check"></i><b>2.15.3</b> Algunas probabilidades</a></li>
<li class="chapter" data-level="2.15.4" data-path="variables-aleatorias-y-distribuciones-de-probabilidad.html"><a href="variables-aleatorias-y-distribuciones-de-probabilidad.html#funci%C3%B3n-de-distribuci%C3%B3n"><i class="fa fa-check"></i><b>2.15.4</b> Función de distribución</a></li>
<li class="chapter" data-level="2.15.5" data-path="variables-aleatorias-y-distribuciones-de-probabilidad.html"><a href="variables-aleatorias-y-distribuciones-de-probabilidad.html#clasificaci%C3%B3n-de-las-variables"><i class="fa fa-check"></i><b>2.15.5</b> Clasificación de las variables</a></li>
<li class="chapter" data-level="2.15.6" data-path="variables-aleatorias-y-distribuciones-de-probabilidad.html"><a href="variables-aleatorias-y-distribuciones-de-probabilidad.html#funci%C3%B3n-de-densidad-discreta"><i class="fa fa-check"></i><b>2.15.6</b> Función de densidad discreta</a></li>
<li class="chapter" data-level="2.15.7" data-path="variables-aleatorias-y-distribuciones-de-probabilidad.html"><a href="variables-aleatorias-y-distribuciones-de-probabilidad.html#probabilidad-de-intervalos-1"><i class="fa fa-check"></i><b>2.15.7</b> Probabilidad de intervalos</a></li>
<li class="chapter" data-level="2.15.8" data-path="variables-aleatorias-y-distribuciones-de-probabilidad.html"><a href="variables-aleatorias-y-distribuciones-de-probabilidad.html#esperanza"><i class="fa fa-check"></i><b>2.15.8</b> Esperanza</a></li>
<li class="chapter" data-level="2.15.9" data-path="variables-aleatorias-y-distribuciones-de-probabilidad.html"><a href="variables-aleatorias-y-distribuciones-de-probabilidad.html#esperanza-de-un-juego"><i class="fa fa-check"></i><b>2.15.9</b> Esperanza de un juego</a></li>
<li class="chapter" data-level="2.15.10" data-path="variables-aleatorias-y-distribuciones-de-probabilidad.html"><a href="variables-aleatorias-y-distribuciones-de-probabilidad.html#esperanza-con-recorrido-infinito"><i class="fa fa-check"></i><b>2.15.10</b> Esperanza con recorrido infinito</a></li>
<li class="chapter" data-level="2.15.11" data-path="variables-aleatorias-y-distribuciones-de-probabilidad.html"><a href="variables-aleatorias-y-distribuciones-de-probabilidad.html#esperanza-infinita"><i class="fa fa-check"></i><b>2.15.11</b> Esperanza infinita</a></li>
<li class="chapter" data-level="2.15.12" data-path="variables-aleatorias-y-distribuciones-de-probabilidad.html"><a href="variables-aleatorias-y-distribuciones-de-probabilidad.html#varianza"><i class="fa fa-check"></i><b>2.15.12</b> Varianza</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="distribuciones-notables.html"><a href="distribuciones-notables.html"><i class="fa fa-check"></i><b>3</b> Distribuciones Notables</a>
<ul>
<li class="chapter" data-level="3.1" data-path="distribuciones-notables.html"><a href="distribuciones-notables.html#distribuciones-discretas"><i class="fa fa-check"></i><b>3.1</b> Distribuciones discretas</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="distribuciones-notables.html"><a href="distribuciones-notables.html#la-distribuci%C3%B3n-de-bernouilli"><i class="fa fa-check"></i><b>3.1.1</b> La distribución de Bernouilli</a></li>
<li class="chapter" data-level="3.1.2" data-path="distribuciones-notables.html"><a href="distribuciones-notables.html#la-distribuci%C3%B3n-binomial"><i class="fa fa-check"></i><b>3.1.2</b> La distribución Binomial</a></li>
<li class="chapter" data-level="3.1.3" data-path="distribuciones-notables.html"><a href="distribuciones-notables.html#la-distribuci%C3%B3n-de-poisson"><i class="fa fa-check"></i><b>3.1.3</b> La distribución de Poisson</a></li>
<li class="chapter" data-level="3.1.4" data-path="distribuciones-notables.html"><a href="distribuciones-notables.html#la-distribuci%C3%B3n-uniforme-discreta"><i class="fa fa-check"></i><b>3.1.4</b> La distribución Uniforme discreta</a></li>
<li class="chapter" data-level="3.1.5" data-path="distribuciones-notables.html"><a href="distribuciones-notables.html#la-distribuci%C3%B3n-hipergeom%C3%A9trica"><i class="fa fa-check"></i><b>3.1.5</b> La distribución Hipergeométrica</a></li>
<li class="chapter" data-level="3.1.6" data-path="distribuciones-notables.html"><a href="distribuciones-notables.html#la-distribuci%C3%B3n-geom%C3%A9trica-o-de-pascal"><i class="fa fa-check"></i><b>3.1.6</b> La distribución Geométrica o de Pascal</a></li>
<li class="chapter" data-level="3.1.7" data-path="distribuciones-notables.html"><a href="distribuciones-notables.html#la-distribuci%C3%B3n-binomial-negativa"><i class="fa fa-check"></i><b>3.1.7</b> La distribución Binomial negativa</a></li>
<li class="chapter" data-level="3.1.8" data-path="distribuciones-notables.html"><a href="distribuciones-notables.html#tabla-resumen-de-las-distribuciones-discretas-principales"><i class="fa fa-check"></i><b>3.1.8</b> Tabla resumen de las distribuciones discretas principales</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="distribuciones-notables.html"><a href="distribuciones-notables.html#distribuciones-continuas"><i class="fa fa-check"></i><b>3.2</b> Distribuciones Continuas</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="distribuciones-notables.html"><a href="distribuciones-notables.html#la-distribuci%C3%B3n-uniforme"><i class="fa fa-check"></i><b>3.2.1</b> La distribución Uniforme</a></li>
<li class="chapter" data-level="3.2.2" data-path="distribuciones-notables.html"><a href="distribuciones-notables.html#la-distribuci%C3%B3n-exponencial"><i class="fa fa-check"></i><b>3.2.2</b> La distribución Exponencial</a></li>
<li class="chapter" data-level="3.2.3" data-path="distribuciones-notables.html"><a href="distribuciones-notables.html#la-distribuci%C3%B3n-normal"><i class="fa fa-check"></i><b>3.2.3</b> La distribución Normal</a></li>
<li class="chapter" data-level="3.2.4" data-path="distribuciones-notables.html"><a href="distribuciones-notables.html#la-distribuci%C3%B3n-gamma"><i class="fa fa-check"></i><b>3.2.4</b> La distribución Gamma</a></li>
<li class="chapter" data-level="3.2.5" data-path="distribuciones-notables.html"><a href="distribuciones-notables.html#la-distribuci%C3%B3n-de-cauchy"><i class="fa fa-check"></i><b>3.2.5</b> La distribución de Cauchy</a></li>
<li class="chapter" data-level="3.2.6" data-path="distribuciones-notables.html"><a href="distribuciones-notables.html#la-distribuci%C3%B3n-de-weibull"><i class="fa fa-check"></i><b>3.2.6</b> La distribución de Weibull</a></li>
<li class="chapter" data-level="3.2.7" data-path="distribuciones-notables.html"><a href="distribuciones-notables.html#tabla-resumen-de-las-principales-distribuciones-continuas"><i class="fa fa-check"></i><b>3.2.7</b> Tabla resumen de las principales distribuciones continuas</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="distribuciones-notables.html"><a href="distribuciones-notables.html#distribuciones-con-r-y-python"><i class="fa fa-check"></i><b>3.3</b> Distribuciones con R (y Python)</a></li>
<li class="chapter" data-level="3.4" data-path="distribuciones-notables.html"><a href="distribuciones-notables.html#la-familia-exponencial-de-distribuciones"><i class="fa fa-check"></i><b>3.4</b> La familia exponencial de distribuciones</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="distribuciones-notables.html"><a href="distribuciones-notables.html#ejemplos-de-distribuciones-de-esta-familia"><i class="fa fa-check"></i><b>3.4.1</b> Ejemplos de distribuciones de esta familia</a></li>
<li class="chapter" data-level="3.4.2" data-path="distribuciones-notables.html"><a href="distribuciones-notables.html#distribuci%C3%B3n-binomial"><i class="fa fa-check"></i><b>3.4.2</b> Distribución Binomial</a></li>
<li class="chapter" data-level="3.4.3" data-path="distribuciones-notables.html"><a href="distribuciones-notables.html#importancia-y-utilidad-de-la-familia-exponencial"><i class="fa fa-check"></i><b>3.4.3</b> Importancia y utilidad de la familia exponencial</a></li>
<li class="chapter" data-level="3.4.4" data-path="distribuciones-notables.html"><a href="distribuciones-notables.html#los-modelos-lineales-generalizados-glms"><i class="fa fa-check"></i><b>3.4.4</b> Los modelos lineales generalizados (GLMs)</a></li>
<li class="chapter" data-level="3.4.5" data-path="distribuciones-notables.html"><a href="distribuciones-notables.html#estimaci%C3%B3n-en-la-familia-exponencial"><i class="fa fa-check"></i><b>3.4.5</b> Estimación en la familia exponencial</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="distribuciones-de-probabilidad-multidimensionales.html"><a href="distribuciones-de-probabilidad-multidimensionales.html"><i class="fa fa-check"></i><b>4</b> Distribuciones de probabilidad multidimensionales</a>
<ul>
<li class="chapter" data-level="4.1" data-path="distribuciones-de-probabilidad-multidimensionales.html"><a href="distribuciones-de-probabilidad-multidimensionales.html#distribuciones-conjuntas-de-probabilidades"><i class="fa fa-check"></i><b>4.1</b> Distribuciones conjuntas de probabilidades</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="distribuciones-de-probabilidad-multidimensionales.html"><a href="distribuciones-de-probabilidad-multidimensionales.html#variable-aleatoria-bivariante"><i class="fa fa-check"></i><b>4.1.1</b> Variable aleatoria bivariante</a></li>
<li class="chapter" data-level="4.1.2" data-path="distribuciones-de-probabilidad-multidimensionales.html"><a href="distribuciones-de-probabilidad-multidimensionales.html#funci%C3%B3n-de-distribuci%C3%B3n-bivariante"><i class="fa fa-check"></i><b>4.1.2</b> Función de distribución bivariante</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="distribuciones-de-probabilidad-multidimensionales.html"><a href="distribuciones-de-probabilidad-multidimensionales.html#variable-aleatorias-bivariantes-discretas"><i class="fa fa-check"></i><b>4.2</b> Variable aleatorias bivariantes discretas</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="distribuciones-de-probabilidad-multidimensionales.html"><a href="distribuciones-de-probabilidad-multidimensionales.html#funci%C3%B3n-de-masa-de-probabilidad-discreta-fmp"><i class="fa fa-check"></i><b>4.2.1</b> Función de masa de probabilidad discreta (fmp)</a></li>
<li class="chapter" data-level="4.2.2" data-path="distribuciones-de-probabilidad-multidimensionales.html"><a href="distribuciones-de-probabilidad-multidimensionales.html#propiedades-de-la-fmp-bivariante"><i class="fa fa-check"></i><b>4.2.2</b> Propiedades de la fmp bivariante</a></li>
<li class="chapter" data-level="4.2.3" data-path="distribuciones-de-probabilidad-multidimensionales.html"><a href="distribuciones-de-probabilidad-multidimensionales.html#ejemplo-de-distribuci%C3%B3n-bivariante-discreta"><i class="fa fa-check"></i><b>4.2.3</b> Ejemplo de distribución bivariante discreta</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="distribuciones-de-probabilidad-multidimensionales.html"><a href="distribuciones-de-probabilidad-multidimensionales.html#la-distribuci%C3%B3n-multinomial"><i class="fa fa-check"></i><b>4.3</b> La distribución multinomial</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="distribuciones-de-probabilidad-multidimensionales.html"><a href="distribuciones-de-probabilidad-multidimensionales.html#generaci%C3%B3n-de-las-observaciones"><i class="fa fa-check"></i><b>4.3.1</b> Generación de las observaciones</a></li>
<li class="chapter" data-level="4.3.2" data-path="distribuciones-de-probabilidad-multidimensionales.html"><a href="distribuciones-de-probabilidad-multidimensionales.html#funcion-de-masa-de-probabilidad-de-la-distribuci%C3%B3n-multinomial"><i class="fa fa-check"></i><b>4.3.2</b> Funcion de masa de probabilidad de la distribución multinomial</a></li>
<li class="chapter" data-level="4.3.3" data-path="distribuciones-de-probabilidad-multidimensionales.html"><a href="distribuciones-de-probabilidad-multidimensionales.html#relaci%C3%B3n-con-la-distribuci%C3%B3n-binomial"><i class="fa fa-check"></i><b>4.3.3</b> Relación con la distribución binomial</a></li>
<li class="chapter" data-level="4.3.4" data-path="distribuciones-de-probabilidad-multidimensionales.html"><a href="distribuciones-de-probabilidad-multidimensionales.html#un-caso-particular-la-distribuci%C3%B3n-trinomial"><i class="fa fa-check"></i><b>4.3.4</b> Un caso particular: La distribución trinomial</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="distribuciones-de-probabilidad-multidimensionales.html"><a href="distribuciones-de-probabilidad-multidimensionales.html#distribuciones-marginales"><i class="fa fa-check"></i><b>4.4</b> Distribuciones marginales</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="distribuciones-de-probabilidad-multidimensionales.html"><a href="distribuciones-de-probabilidad-multidimensionales.html#las-marginales-est%C3%A1n-en-los-m%C3%A1rgenes"><i class="fa fa-check"></i><b>4.4.1</b> Las marginales están en los márgenes</a></li>
<li class="chapter" data-level="4.4.2" data-path="distribuciones-de-probabilidad-multidimensionales.html"><a href="distribuciones-de-probabilidad-multidimensionales.html#densidades-marginales-discretas"><i class="fa fa-check"></i><b>4.4.2</b> Densidades marginales discretas</a></li>
<li class="chapter" data-level="4.4.3" data-path="distribuciones-de-probabilidad-multidimensionales.html"><a href="distribuciones-de-probabilidad-multidimensionales.html#trinomial-m5-0.6-0.2-distribuciones-marginales"><i class="fa fa-check"></i><b>4.4.3</b> Trinomial M(5; 0.6, 0.2): Distribuciones marginales</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="distribuciones-de-probabilidad-multidimensionales.html"><a href="distribuciones-de-probabilidad-multidimensionales.html#distribuciones-condicionales"><i class="fa fa-check"></i><b>4.5</b> Distribuciones condicionales</a>
<ul>
<li class="chapter" data-level="4.5.1" data-path="distribuciones-de-probabilidad-multidimensionales.html"><a href="distribuciones-de-probabilidad-multidimensionales.html#densidad-condicional"><i class="fa fa-check"></i><b>4.5.1</b> Densidad condicional</a></li>
<li class="chapter" data-level="4.5.2" data-path="distribuciones-de-probabilidad-multidimensionales.html"><a href="distribuciones-de-probabilidad-multidimensionales.html#trinomial-m5-0.6-0.2-distribuci%C3%B3n-condicional"><i class="fa fa-check"></i><b>4.5.2</b> Trinomial M(5; 0.6, 0.2): Distribución condicional</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="distribuciones-de-probabilidad-multidimensionales.html"><a href="distribuciones-de-probabilidad-multidimensionales.html#vectores-aleatorios-absolutamente-continuos"><i class="fa fa-check"></i><b>4.6</b> Vectores aleatorios absolutamente continuos</a>
<ul>
<li class="chapter" data-level="4.6.1" data-path="distribuciones-de-probabilidad-multidimensionales.html"><a href="distribuciones-de-probabilidad-multidimensionales.html#propiedades-de-la-funci%C3%B3n-de-densidad-conjunta"><i class="fa fa-check"></i><b>4.6.1</b> Propiedades de la función de densidad conjunta</a></li>
<li class="chapter" data-level="4.6.2" data-path="distribuciones-de-probabilidad-multidimensionales.html"><a href="distribuciones-de-probabilidad-multidimensionales.html#densidades-marginales-en-el-caso-continuo"><i class="fa fa-check"></i><b>4.6.2</b> Densidades marginales en el caso continuo</a></li>
<li class="chapter" data-level="4.6.3" data-path="distribuciones-de-probabilidad-multidimensionales.html"><a href="distribuciones-de-probabilidad-multidimensionales.html#densidad-condicional-en-el-caso-continuo"><i class="fa fa-check"></i><b>4.6.3</b> Densidad condicional en el caso continuo</a></li>
<li class="chapter" data-level="4.6.4" data-path="distribuciones-de-probabilidad-multidimensionales.html"><a href="distribuciones-de-probabilidad-multidimensionales.html#la-distribuci%C3%B3n-normal-bivariante"><i class="fa fa-check"></i><b>4.6.4</b> La Distribución Normal Bivariante</a></li>
<li class="chapter" data-level="4.6.5" data-path="distribuciones-de-probabilidad-multidimensionales.html"><a href="distribuciones-de-probabilidad-multidimensionales.html#distribuciones-condicionales-1"><i class="fa fa-check"></i><b>4.6.5</b> Distribuciones Condicionales</a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="distribuciones-de-probabilidad-multidimensionales.html"><a href="distribuciones-de-probabilidad-multidimensionales.html#independencia-de-variables-aleatorias"><i class="fa fa-check"></i><b>4.7</b> Independencia de variables aleatorias</a>
<ul>
<li class="chapter" data-level="4.7.1" data-path="distribuciones-de-probabilidad-multidimensionales.html"><a href="distribuciones-de-probabilidad-multidimensionales.html#primera-caracterizaci%C3%B3n-de-la-independencia"><i class="fa fa-check"></i><b>4.7.1</b> Primera caracterización de la independencia</a></li>
<li class="chapter" data-level="4.7.2" data-path="distribuciones-de-probabilidad-multidimensionales.html"><a href="distribuciones-de-probabilidad-multidimensionales.html#propiedades-de-las-variables-independientes"><i class="fa fa-check"></i><b>4.7.2</b> Propiedades de las variables independientes</a></li>
</ul></li>
<li class="chapter" data-level="4.8" data-path="distribuciones-de-probabilidad-multidimensionales.html"><a href="distribuciones-de-probabilidad-multidimensionales.html#momentos-de-vectores-aleatorios"><i class="fa fa-check"></i><b>4.8</b> Momentos de vectores aleatorios</a>
<ul>
<li class="chapter" data-level="4.8.1" data-path="distribuciones-de-probabilidad-multidimensionales.html"><a href="distribuciones-de-probabilidad-multidimensionales.html#esperanza-de-un-vector-aleatorio-o-vector-de-medias"><i class="fa fa-check"></i><b>4.8.1</b> Esperanza de un vector aleatorio o vector de medias</a></li>
<li class="chapter" data-level="4.8.2" data-path="distribuciones-de-probabilidad-multidimensionales.html"><a href="distribuciones-de-probabilidad-multidimensionales.html#covarianza-entre-dos-variables-aleatorias"><i class="fa fa-check"></i><b>4.8.2</b> Covarianza entre dos variables aleatorias</a></li>
<li class="chapter" data-level="4.8.3" data-path="distribuciones-de-probabilidad-multidimensionales.html"><a href="distribuciones-de-probabilidad-multidimensionales.html#covarianza-y-correlaci%C3%B3n"><i class="fa fa-check"></i><b>4.8.3</b> Covarianza y correlación</a></li>
<li class="chapter" data-level="4.8.4" data-path="distribuciones-de-probabilidad-multidimensionales.html"><a href="distribuciones-de-probabilidad-multidimensionales.html#matriz-de-varianzas-covarianzas"><i class="fa fa-check"></i><b>4.8.4</b> Matriz de varianzas-covarianzas</a></li>
<li class="chapter" data-level="4.8.5" data-path="distribuciones-de-probabilidad-multidimensionales.html"><a href="distribuciones-de-probabilidad-multidimensionales.html#matriz-de-correlaciones"><i class="fa fa-check"></i><b>4.8.5</b> Matriz de correlaciones</a></li>
<li class="chapter" data-level="4.8.6" data-path="distribuciones-de-probabilidad-multidimensionales.html"><a href="distribuciones-de-probabilidad-multidimensionales.html#segunda-caracterizaci%C3%B3n-de-la-independencia"><i class="fa fa-check"></i><b>4.8.6</b> Segunda caracterización de la independencia</a></li>
<li class="chapter" data-level="4.8.7" data-path="distribuciones-de-probabilidad-multidimensionales.html"><a href="distribuciones-de-probabilidad-multidimensionales.html#relaci%C3%B3n-entre-incorrelaci%C3%B3n-e-independencia"><i class="fa fa-check"></i><b>4.8.7</b> Relación entre incorrelación e independencia</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="grandes-muestras.html"><a href="grandes-muestras.html"><i class="fa fa-check"></i><b>5</b> Grandes muestras</a>
<ul>
<li class="chapter" data-level="5.1" data-path="grandes-muestras.html"><a href="grandes-muestras.html#introducci%C3%B3n-aproximaciones-asint%C3%B3ticas"><i class="fa fa-check"></i><b>5.1</b> Introducción: Aproximaciones asintóticas</a></li>
<li class="chapter" data-level="5.2" data-path="grandes-muestras.html"><a href="grandes-muestras.html#ley-de-los-grandes-n%C3%BAmeros-ley-d%C3%A9bil"><i class="fa fa-check"></i><b>5.2</b> Ley de los Grandes Números (Ley débil)</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="grandes-muestras.html"><a href="grandes-muestras.html#ejemplo-3"><i class="fa fa-check"></i><b>5.2.1</b> Ejemplo</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="grandes-muestras.html"><a href="grandes-muestras.html#el-teorema-central-del-l%C3%ADmite"><i class="fa fa-check"></i><b>5.3</b> El teorema central del límite</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="grandes-muestras.html"><a href="grandes-muestras.html#sumas-de-variables-aleatorias"><i class="fa fa-check"></i><b>5.3.1</b> Sumas de variables aleatorias</a></li>
<li class="chapter" data-level="5.3.2" data-path="grandes-muestras.html"><a href="grandes-muestras.html#definici%C3%B3n-de-convergencia-en-ley"><i class="fa fa-check"></i><b>5.3.2</b> Definición de convergencia en ley</a></li>
<li class="chapter" data-level="5.3.3" data-path="grandes-muestras.html"><a href="grandes-muestras.html#enunciado-del-teorema-central-del-l%C3%ADmite"><i class="fa fa-check"></i><b>5.3.3</b> Enunciado del teorema central del límite</a></li>
<li class="chapter" data-level="5.3.4" data-path="grandes-muestras.html"><a href="grandes-muestras.html#algunos-ejemplos-de-aplicaci%C3%B3n-del-tcl"><i class="fa fa-check"></i><b>5.3.4</b> Algunos ejemplos de aplicación del TCL</a></li>
<li class="chapter" data-level="5.3.5" data-path="grandes-muestras.html"><a href="grandes-muestras.html#casos-particulares-m%C3%A1s-notables"><i class="fa fa-check"></i><b>5.3.5</b> Casos particulares más notables</a></li>
<li class="chapter" data-level="5.3.6" data-path="grandes-muestras.html"><a href="grandes-muestras.html#interpretaci%C3%B3n-del-teorema-central-del-l%C3%ADmite"><i class="fa fa-check"></i><b>5.3.6</b> Interpretación del teorema central del límite</a></li>
<li class="chapter" data-level="5.3.7" data-path="grandes-muestras.html"><a href="grandes-muestras.html#acerca-de-las-variables-aproximadamente-normales"><i class="fa fa-check"></i><b>5.3.7</b> Acerca de las variables aproximadamente normales</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="introducción-a-la-inferencia-estadística.html"><a href="introducción-a-la-inferencia-estadística.html"><i class="fa fa-check"></i><b>6</b> Introducción a la inferencia estadística</a>
<ul>
<li class="chapter" data-level="6.1" data-path="introducción-a-la-inferencia-estadística.html"><a href="introducción-a-la-inferencia-estadística.html#inferencia-estad%C3%ADstica"><i class="fa fa-check"></i><b>6.1</b> Inferencia estadística</a></li>
<li class="chapter" data-level="6.2" data-path="introducción-a-la-inferencia-estadística.html"><a href="introducción-a-la-inferencia-estadística.html#problemas-de-inferencia-estad%C3%ADstica"><i class="fa fa-check"></i><b>6.2</b> Problemas de inferencia estadística</a></li>
<li class="chapter" data-level="6.3" data-path="introducción-a-la-inferencia-estadística.html"><a href="introducción-a-la-inferencia-estadística.html#distribuci%C3%B3n-de-la-poblaci%C3%B3n"><i class="fa fa-check"></i><b>6.3</b> Distribución de la población</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="introducción-a-la-inferencia-estadística.html"><a href="introducción-a-la-inferencia-estadística.html#ejemplo-4"><i class="fa fa-check"></i><b>6.3.1</b> Ejemplo</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="introducción-a-la-inferencia-estadística.html"><a href="introducción-a-la-inferencia-estadística.html#muestra-aleatoria-simple"><i class="fa fa-check"></i><b>6.4</b> Muestra aleatoria simple</a>
<ul>
<li class="chapter" data-level="6.4.1" data-path="introducción-a-la-inferencia-estadística.html"><a href="introducción-a-la-inferencia-estadística.html#definici%C3%B3n-muestra-aleatoria-simple"><i class="fa fa-check"></i><b>6.4.1</b> Definición (Muestra aleatoria simple)</a></li>
<li class="chapter" data-level="6.4.2" data-path="introducción-a-la-inferencia-estadística.html"><a href="introducción-a-la-inferencia-estadística.html#distribuci%C3%B3n-de-la-muestra"><i class="fa fa-check"></i><b>6.4.2</b> Distribución de la muestra</a></li>
<li class="chapter" data-level="6.4.3" data-path="introducción-a-la-inferencia-estadística.html"><a href="introducción-a-la-inferencia-estadística.html#casos-particulares"><i class="fa fa-check"></i><b>6.4.3</b> Casos particulares</a></li>
<li class="chapter" data-level="6.4.4" data-path="introducción-a-la-inferencia-estadística.html"><a href="introducción-a-la-inferencia-estadística.html#ejemplo-5"><i class="fa fa-check"></i><b>6.4.4</b> Ejemplo</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="introducción-a-la-inferencia-estadística.html"><a href="introducción-a-la-inferencia-estadística.html#estad%C3%ADsticos"><i class="fa fa-check"></i><b>6.5</b> Estadísticos</a>
<ul>
<li class="chapter" data-level="6.5.1" data-path="introducción-a-la-inferencia-estadística.html"><a href="introducción-a-la-inferencia-estadística.html#definici%C3%B3n"><i class="fa fa-check"></i><b>6.5.1</b> Definición</a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="introducción-a-la-inferencia-estadística.html"><a href="introducción-a-la-inferencia-estadística.html#distribuci%C3%B3n-en-el-muestreo-de-un-estad%C3%ADstico"><i class="fa fa-check"></i><b>6.6</b> Distribución en el muestreo de un estadístico</a>
<ul>
<li class="chapter" data-level="6.6.1" data-path="introducción-a-la-inferencia-estadística.html"><a href="introducción-a-la-inferencia-estadística.html#ejemplo-6"><i class="fa fa-check"></i><b>6.6.1</b> Ejemplo</a></li>
<li class="chapter" data-level="6.6.2" data-path="introducción-a-la-inferencia-estadística.html"><a href="introducción-a-la-inferencia-estadística.html#ejemplo-7"><i class="fa fa-check"></i><b>6.6.2</b> Ejemplo</a></li>
<li class="chapter" data-level="6.6.3" data-path="introducción-a-la-inferencia-estadística.html"><a href="introducción-a-la-inferencia-estadística.html#ejemplo-8"><i class="fa fa-check"></i><b>6.6.3</b> Ejemplo</a></li>
</ul></li>
<li class="chapter" data-level="6.7" data-path="introducción-a-la-inferencia-estadística.html"><a href="introducción-a-la-inferencia-estadística.html#la-distribuci%C3%B3n-emp%C3%ADrica"><i class="fa fa-check"></i><b>6.7</b> La distribución empírica</a>
<ul>
<li class="chapter" data-level="6.7.1" data-path="introducción-a-la-inferencia-estadística.html"><a href="introducción-a-la-inferencia-estadística.html#ejemplo-9"><i class="fa fa-check"></i><b>6.7.1</b> Ejemplo</a></li>
</ul></li>
<li class="chapter" data-level="6.8" data-path="introducción-a-la-inferencia-estadística.html"><a href="introducción-a-la-inferencia-estadística.html#momentos-muestrales"><i class="fa fa-check"></i><b>6.8</b> Momentos muestrales</a>
<ul>
<li class="chapter" data-level="6.8.1" data-path="introducción-a-la-inferencia-estadística.html"><a href="introducción-a-la-inferencia-estadística.html#observaciones"><i class="fa fa-check"></i><b>6.8.1</b> Observaciones</a></li>
</ul></li>
<li class="chapter" data-level="6.9" data-path="introducción-a-la-inferencia-estadística.html"><a href="introducción-a-la-inferencia-estadística.html#distribuci%C3%B3n-en-el-muestreo-de-los-momentos-muestrales"><i class="fa fa-check"></i><b>6.9</b> Distribución en el muestreo de los momentos muestrales</a></li>
<li class="chapter" data-level="6.10" data-path="introducción-a-la-inferencia-estadística.html"><a href="introducción-a-la-inferencia-estadística.html#muestreo-en-poblaciones-normales"><i class="fa fa-check"></i><b>6.10</b> Muestreo en poblaciones normales</a>
<ul>
<li class="chapter" data-level="6.10.1" data-path="introducción-a-la-inferencia-estadística.html"><a href="introducción-a-la-inferencia-estadística.html#la-distribuci%C3%B3n-chi-cuadrado-y-la-varianza-muestral"><i class="fa fa-check"></i><b>6.10.1</b> La distribución chi-cuadrado y la varianza muestral</a></li>
<li class="chapter" data-level="6.10.2" data-path="introducción-a-la-inferencia-estadística.html"><a href="introducción-a-la-inferencia-estadística.html#la-distribuci%C3%B3n-t-de-student-y-el-estad%C3%ADstico-t"><i class="fa fa-check"></i><b>6.10.2</b> La distribución t de Student y el estadístico <span class="math inline">\(T\)</span></a></li>
<li class="chapter" data-level="6.10.3" data-path="introducción-a-la-inferencia-estadística.html"><a href="introducción-a-la-inferencia-estadística.html#la-distribuci%C3%B3n-f-de-fisher-y-la-raz%C3%B3n-de-varianzas."><i class="fa fa-check"></i><b>6.10.3</b> La distribución F de Fisher y la razón de varianzas.</a></li>
</ul></li>
<li class="chapter" data-level="6.11" data-path="introducción-a-la-inferencia-estadística.html"><a href="introducción-a-la-inferencia-estadística.html#ap%C3%A9ndice-t%C3%A9cnico-opcional"><i class="fa fa-check"></i><b>6.11</b> Apéndice técnico (opcional)</a>
<ul>
<li class="chapter" data-level="6.11.1" data-path="introducción-a-la-inferencia-estadística.html"><a href="introducción-a-la-inferencia-estadística.html#funci%C3%B3n-generadora-de-momentos-de-la-media-muestral"><i class="fa fa-check"></i><b>6.11.1</b> Función generadora de momentos de la media muestral</a></li>
<li class="chapter" data-level="6.11.2" data-path="introducción-a-la-inferencia-estadística.html"><a href="introducción-a-la-inferencia-estadística.html#momentos-centrales-y-relaci%C3%B3n-con-la-varianza-muestral"><i class="fa fa-check"></i><b>6.11.2</b> Momentos centrales y relación con la varianza muestral</a></li>
<li class="chapter" data-level="6.11.3" data-path="introducción-a-la-inferencia-estadística.html"><a href="introducción-a-la-inferencia-estadística.html#propiedades-asint%C3%B3ticas-de-los-momentos-muestrales"><i class="fa fa-check"></i><b>6.11.3</b> Propiedades asintóticas de los momentos muestrales</a></li>
<li class="chapter" data-level="6.11.4" data-path="introducción-a-la-inferencia-estadística.html"><a href="introducción-a-la-inferencia-estadística.html#recordatorio-propiedades-%C3%BAtiles-de-la-distribuci%C3%B3n-gamma"><i class="fa fa-check"></i><b>6.11.4</b> Recordatorio: propiedades útiles de la distribución Gamma</a></li>
<li class="chapter" data-level="6.11.5" data-path="introducción-a-la-inferencia-estadística.html"><a href="introducción-a-la-inferencia-estadística.html#derivaci%C3%B3n-estructurada-de-chi2-t-y-f"><i class="fa fa-check"></i><b>6.11.5</b> Derivación estructurada de <span class="math inline">\(\chi^2\)</span>, <span class="math inline">\(t\)</span> y <span class="math inline">\(F\)</span></a></li>
<li class="chapter" data-level="6.11.6" data-path="introducción-a-la-inferencia-estadística.html"><a href="introducción-a-la-inferencia-estadística.html#asint%C3%B3tica-%C3%BAtil-para-inferencia"><i class="fa fa-check"></i><b>6.11.6</b> Asintótica útil para inferencia</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="estimación-puntual.html"><a href="estimación-puntual.html"><i class="fa fa-check"></i><b>7</b> Estimación puntual</a>
<ul>
<li class="chapter" data-level="7.1" data-path="estimación-puntual.html"><a href="estimación-puntual.html#el-problema-de-la-estimaci%C3%B3n-puntual"><i class="fa fa-check"></i><b>7.1</b> El problema de la estimación puntual</a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="estimación-puntual.html"><a href="estimación-puntual.html#criterios-de-optimalidad-de-estimadores.-el-riesgo"><i class="fa fa-check"></i><b>7.1.1</b> Criterios de optimalidad de estimadores. El Riesgo</a></li>
<li class="chapter" data-level="7.1.2" data-path="estimación-puntual.html"><a href="estimación-puntual.html#el-error-cuadr%C3%A1tico-medio"><i class="fa fa-check"></i><b>7.1.2</b> El error cuadrático medio</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="estimación-puntual.html"><a href="estimación-puntual.html#estudio-de-las-propiedades-deseables-de-los-estimadores"><i class="fa fa-check"></i><b>7.2</b> Estudio de las propiedades deseables de los estimadores</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="estimación-puntual.html"><a href="estimación-puntual.html#el-sesgo"><i class="fa fa-check"></i><b>7.2.1</b> El sesgo</a></li>
<li class="chapter" data-level="7.2.2" data-path="estimación-puntual.html"><a href="estimación-puntual.html#consistencia"><i class="fa fa-check"></i><b>7.2.2</b> Consistencia</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="estimación-puntual.html"><a href="estimación-puntual.html#propiedades-de-los-estimadores-consistentes"><i class="fa fa-check"></i><b>7.3</b> Propiedades de los estimadores consistentes</a>
<ul>
<li class="chapter" data-level="7.3.1" data-path="estimación-puntual.html"><a href="estimación-puntual.html#eficiencia"><i class="fa fa-check"></i><b>7.3.1</b> Eficiencia</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="estimación-puntual.html"><a href="estimación-puntual.html#informaci%C3%B3n-de-fisher-y-cota-de-cramerrao"><i class="fa fa-check"></i><b>7.4</b> Información de Fisher y cota de CramerRao</a></li>
<li class="chapter" data-level="7.5" data-path="estimación-puntual.html"><a href="estimación-puntual.html#informaci%C3%B3n-y-verosimilitud-de-un-modelo-estad%C3%ADstico"><i class="fa fa-check"></i><b>7.5</b> Información y verosimilitud de un modelo estadístico</a></li>
<li class="chapter" data-level="7.6" data-path="estimación-puntual.html"><a href="estimación-puntual.html#informaci%C3%B3n-de-fisher"><i class="fa fa-check"></i><b>7.6</b> Información de Fisher</a></li>
<li class="chapter" data-level="7.7" data-path="estimación-puntual.html"><a href="estimación-puntual.html#la-desigualdad-de-cramer-rao"><i class="fa fa-check"></i><b>7.7</b> La desigualdad de Cramer-Rao</a></li>
<li class="chapter" data-level="7.8" data-path="estimación-puntual.html"><a href="estimación-puntual.html#caracterizaci%C3%B3n-del-estimador-eficiente"><i class="fa fa-check"></i><b>7.8</b> Caracterización del estimador eficiente</a></li>
<li class="chapter" data-level="7.9" data-path="estimación-puntual.html"><a href="estimación-puntual.html#estad%C3%ADsticos-suficientes"><i class="fa fa-check"></i><b>7.9</b> Estadísticos suficientes</a>
<ul>
<li class="chapter" data-level="7.9.1" data-path="estimación-puntual.html"><a href="estimación-puntual.html#definici%C3%B3-de-estad%C3%ADsticop-suficiente"><i class="fa fa-check"></i><b>7.9.1</b> Definició de estadísticop suficiente</a></li>
<li class="chapter" data-level="7.9.2" data-path="estimación-puntual.html"><a href="estimación-puntual.html#teorema-de-factorizaci%C3%B3n"><i class="fa fa-check"></i><b>7.9.2</b> Teorema de factorización</a></li>
<li class="chapter" data-level="7.9.3" data-path="estimación-puntual.html"><a href="estimación-puntual.html#propiedades-de-los-estad%C3%ADsticos-suficientes"><i class="fa fa-check"></i><b>7.9.3</b> Propiedades de los estadísticos suficientes</a></li>
</ul></li>
<li class="chapter" data-level="7.10" data-path="estimación-puntual.html"><a href="estimación-puntual.html#obtenci%C3%B3n-de-estimadores"><i class="fa fa-check"></i><b>7.10</b> Obtención de estimadores</a></li>
<li class="chapter" data-level="7.11" data-path="estimación-puntual.html"><a href="estimación-puntual.html#el-m%C3%A9todo-de-los-momentos"><i class="fa fa-check"></i><b>7.11</b> El método de los momentos</a>
<ul>
<li class="chapter" data-level="7.11.1" data-path="estimación-puntual.html"><a href="estimación-puntual.html#observaciones-1"><i class="fa fa-check"></i><b>7.11.1</b> Observaciones</a></li>
</ul></li>
<li class="chapter" data-level="7.12" data-path="estimación-puntual.html"><a href="estimación-puntual.html#el-m%C3%A9todo-del-m%C3%A1ximo-de-verosimilitud"><i class="fa fa-check"></i><b>7.12</b> El método del máximo de verosimilitud</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html"><i class="fa fa-check"></i><b>8</b> Estimación por intérvalos</a>
<ul>
<li class="chapter" data-level="8.1" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#motivaci%C3%B3n-de-los-intervalos-de-confianza-la-estimaci%C3%B3n-puntual-casi-siempre-es-falsa"><i class="fa fa-check"></i><b>8.1</b> Motivación de los intervalos de confianza: la estimación puntual casi siempre es falsa</a></li>
<li class="chapter" data-level="8.2" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#definici%C3%B3n-formal-de-intervalo-de-confianza"><i class="fa fa-check"></i><b>8.2</b> Definición formal de intervalo de confianza</a></li>
<li class="chapter" data-level="8.3" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#un-ejemplo-de-construcci%C3%B3n-de-un-intervalo-de-confianza"><i class="fa fa-check"></i><b>8.3</b> Un ejemplo de construcción de un intervalo de confianza</a>
<ul>
<li class="chapter" data-level="8.3.1" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#planteamiento"><i class="fa fa-check"></i><b>8.3.1</b> Planteamiento</a></li>
<li class="chapter" data-level="8.3.2" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#desarrollo-de-la-construcci%C3%B3n"><i class="fa fa-check"></i><b>8.3.2</b> Desarrollo de la construcción</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#por-qu%C3%A9-hablamos-de-confianza-y-no-de-probabilidad"><i class="fa fa-check"></i><b>8.4</b> ¿Por qué hablamos de confianza y no de probabilidad?</a></li>
<li class="chapter" data-level="8.5" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#elementos-de-un-intervalo-de-confianza"><i class="fa fa-check"></i><b>8.5</b> Elementos de un intervalo de confianza</a></li>
<li class="chapter" data-level="8.6" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#m%C3%A9todo-del-pivote"><i class="fa fa-check"></i><b>8.6</b> Método del pivote</a></li>
<li class="chapter" data-level="8.7" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#algunos-estad%C3%ADsticos-pivote"><i class="fa fa-check"></i><b>8.7</b> Algunos estadísticos pivote</a></li>
<li class="chapter" data-level="8.8" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#intervalo-de-confianza-para-la-media-de-una-distribuci%C3%B3n-normal"><i class="fa fa-check"></i><b>8.8</b> Intervalo de confianza para la media de una distribución Normal</a>
<ul>
<li class="chapter" data-level="8.8.1" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#caso-de-varianza-conocida"><i class="fa fa-check"></i><b>8.8.1</b> Caso de varianza conocida</a></li>
<li class="chapter" data-level="8.8.2" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#caso-de-varianza-desconocida"><i class="fa fa-check"></i><b>8.8.2</b> Caso de varianza desconocida</a></li>
<li class="chapter" data-level="8.8.3" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#calculo-con-r"><i class="fa fa-check"></i><b>8.8.3</b> Calculo con R</a></li>
<li class="chapter" data-level="8.8.4" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#tama%C3%B1o-de-muestra-para-la-media-de-una-distribuci%C3%B3n-normal"><i class="fa fa-check"></i><b>8.8.4</b> Tamaño de muestra para la media de una distribución Normal</a></li>
</ul></li>
<li class="chapter" data-level="8.9" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#intervalo-de-confianza-para-la-varianza-de-una-distribuci%C3%B3n-normal"><i class="fa fa-check"></i><b>8.9</b> Intervalo de confianza para la varianza de una distribución Normal</a></li>
<li class="chapter" data-level="8.10" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#intervalo-de-confianza-para-una-proporci%C3%B3n"><i class="fa fa-check"></i><b>8.10</b> Intervalo de confianza para una proporción</a>
<ul>
<li class="chapter" data-level="8.10.1" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#aproximaci%C3%B3n-asint%C3%B3tica"><i class="fa fa-check"></i><b>8.10.1</b> Aproximación asintótica</a></li>
<li class="chapter" data-level="8.10.2" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#intervalo-exacto"><i class="fa fa-check"></i><b>8.10.2</b> Intervalo exacto</a></li>
<li class="chapter" data-level="8.10.3" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#tama%C3%B1o-muestral-para-una-proporci%C3%B3n"><i class="fa fa-check"></i><b>8.10.3</b> Tamaño muestral para una proporción</a></li>
</ul></li>
<li class="chapter" data-level="8.11" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#intervalo-de-confianza-para-el-par%C3%A1metro-de-una-distribuci%C3%B3n-de-poisson"><i class="fa fa-check"></i><b>8.11</b> Intervalo de confianza para el parámetro de una distribución de Poisson</a>
<ul>
<li class="chapter" data-level="8.11.1" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#aproximaci%C3%B3n-asint%C3%B3tica-1"><i class="fa fa-check"></i><b>8.11.1</b> Aproximación asintótica</a></li>
<li class="chapter" data-level="8.11.2" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#intervalo-exacto-1"><i class="fa fa-check"></i><b>8.11.2</b> Intervalo exacto</a></li>
<li class="chapter" data-level="8.11.3" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#tama%C3%B1o-de-muestra-para-el-par%C3%A1metro-de-una-distribuci%C3%B3n-de-poisson"><i class="fa fa-check"></i><b>8.11.3</b> Tamaño de muestra para el parámetro de una distribución de Poisson</a></li>
</ul></li>
<li class="chapter" data-level="8.12" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#intervalo-de-confianza-para-la-diferencia-de-medias-de-distribuciones-normales-independientes."><i class="fa fa-check"></i><b>8.12</b> Intervalo de confianza para la diferencia de medias de distribuciones normales independientes.</a>
<ul>
<li class="chapter" data-level="8.12.1" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#varianza-com%C3%BAn"><i class="fa fa-check"></i><b>8.12.1</b> Varianza común</a></li>
</ul></li>
<li class="chapter" data-level="8.13" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#intervalo-de-confianza-para-la-diferencia-de-medias-de-distribuciones-normales-independientes.-1"><i class="fa fa-check"></i><b>8.13</b> Intervalo de confianza para la diferencia de medias de distribuciones normales independientes.</a>
<ul>
<li class="chapter" data-level="8.13.1" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#varianza-diferente"><i class="fa fa-check"></i><b>8.13.1</b> Varianza diferente</a></li>
<li class="chapter" data-level="8.13.2" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#caso-de-varianzas-desconocidas-y-diferentes"><i class="fa fa-check"></i><b>8.13.2</b> Caso de varianzas desconocidas y diferentes</a></li>
<li class="chapter" data-level="8.13.3" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#int%C3%A9rvalos-de-confianza-y-decisiones-estad%C3%ADsticas"><i class="fa fa-check"></i><b>8.13.3</b> Intérvalos de confianza y decisiones estadísticas</a></li>
</ul></li>
<li class="chapter" data-level="8.14" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#intervalo-de-confianza-para-el-cociente-de-varianzas-de-distribuciones-normales-independientes"><i class="fa fa-check"></i><b>8.14</b> Intervalo de confianza para el cociente de varianzas de distribuciones normales independientes</a></li>
<li class="chapter" data-level="8.15" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#complementos"><i class="fa fa-check"></i><b>8.15</b> Complementos</a>
<ul>
<li class="chapter" data-level="8.15.1" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#interpretaci%C3%B3n-geom%C3%A9trica-de-los-intervalos-de-confianza"><i class="fa fa-check"></i><b>8.15.1</b> Interpretación geométrica de los intervalos de confianza</a></li>
<li class="chapter" data-level="8.15.2" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#intervalos-para-muestras-grandes"><i class="fa fa-check"></i><b>8.15.2</b> Intervalos para muestras grandes</a></li>
<li class="chapter" data-level="8.15.3" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#intervalos-exactos-para-distribuciones-discretas"><i class="fa fa-check"></i><b>8.15.3</b> Intervalos exactos para distribuciones discretas</a></li>
<li class="chapter" data-level="8.15.4" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#una-aproximaci%C3%B3n-diferente-para-la-distribuci%C3%B3n-de-poisson"><i class="fa fa-check"></i><b>8.15.4</b> Una aproximación diferente para la distribución de Poisson</a></li>
<li class="chapter" data-level="8.15.5" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#aproximaci%C3%B3n-mediante-ch%C3%A9bishev"><i class="fa fa-check"></i><b>8.15.5</b> Aproximación mediante Chébishev</a></li>
</ul></li>
<li class="chapter" data-level="8.16" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#introducci%C3%B3n-2"><i class="fa fa-check"></i><b>8.16</b> Introducción</a>
<ul>
<li class="chapter" data-level="8.16.1" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#de-las-hip%C3%B3tesis-cient%C3%ADficas-a-las-hip%C3%B3tesis-estad%C3%ADsticas"><i class="fa fa-check"></i><b>8.16.1</b> De las hipótesis científicas a las hipótesis estadísticas</a></li>
<li class="chapter" data-level="8.16.2" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#del-lenguaje-natural-a-la-hip%C3%B3tesis-estad%C3%ADstica"><i class="fa fa-check"></i><b>8.16.2</b> Del lenguaje natural a la hipótesis estadística</a></li>
<li class="chapter" data-level="8.16.3" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#caso-1-presentaci%C3%B3n"><i class="fa fa-check"></i><b>8.16.3</b> Caso 1: Presentación</a></li>
<li class="chapter" data-level="8.16.4" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#caso-1-modelo-de-probabilidad"><i class="fa fa-check"></i><b>8.16.4</b> Caso 1: Modelo de probabilidad</a></li>
<li class="chapter" data-level="8.16.5" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#caso-2-presentaci%C3%B3n"><i class="fa fa-check"></i><b>8.16.5</b> Caso 2: Presentación</a></li>
<li class="chapter" data-level="8.16.6" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#caso-2-modelo-de-probabilidad"><i class="fa fa-check"></i><b>8.16.6</b> Caso 2: Modelo de probabilidad</a></li>
</ul></li>
<li class="chapter" data-level="8.17" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#las-hip%C3%B3tesis-del-contraste-de-hip%C3%B3tesis"><i class="fa fa-check"></i><b>8.17</b> Las hipótesis del contraste de hipótesis</a>
<ul>
<li class="chapter" data-level="8.17.1" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#caso-1-hip%C3%B3tesis-para-dirimir-la-controversia-sobre-el-n%C3%BAmero-de-hembras"><i class="fa fa-check"></i><b>8.17.1</b> Caso 1: Hipótesis para dirimir la controversia sobre el número de hembras</a></li>
<li class="chapter" data-level="8.17.2" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#caso-2-hip%C3%B3tesis-a-contrastar-en-el-problema-de-la-tasa-de-statdrolona"><i class="fa fa-check"></i><b>8.17.2</b> Caso 2: Hipótesis a contrastar en el problema de la tasa de statdrolona</a></li>
</ul></li>
<li class="chapter" data-level="8.18" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#compatibilidad-de-resultados-e-hip%C3%B3tesis"><i class="fa fa-check"></i><b>8.18</b> Compatibilidad de resultados e hipótesis</a>
<ul>
<li class="chapter" data-level="8.18.1" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#caso-1-compatibilidad-de-resultados-e-hip%C3%B3tesis"><i class="fa fa-check"></i><b>8.18.1</b> Caso 1: Compatibilidad de resultados e hipótesis</a></li>
<li class="chapter" data-level="8.18.2" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#caso-2-compatibilidad-de-resultados-e-hip%C3%B3tesis"><i class="fa fa-check"></i><b>8.18.2</b> Caso 2: Compatibilidad de resultados e hipótesis</a></li>
</ul></li>
<li class="chapter" data-level="8.19" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#no-todo-es-igualmente-probable"><i class="fa fa-check"></i><b>8.19</b> No todo es igualmente probable…</a>
<ul>
<li class="chapter" data-level="8.19.1" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#caso-1-una-regi%C3%B3n-con-n%C3%BAmero-de-hembras-con-baja-probabilidad-bajo-mathrmh_0"><i class="fa fa-check"></i><b>8.19.1</b> Caso 1: Una región con número de hembras con baja probabilidad bajo <span class="math inline">\(\mathrm{H}_{0}\)</span></a></li>
<li class="chapter" data-level="8.19.2" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#caso-2-medias-de-las-tasas-de-statdrolona-improbables-si-se-cumple-mathrmh_0"><i class="fa fa-check"></i><b>8.19.2</b> Caso 2: Medias de las tasas de statdrolona improbables si se cumple <span class="math inline">\(\mathrm{H}_{0}\)</span></a></li>
</ul></li>
<li class="chapter" data-level="8.20" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#el-papel-privilegiado-de-la-hip%C3%B3tesis-nula-criterio-de-decisi%C3%B3n"><i class="fa fa-check"></i><b>8.20</b> El papel privilegiado de la hipótesis nula: criterio de decisión</a>
<ul>
<li class="chapter" data-level="8.20.1" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#caso-1-n.%C2%BA-de-nidos-propuestos-ad-hoc-como-inicio-de-regi%C3%B3n-cr%C3%ADtica.-regla-de-decisi%C3%B3n-resultante"><i class="fa fa-check"></i><b>8.20.1</b> Caso 1: N.º de nidos propuestos ad hoc como inicio de región crítica. Regla de decisión resultante</a></li>
</ul></li>
<li class="chapter" data-level="8.21" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#hip%C3%B3tesis-nula-y-nivel-de-significaci%C3%B3n"><i class="fa fa-check"></i><b>8.21</b> Hipótesis nula y nivel de significación</a>
<ul>
<li class="chapter" data-level="8.21.1" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#caso-1-nivel-de-significaci%C3%B3n"><i class="fa fa-check"></i><b>8.21.1</b> Caso 1: Nivel de significación</a></li>
<li class="chapter" data-level="8.21.2" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#caso-1-elecci%C3%B3n-de-la-regi%C3%B3n-cr%C3%ADtica"><i class="fa fa-check"></i><b>8.21.2</b> Caso 1: Elección de la región crítica</a></li>
<li class="chapter" data-level="8.21.3" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#caso-2-elecci%C3%B3n-de-la-regi%C3%B3n-cr%C3%ADtica"><i class="fa fa-check"></i><b>8.21.3</b> Caso 2: Elección de la región crítica</a></li>
</ul></li>
<li class="chapter" data-level="8.22" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#regi%C3%B3n-cr%C3%ADtica-y-formalizaci%C3%B3n-del-contraste"><i class="fa fa-check"></i><b>8.22</b> Región crítica y formalización del contraste</a>
<ul>
<li class="chapter" data-level="8.22.1" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#caso-1-resumen-de-conceptos-asociados-al-contraste.-regi%C3%B3n-cr%C3%ADtica"><i class="fa fa-check"></i><b>8.22.1</b> Caso 1: Resumen de conceptos asociados al contraste. Región crítica</a></li>
<li class="chapter" data-level="8.22.2" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#caso-2-tabla-resumen-de-la-regi%C3%B3n-cr%C3%ADtica-el-estad%C3%ADstico-de-test-y-del-criterio-de-decisi%C3%B3n"><i class="fa fa-check"></i><b>8.22.2</b> Caso 2: Tabla resumen de la región crítica, el estadístico de test y del criterio de decisión</a></li>
<li class="chapter" data-level="8.22.3" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#regi%C3%B3n-cr%C3%ADtica-frente-a-estad%C3%ADstico-de-test"><i class="fa fa-check"></i><b>8.22.3</b> Región crítica frente a Estadístico de Test</a></li>
</ul></li>
<li class="chapter" data-level="8.23" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#tabla-de-decisi%C3%B3n-del-contraste"><i class="fa fa-check"></i><b>8.23</b> Tabla de decisión del contraste</a>
<ul>
<li class="chapter" data-level="8.23.1" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#caso-1-evaluaci%C3%B3n-de-los-dos-errores-asociados-al-contraste"><i class="fa fa-check"></i><b>8.23.1</b> Caso 1: Evaluación de los dos errores asociados al contraste</a></li>
<li class="chapter" data-level="8.23.2" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#caso-2-c%C3%A1lculo-expl%C3%ADcito-de-los-errores-de-primera-alpha-y-segunda-especie-1--beta"><i class="fa fa-check"></i><b>8.23.2</b> Caso 2: Cálculo explícito de los errores de primera ( <span class="math inline">\(\alpha\)</span> ) y segunda especie (1- <span class="math inline">\(\beta\)</span> )</a></li>
</ul></li>
<li class="chapter" data-level="8.24" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#relaci%C3%B3n-entre-el-error-de-tipo-i-y-el-de-tipo-ii"><i class="fa fa-check"></i><b>8.24</b> Relación entre el error de tipo I y el de tipo II</a>
<ul>
<li class="chapter" data-level="8.24.1" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#caso-1-evaluaci%C3%B3n-de-alpha-y-1--beta-para-diferentes-regiones-cr%C3%ADticas"><i class="fa fa-check"></i><b>8.24.1</b> Caso 1: Evaluación de <span class="math inline">\(\alpha\)</span> y 1- <span class="math inline">\(\beta\)</span> para diferentes regiones críticas</a></li>
<li class="chapter" data-level="8.24.2" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#caso-2-relaci%C3%B3n-entre-los-errores-de-primera-alpha-y-segunda-especie-1--beta"><i class="fa fa-check"></i><b>8.24.2</b> Caso 2: Relación entre los errores de primera ( <span class="math inline">\(\alpha\)</span> ) y segunda especie (1- <span class="math inline">\(\beta\)</span> )</a></li>
</ul></li>
<li class="chapter" data-level="8.25" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#potencia-y-test-m%C3%A1s-potente"><i class="fa fa-check"></i><b>8.25</b> Potencia y test más potente</a>
<ul>
<li class="chapter" data-level="8.25.1" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#caso-1-potencia-en-hip%C3%B3tesis-simple-vs-simple"><i class="fa fa-check"></i><b>8.25.1</b> Caso 1: Potencia en hipótesis simple vs simple</a></li>
<li class="chapter" data-level="8.25.2" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#caso-2-potencia-en-hip%C3%B3tesis-simple-vs-simple"><i class="fa fa-check"></i><b>8.25.2</b> Caso 2: Potencia en hipótesis simple vs simple</a></li>
</ul></li>
<li class="chapter" data-level="8.26" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#efecto-del-tama%C3%B1o-muestral"><i class="fa fa-check"></i><b>8.26</b> Efecto del tamaño muestral</a>
<ul>
<li class="chapter" data-level="8.26.1" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#caso-1"><i class="fa fa-check"></i><b>8.26.1</b> Caso 1</a></li>
<li class="chapter" data-level="8.26.2" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#caso-2"><i class="fa fa-check"></i><b>8.26.2</b> Caso 2</a></li>
</ul></li>
<li class="chapter" data-level="8.27" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#hip%C3%B3tesis-simples-vs.-hip%C3%B3tesis-compuestas"><i class="fa fa-check"></i><b>8.27</b> Hipótesis simples vs. hipótesis compuestas</a>
<ul>
<li class="chapter" data-level="8.27.1" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#caso-1-hip%C3%B3tesis-compuestas"><i class="fa fa-check"></i><b>8.27.1</b> Caso 1: Hipótesis compuestas</a></li>
<li class="chapter" data-level="8.27.2" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#caso-2-hip%C3%B3tesis-compuestas"><i class="fa fa-check"></i><b>8.27.2</b> Caso 2: Hipótesis compuestas</a></li>
</ul></li>
<li class="chapter" data-level="8.28" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#funci%C3%B3n-de-potencia"><i class="fa fa-check"></i><b>8.28</b> Función de potencia</a>
<ul>
<li class="chapter" data-level="8.28.1" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#caso-1-funci%C3%B3n-de-potencia"><i class="fa fa-check"></i><b>8.28.1</b> Caso 1: Función de potencia</a></li>
<li class="chapter" data-level="8.28.2" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#caso-2-funci%C3%B3n-de-potencia"><i class="fa fa-check"></i><b>8.28.2</b> Caso 2: Función de potencia</a></li>
</ul></li>
<li class="chapter" data-level="8.29" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#tests-%C3%B3ptimos"><i class="fa fa-check"></i><b>8.29</b> Tests óptimos</a></li>
<li class="chapter" data-level="8.30" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#pruebas-bilaterales-y-pruebas-unilaterales"><i class="fa fa-check"></i><b>8.30</b> Pruebas bilaterales y pruebas unilaterales</a>
<ul>
<li class="chapter" data-level="8.30.1" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#caso-1-prueba-unilateral"><i class="fa fa-check"></i><b>8.30.1</b> Caso 1: Prueba unilateral</a></li>
<li class="chapter" data-level="8.30.2" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#caso-2-prueba-unilateral"><i class="fa fa-check"></i><b>8.30.2</b> Caso 2: Prueba unilateral</a></li>
</ul></li>
<li class="chapter" data-level="8.31" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#elecci%C3%B3n-del-nivel-de-significaci%C3%B3n"><i class="fa fa-check"></i><b>8.31</b> Elección del nivel de significación</a></li>
<li class="chapter" data-level="8.32" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#el-p-valor"><i class="fa fa-check"></i><b>8.32</b> El p-valor</a>
<ul>
<li class="chapter" data-level="8.32.1" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#caso-1-c%C3%A1lculo-del-p-valor-prueba-unilateral"><i class="fa fa-check"></i><b>8.32.1</b> Caso 1: Cálculo del p-valor (prueba unilateral)</a></li>
<li class="chapter" data-level="8.32.2" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#caso-2-c%C3%A1lculo-del-p-valor-prueba-unilateral"><i class="fa fa-check"></i><b>8.32.2</b> Caso 2: Cálculo del p-valor (prueba unilateral)</a></li>
<li class="chapter" data-level="8.32.3" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#caso-2-c%C3%A1lculo-del-p-valor-prueba-bilateral"><i class="fa fa-check"></i><b>8.32.3</b> Caso 2: Cálculo del p-valor (prueba bilateral)</a></li>
</ul></li>
<li class="chapter" data-level="8.33" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#pruebas-exactas-y-pruebas-asint%C3%B3ticas"><i class="fa fa-check"></i><b>8.33</b> Pruebas exactas y pruebas asintóticas</a>
<ul>
<li class="chapter" data-level="8.33.1" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#caso-1-test-asint%C3%B3tico"><i class="fa fa-check"></i><b>8.33.1</b> Caso 1: Test asintótico</a></li>
<li class="chapter" data-level="8.33.2" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#caso-2-test-exacto"><i class="fa fa-check"></i><b>8.33.2</b> Caso 2: Test exacto</a></li>
</ul></li>
<li class="chapter" data-level="8.34" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#relaci%C3%B3n-con-los-intervalos-de-confianza"><i class="fa fa-check"></i><b>8.34</b> Relación con los intervalos de confianza</a>
<ul>
<li class="chapter" data-level="8.34.1" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#caso-2-relaci%C3%B3n-con-los-intervalos-de-confianza"><i class="fa fa-check"></i><b>8.34.1</b> Caso 2: Relación con los intervalos de confianza</a></li>
</ul></li>
<li class="chapter" data-level="8.35" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#tama%C3%B1os-de-muestra.-diferencia-m%C3%ADnima-significativa"><i class="fa fa-check"></i><b>8.35</b> Tamaños de muestra. Diferencia mínima significativa</a>
<ul>
<li class="chapter" data-level="8.35.1" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#caso-2-c%C3%A1lculo-del-tama%C3%B1o-de-la-muestra"><i class="fa fa-check"></i><b>8.35.1</b> Caso 2: Cálculo del tamaño de la muestra</a></li>
</ul></li>
<li class="chapter" data-level="8.36" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#esquema-de-un-contraste-correctamente-planteado"><i class="fa fa-check"></i><b>8.36</b> Esquema de un contraste correctamente planteado</a></li>
<li class="chapter" data-level="8.37" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#significaci%C3%B3n-estad%C3%ADstica-y-significaci%C3%B3n-aplicada"><i class="fa fa-check"></i><b>8.37</b> Significación estadística y significación aplicada</a>
<ul>
<li class="chapter" data-level="8.37.1" data-path="estimación-por-intérvalos.html"><a href="estimación-por-intérvalos.html#caso-2-significaci%C3%B3n-estad%C3%ADstica-y-aplicada"><i class="fa fa-check"></i><b>8.37.1</b> Caso 2: Significación estadística y aplicada</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="construcción-de-contrastes-de-hipótesis.html"><a href="construcción-de-contrastes-de-hipótesis.html"><i class="fa fa-check"></i><b>9</b> Construcción de contrastes de hipótesis</a>
<ul>
<li class="chapter" data-level="9.1" data-path="construcción-de-contrastes-de-hipótesis.html"><a href="construcción-de-contrastes-de-hipótesis.html#qu%C3%A9-significa-construir-un-contraste-de-hip%C3%B3tesis"><i class="fa fa-check"></i><b>9.1</b> ¿Qué significa “construir” un contraste de hipótesis?</a>
<ul>
<li class="chapter" data-level="9.1.1" data-path="construcción-de-contrastes-de-hipótesis.html"><a href="construcción-de-contrastes-de-hipótesis.html#el-contraste-como-regla-de-decisi%C3%B3n"><i class="fa fa-check"></i><b>9.1.1</b> El contraste como regla de decisión</a></li>
<li class="chapter" data-level="9.1.2" data-path="construcción-de-contrastes-de-hipótesis.html"><a href="construcción-de-contrastes-de-hipótesis.html#nivel-de-significaci%C3%B3n-como-restricci%C3%B3n-b%C3%A1sica"><i class="fa fa-check"></i><b>9.1.2</b> Nivel de significación como restricción básica</a></li>
<li class="chapter" data-level="9.1.3" data-path="construcción-de-contrastes-de-hipótesis.html"><a href="construcción-de-contrastes-de-hipótesis.html#primer-ejemplo-distintos-contrastes-con-el-mismo-nivel"><i class="fa fa-check"></i><b>9.1.3</b> Primer ejemplo: distintos contrastes con el mismo nivel</a></li>
<li class="chapter" data-level="9.1.4" data-path="construcción-de-contrastes-de-hipótesis.html"><a href="construcción-de-contrastes-de-hipótesis.html#potencia-como-criterio-de-comparaci%C3%B3n"><i class="fa fa-check"></i><b>9.1.4</b> Potencia como criterio de comparación</a></li>
<li class="chapter" data-level="9.1.5" data-path="construcción-de-contrastes-de-hipótesis.html"><a href="construcción-de-contrastes-de-hipótesis.html#segundo-ejemplo-misma-alpha-distinta-potencia"><i class="fa fa-check"></i><b>9.1.5</b> Segundo ejemplo: misma alpha, distinta potencia</a></li>
<li class="chapter" data-level="9.1.6" data-path="construcción-de-contrastes-de-hipótesis.html"><a href="construcción-de-contrastes-de-hipótesis.html#de-contrastes-razonables-a-contrastes-%C3%B3ptimos"><i class="fa fa-check"></i><b>9.1.6</b> De contrastes razonables a contrastes óptimos</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="construcción-de-contrastes-de-hipótesis.html"><a href="construcción-de-contrastes-de-hipótesis.html#evidencia-y-decisi%C3%B3n-dos-enfoques-cl%C3%A1sicos"><i class="fa fa-check"></i><b>9.2</b> Evidencia y decisión: dos enfoques clásicos</a>
<ul>
<li class="chapter" data-level="9.2.1" data-path="construcción-de-contrastes-de-hipótesis.html"><a href="construcción-de-contrastes-de-hipótesis.html#el-enfoque-de-fisher-tests-de-significaci%C3%B3n"><i class="fa fa-check"></i><b>9.2.1</b> El enfoque de Fisher: tests de significación</a></li>
<li class="chapter" data-level="9.2.2" data-path="construcción-de-contrastes-de-hipótesis.html"><a href="construcción-de-contrastes-de-hipótesis.html#el-enfoque-de-neymanpearson-contraste-como-regla-de-decisi%C3%B3n"><i class="fa fa-check"></i><b>9.2.2</b> El enfoque de Neyman–Pearson: contraste como regla de decisión</a></li>
<li class="chapter" data-level="9.2.3" data-path="construcción-de-contrastes-de-hipótesis.html"><a href="construcción-de-contrastes-de-hipótesis.html#diferencias-conceptuales-clave"><i class="fa fa-check"></i><b>9.2.3</b> Diferencias conceptuales clave</a></li>
<li class="chapter" data-level="9.2.4" data-path="construcción-de-contrastes-de-hipótesis.html"><a href="construcción-de-contrastes-de-hipótesis.html#convivencia-de-ambos-enfoques-en-la-pr%C3%A1ctica"><i class="fa fa-check"></i><b>9.2.4</b> Convivencia de ambos enfoques en la práctica</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="construcción-de-contrastes-de-hipótesis.html"><a href="construcción-de-contrastes-de-hipótesis.html#tests-%C3%B3ptimos-el-lema-de-neymanpearson"><i class="fa fa-check"></i><b>9.3</b> Tests óptimos: el lema de Neyman–Pearson</a>
<ul>
<li class="chapter" data-level="9.3.1" data-path="construcción-de-contrastes-de-hipótesis.html"><a href="construcción-de-contrastes-de-hipótesis.html#hip%C3%B3tesis-simples-y-raz%C3%B3n-de-verosimilitudes"><i class="fa fa-check"></i><b>9.3.1</b> Hipótesis simples y razón de verosimilitudes</a></li>
<li class="chapter" data-level="9.3.2" data-path="construcción-de-contrastes-de-hipótesis.html"><a href="construcción-de-contrastes-de-hipótesis.html#enunciado-del-lema-de-neymanpearson"><i class="fa fa-check"></i><b>9.3.2</b> Enunciado del lema de Neyman–Pearson</a></li>
<li class="chapter" data-level="9.3.3" data-path="construcción-de-contrastes-de-hipótesis.html"><a href="construcción-de-contrastes-de-hipótesis.html#ejemplo-modelo-normal-con-varianza-conocida"><i class="fa fa-check"></i><b>9.3.3</b> Ejemplo: modelo normal con varianza conocida</a></li>
<li class="chapter" data-level="9.3.4" data-path="construcción-de-contrastes-de-hipótesis.html"><a href="construcción-de-contrastes-de-hipótesis.html#ejemplo-modelo-de-poisson"><i class="fa fa-check"></i><b>9.3.4</b> Ejemplo: modelo de Poisson</a></li>
<li class="chapter" data-level="9.3.5" data-path="construcción-de-contrastes-de-hipótesis.html"><a href="construcción-de-contrastes-de-hipótesis.html#extensiones-del-lema-de-neymanpearson"><i class="fa fa-check"></i><b>9.3.5</b> Extensiones del lema de Neyman–Pearson</a></li>
<li class="chapter" data-level="9.3.6" data-path="construcción-de-contrastes-de-hipótesis.html"><a href="construcción-de-contrastes-de-hipótesis.html#l%C3%ADmites-del-enfoque"><i class="fa fa-check"></i><b>9.3.6</b> Límites del enfoque</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="construcción-de-contrastes-de-hipótesis.html"><a href="construcción-de-contrastes-de-hipótesis.html#contrastes-de-raz%C3%B3n-de-verosimilitudes-generalizados"><i class="fa fa-check"></i><b>9.4</b> Contrastes de razón de verosimilitudes generalizados</a>
<ul>
<li class="chapter" data-level="9.4.1" data-path="construcción-de-contrastes-de-hipótesis.html"><a href="construcción-de-contrastes-de-hipótesis.html#definici%C3%B3n-del-estad%C3%ADstico-de-raz%C3%B3n-de-verosimilitudes"><i class="fa fa-check"></i><b>9.4.1</b> Definición del estadístico de razón de verosimilitudes</a></li>
<li class="chapter" data-level="9.4.2" data-path="construcción-de-contrastes-de-hipótesis.html"><a href="construcción-de-contrastes-de-hipótesis.html#regla-de-decisi%C3%B3n-y-aproximaci%C3%B3n-asint%C3%B3tica"><i class="fa fa-check"></i><b>9.4.2</b> Regla de decisión y aproximación asintótica</a></li>
<li class="chapter" data-level="9.4.3" data-path="construcción-de-contrastes-de-hipótesis.html"><a href="construcción-de-contrastes-de-hipótesis.html#ejemplo-comparaci%C3%B3n-de-par%C3%A1metros-en-un-modelo-de-poisson"><i class="fa fa-check"></i><b>9.4.3</b> Ejemplo: comparación de parámetros en un modelo de Poisson</a></li>
<li class="chapter" data-level="9.4.4" data-path="construcción-de-contrastes-de-hipótesis.html"><a href="construcción-de-contrastes-de-hipótesis.html#ejemplo-contraste-en-un-modelo-exponencial"><i class="fa fa-check"></i><b>9.4.4</b> Ejemplo: contraste en un modelo exponencial</a></li>
<li class="chapter" data-level="9.4.5" data-path="construcción-de-contrastes-de-hipótesis.html"><a href="construcción-de-contrastes-de-hipótesis.html#ejemplo-contraste-en-un-modelo-trinomial"><i class="fa fa-check"></i><b>9.4.5</b> Ejemplo: contraste en un modelo trinomial</a></li>
<li class="chapter" data-level="9.4.6" data-path="construcción-de-contrastes-de-hipótesis.html"><a href="construcción-de-contrastes-de-hipótesis.html#del-contraste-de-raz%C3%B3n-de-verosimilitudes-al-test-ji-cuadrado"><i class="fa fa-check"></i><b>9.4.6</b> Del contraste de razón de verosimilitudes al test ji-cuadrado</a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="construcción-de-contrastes-de-hipótesis.html"><a href="construcción-de-contrastes-de-hipótesis.html#tests-de-permutaciones"><i class="fa fa-check"></i><b>9.5</b> Tests de permutaciones</a>
<ul>
<li class="chapter" data-level="9.5.1" data-path="construcción-de-contrastes-de-hipótesis.html"><a href="construcción-de-contrastes-de-hipótesis.html#idea-b%C3%A1sica"><i class="fa fa-check"></i><b>9.5.1</b> Idea básica</a></li>
<li class="chapter" data-level="9.5.2" data-path="construcción-de-contrastes-de-hipótesis.html"><a href="construcción-de-contrastes-de-hipótesis.html#ejemplo-1-test-de-permutaciones-con-enumeraci%C3%B3n-completa"><i class="fa fa-check"></i><b>9.5.2</b> Ejemplo 1: test de permutaciones con enumeración completa</a></li>
<li class="chapter" data-level="9.5.3" data-path="construcción-de-contrastes-de-hipótesis.html"><a href="construcción-de-contrastes-de-hipótesis.html#valor-observado-del-estad%C3%ADstico"><i class="fa fa-check"></i><b>9.5.3</b> Valor observado del estadístico</a></li>
<li class="chapter" data-level="9.5.4" data-path="construcción-de-contrastes-de-hipótesis.html"><a href="construcción-de-contrastes-de-hipótesis.html#distribuci%C3%B3n-exacta-por-permutaciones"><i class="fa fa-check"></i><b>9.5.4</b> Distribución exacta por permutaciones</a></li>
<li class="chapter" data-level="9.5.5" data-path="construcción-de-contrastes-de-hipótesis.html"><a href="construcción-de-contrastes-de-hipótesis.html#ejemplo-2-test-de-permutaciones-mediante-simulaci%C3%B3n"><i class="fa fa-check"></i><b>9.5.5</b> Ejemplo 2: test de permutaciones mediante simulación</a></li>
<li class="chapter" data-level="9.5.6" data-path="construcción-de-contrastes-de-hipótesis.html"><a href="construcción-de-contrastes-de-hipótesis.html#comentario-final"><i class="fa fa-check"></i><b>9.5.6</b> Comentario final</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="pruebas-de-una-muestra.html"><a href="pruebas-de-una-muestra.html"><i class="fa fa-check"></i><b>10</b> Pruebas de una muestra</a>
<ul>
<li class="chapter" data-level="10.1" data-path="pruebas-de-una-muestra.html"><a href="pruebas-de-una-muestra.html#introducci%C3%B3n-a-los-contrastes-de-una-muestra."><i class="fa fa-check"></i><b>10.1</b> Introducción a los contrastes de una muestra.</a>
<ul>
<li class="chapter" data-level="10.1.1" data-path="pruebas-de-una-muestra.html"><a href="pruebas-de-una-muestra.html#esquema-de-los-contrastes-presentados"><i class="fa fa-check"></i><b>10.1.1</b> Esquema de los contrastes presentados</a></li>
<li class="chapter" data-level="10.1.2" data-path="pruebas-de-una-muestra.html"><a href="pruebas-de-una-muestra.html#contrastes-sobre-los-par%C3%A1metros-de-una-distribuci%C3%B3n-normal"><i class="fa fa-check"></i><b>10.1.2</b> Contrastes sobre los parámetros de una distribución Normal</a></li>
<li class="chapter" data-level="10.1.3" data-path="pruebas-de-una-muestra.html"><a href="pruebas-de-una-muestra.html#contrastes-sobre-una-proporci%C3%B3n"><i class="fa fa-check"></i><b>10.1.3</b> Contrastes sobre una proporción</a></li>
<li class="chapter" data-level="10.1.4" data-path="pruebas-de-una-muestra.html"><a href="pruebas-de-una-muestra.html#esquema-general-de-los-contrastes-presentados"><i class="fa fa-check"></i><b>10.1.4</b> Esquema general de los contrastes presentados</a></li>
<li class="chapter" data-level="10.1.5" data-path="pruebas-de-una-muestra.html"><a href="pruebas-de-una-muestra.html#contrastes-param%C3%A9tricos-frente-a-no-param%C3%A9tricos"><i class="fa fa-check"></i><b>10.1.5</b> Contrastes paramétricos frente a no paramétricos</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="pruebas-de-una-muestra.html"><a href="pruebas-de-una-muestra.html#contraste-de-hip%C3%B3tesis-para-la-media-de-una-distribuci%C3%B3n-normal-con-varianza-conocida-z-test."><i class="fa fa-check"></i><b>10.2</b> Contraste de hipótesis para la media de una distribución Normal con varianza conocida: <span class="math inline">\(Z\)</span>-test.</a>
<ul>
<li class="chapter" data-level="10.2.1" data-path="pruebas-de-una-muestra.html"><a href="pruebas-de-una-muestra.html#introducci%C3%B3n-3"><i class="fa fa-check"></i><b>10.2.1</b> Introducción</a></li>
<li class="chapter" data-level="10.2.2" data-path="pruebas-de-una-muestra.html"><a href="pruebas-de-una-muestra.html#resoluci%C3%B3n-del-contraste-para-la-media-de-una-distribuci%C3%B3n-normal-con-varianza-conocida."><i class="fa fa-check"></i><b>10.2.2</b> Resolución del contraste para la media de una distribución Normal con varianza conocida.</a></li>
<li class="chapter" data-level="10.2.3" data-path="pruebas-de-una-muestra.html"><a href="pruebas-de-una-muestra.html#intervalo-de-confianza-para-la-media-de-una-distribuci%C3%B3n-normal-con-varianza-conocida."><i class="fa fa-check"></i><b>10.2.3</b> Intervalo de confianza para la media de una distribución Normal con varianza conocida.</a></li>
<li class="chapter" data-level="10.2.4" data-path="pruebas-de-una-muestra.html"><a href="pruebas-de-una-muestra.html#c%C3%A1lculo-del-tama%C3%B1o-muestral-para-la-media-de-una-distribuci%C3%B3n-normal-con-varianza-conocida."><i class="fa fa-check"></i><b>10.2.4</b> Cálculo del tamaño muestral para la media de una distribución Normal con varianza conocida.</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="pruebas-de-una-muestra.html"><a href="pruebas-de-una-muestra.html#contraste-de-hip%C3%B3tesis-para-la-media-de-una-distribuci%C3%B3n-normal-con-varianza-desconocida-t-test."><i class="fa fa-check"></i><b>10.3</b> Contraste de hipótesis para la media de una distribución Normal con varianza desconocida: <span class="math inline">\(T\)</span>-test.</a>
<ul>
<li class="chapter" data-level="10.3.1" data-path="pruebas-de-una-muestra.html"><a href="pruebas-de-una-muestra.html#introducci%C3%B3n-4"><i class="fa fa-check"></i><b>10.3.1</b> Introducción</a></li>
<li class="chapter" data-level="10.3.2" data-path="pruebas-de-una-muestra.html"><a href="pruebas-de-una-muestra.html#resoluci%C3%B3n-del-contraste-para-la-media-de-una-distribuci%C3%B3n-normal-con-varianza-desconocida."><i class="fa fa-check"></i><b>10.3.2</b> Resolución del contraste para la media de una distribución Normal con varianza desconocida.</a></li>
<li class="chapter" data-level="10.3.3" data-path="pruebas-de-una-muestra.html"><a href="pruebas-de-una-muestra.html#intervalo-de-confianza-para-la-media-de-una-distribuci%C3%B3n-normal-con-varianza-desconocida."><i class="fa fa-check"></i><b>10.3.3</b> Intervalo de confianza para la media de una distribución Normal con varianza desconocida.</a></li>
<li class="chapter" data-level="10.3.4" data-path="pruebas-de-una-muestra.html"><a href="pruebas-de-una-muestra.html#c%C3%A1lculo-del-tama%C3%B1o-muestral-para-la-media-de-una-distribuci%C3%B3n-normal-con-varianza-desconocida."><i class="fa fa-check"></i><b>10.3.4</b> Cálculo del tamaño muestral para la media de una distribución Normal con varianza desconocida.</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="pruebas-de-una-muestra.html"><a href="pruebas-de-una-muestra.html#contraste-de-hip%C3%B3tesis-para-la-varianza-de-una-distribuci%C3%B3n-normal."><i class="fa fa-check"></i><b>10.4</b> Contraste de hipótesis para la varianza de una distribución Normal.</a>
<ul>
<li class="chapter" data-level="10.4.1" data-path="pruebas-de-una-muestra.html"><a href="pruebas-de-una-muestra.html#introducci%C3%B3n-5"><i class="fa fa-check"></i><b>10.4.1</b> Introducción</a></li>
<li class="chapter" data-level="10.4.2" data-path="pruebas-de-una-muestra.html"><a href="pruebas-de-una-muestra.html#informaci%C3%B3n-previa-premisas"><i class="fa fa-check"></i><b>10.4.2</b> Información previa (premisas)</a></li>
<li class="chapter" data-level="10.4.3" data-path="pruebas-de-una-muestra.html"><a href="pruebas-de-una-muestra.html#resoluci%C3%B3n-del-contraste-para-la-varianza-de-una-distribuci%C3%B3n-normal."><i class="fa fa-check"></i><b>10.4.3</b> Resolución del contraste para la varianza de una distribución Normal.</a></li>
<li class="chapter" data-level="10.4.4" data-path="pruebas-de-una-muestra.html"><a href="pruebas-de-una-muestra.html#intervalo-de-confianza-para-la-varianza-de-una-distribuci%C3%B3n-normal."><i class="fa fa-check"></i><b>10.4.4</b> Intervalo de confianza para la varianza de una distribución Normal.</a></li>
</ul></li>
<li class="chapter" data-level="10.5" data-path="pruebas-de-una-muestra.html"><a href="pruebas-de-una-muestra.html#contraste-de-hip%C3%B3tesis-para-la-proporci%C3%B3n."><i class="fa fa-check"></i><b>10.5</b> Contraste de hipótesis para la proporción.</a>
<ul>
<li class="chapter" data-level="10.5.1" data-path="pruebas-de-una-muestra.html"><a href="pruebas-de-una-muestra.html#introducci%C3%B3n-6"><i class="fa fa-check"></i><b>10.5.1</b> Introducción:</a></li>
<li class="chapter" data-level="10.5.2" data-path="pruebas-de-una-muestra.html"><a href="pruebas-de-una-muestra.html#informaci%C3%B3n-previa-premisas-1"><i class="fa fa-check"></i><b>10.5.2</b> Información previa (premisas)</a></li>
<li class="chapter" data-level="10.5.3" data-path="pruebas-de-una-muestra.html"><a href="pruebas-de-una-muestra.html#resoluci%C3%B3n-del-contraste-para-la-proporci%C3%B3n."><i class="fa fa-check"></i><b>10.5.3</b> Resolución del contraste para la proporción.</a></li>
<li class="chapter" data-level="10.5.4" data-path="pruebas-de-una-muestra.html"><a href="pruebas-de-una-muestra.html#intervalo-de-confianza-para-la-proporci%C3%B3n."><i class="fa fa-check"></i><b>10.5.4</b> Intervalo de confianza para la proporción.</a></li>
<li class="chapter" data-level="10.5.5" data-path="pruebas-de-una-muestra.html"><a href="pruebas-de-una-muestra.html#c%C3%A1lculo-del-tama%C3%B1o-muestral-para-la-proporci%C3%B3n."><i class="fa fa-check"></i><b>10.5.5</b> Cálculo del tamaño muestral para la proporción.</a></li>
</ul></li>
<li class="chapter" data-level="10.6" data-path="pruebas-de-una-muestra.html"><a href="pruebas-de-una-muestra.html#tabla-resumen-para-una-muestra."><i class="fa fa-check"></i><b>10.6</b> Tabla resumen para una muestra.</a></li>
<li class="chapter" data-level="10.7" data-path="pruebas-de-una-muestra.html"><a href="pruebas-de-una-muestra.html#la-importancia-de-elegir-correctamente-la-hip%C3%B3tesis-nula."><i class="fa fa-check"></i><b>10.7</b> La importancia de elegir correctamente la hipótesis nula.</a></li>
<li class="chapter" data-level="10.8" data-path="pruebas-de-una-muestra.html"><a href="pruebas-de-una-muestra.html#relaci%C3%B3n-con-los-intervalos-de-confianza-1"><i class="fa fa-check"></i><b>10.8</b> Relación con los intervalos de confianza</a></li>
<li class="chapter" data-level="10.9" data-path="pruebas-de-una-muestra.html"><a href="pruebas-de-una-muestra.html#relaci%C3%B3n-entre-el-intervalo-y-el-contraste"><i class="fa fa-check"></i><b>10.9</b> Relación entre el intervalo y el contraste</a>
<ul>
<li class="chapter" data-level="10.9.1" data-path="pruebas-de-una-muestra.html"><a href="pruebas-de-una-muestra.html#intervalo-de-confianza-para-la-varianza-de-una-distribuci%C3%B3n-normal-1"><i class="fa fa-check"></i><b>10.9.1</b> Intervalo de confianza para la varianza de una distribución Normal</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="contrastes-con-dos-muestras.html"><a href="contrastes-con-dos-muestras.html"><i class="fa fa-check"></i><b>11</b> Contrastes con dos muestras</a>
<ul>
<li class="chapter" data-level="11.1" data-path="contrastes-con-dos-muestras.html"><a href="contrastes-con-dos-muestras.html#introducci%C3%B3n-7"><i class="fa fa-check"></i><b>11.1</b> Introducción</a></li>
<li class="chapter" data-level="11.2" data-path="contrastes-con-dos-muestras.html"><a href="contrastes-con-dos-muestras.html#premisas-independencia-vs-datos-apareados"><i class="fa fa-check"></i><b>11.2</b> Premisas: independencia vs datos apareados</a>
<ul>
<li class="chapter" data-level="11.2.1" data-path="contrastes-con-dos-muestras.html"><a href="contrastes-con-dos-muestras.html#ejemplo-datos-independientes-vs-apareados"><i class="fa fa-check"></i><b>11.2.1</b> Ejemplo: Datos independientes vs apareados</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="contrastes-con-dos-muestras.html"><a href="contrastes-con-dos-muestras.html#premisas-e-hip%C3%B3tesis-en-comparaciones-de-medias-de-datos-normales-independientes."><i class="fa fa-check"></i><b>11.3</b> Premisas e hipótesis en comparaciones de medias de datos normales independientes.</a></li>
<li class="chapter" data-level="11.4" data-path="contrastes-con-dos-muestras.html"><a href="contrastes-con-dos-muestras.html#comparaci%C3%B3n-de-medias-de-datos-normales-independientes."><i class="fa fa-check"></i><b>11.4</b> Comparación de medias de datos normales independientes.</a>
<ul>
<li class="chapter" data-level="11.4.1" data-path="contrastes-con-dos-muestras.html"><a href="contrastes-con-dos-muestras.html#estad%C3%ADstico-de-test-y-valores-cr%C3%ADticos"><i class="fa fa-check"></i><b>11.4.1</b> Estadístico de test y valores críticos</a></li>
<li class="chapter" data-level="11.4.2" data-path="contrastes-con-dos-muestras.html"><a href="contrastes-con-dos-muestras.html#intervalo-de-confianza-para-la-diferencia-de-medias."><i class="fa fa-check"></i><b>11.4.2</b> Intervalo de confianza para la diferencia de medias.</a></li>
<li class="chapter" data-level="11.4.3" data-path="contrastes-con-dos-muestras.html"><a href="contrastes-con-dos-muestras.html#c%C3%A1lculo-del-tama%C3%B1o-de-muestra"><i class="fa fa-check"></i><b>11.4.3</b> Cálculo del tamaño de muestra</a></li>
</ul></li>
<li class="chapter" data-level="11.5" data-path="contrastes-con-dos-muestras.html"><a href="contrastes-con-dos-muestras.html#comparaci%C3%B3n-de-varianzas-de-datos-normales-independientes."><i class="fa fa-check"></i><b>11.5</b> Comparación de varianzas de datos normales independientes.</a>
<ul>
<li class="chapter" data-level="11.5.1" data-path="contrastes-con-dos-muestras.html"><a href="contrastes-con-dos-muestras.html#premisas-e-hip%C3%B3tesis-en"><i class="fa fa-check"></i><b>11.5.1</b> Premisas e hipótesis en</a></li>
<li class="chapter" data-level="11.5.2" data-path="contrastes-con-dos-muestras.html"><a href="contrastes-con-dos-muestras.html#estad%C3%ADstico-de-test-y-valores-cr%C3%ADticos-1"><i class="fa fa-check"></i><b>11.5.2</b> Estadístico de test y valores críticos</a></li>
<li class="chapter" data-level="11.5.3" data-path="contrastes-con-dos-muestras.html"><a href="contrastes-con-dos-muestras.html#intervalo-de-confianza-para-la-raz%C3%B3n-de-varianzas"><i class="fa fa-check"></i><b>11.5.3</b> Intervalo de confianza para la razón de varianzas</a></li>
</ul></li>
<li class="chapter" data-level="11.6" data-path="contrastes-con-dos-muestras.html"><a href="contrastes-con-dos-muestras.html#comparaciones-de-medias-de-datos-normales-apareados"><i class="fa fa-check"></i><b>11.6</b> Comparaciones de medias de datos normales apareados</a>
<ul>
<li class="chapter" data-level="11.6.1" data-path="contrastes-con-dos-muestras.html"><a href="contrastes-con-dos-muestras.html#premisas-e-hip%C3%B3tesis"><i class="fa fa-check"></i><b>11.6.1</b> Premisas e hipótesis</a></li>
<li class="chapter" data-level="11.6.2" data-path="contrastes-con-dos-muestras.html"><a href="contrastes-con-dos-muestras.html#relaci%C3%B3n-entre-el-contraste-de-datos-apareados-y-el-de-una-media-datos-normales."><i class="fa fa-check"></i><b>11.6.2</b> Relación entre el contraste de datos apareados y el de una media (datos normales).</a></li>
<li class="chapter" data-level="11.6.3" data-path="contrastes-con-dos-muestras.html"><a href="contrastes-con-dos-muestras.html#int%C3%A9rvalos-de-confianza-para-la-diferencia"><i class="fa fa-check"></i><b>11.6.3</b> Intérvalos de confianza para la diferencia</a></li>
<li class="chapter" data-level="11.6.4" data-path="contrastes-con-dos-muestras.html"><a href="contrastes-con-dos-muestras.html#tama%C3%B1o-muestral"><i class="fa fa-check"></i><b>11.6.4</b> Tamaño muestral</a></li>
<li class="chapter" data-level="11.6.5" data-path="contrastes-con-dos-muestras.html"><a href="contrastes-con-dos-muestras.html#ejemplo-efecto-de-una-intervenci%C3%B3n-sobre-el-colesterol-hdl"><i class="fa fa-check"></i><b>11.6.5</b> Ejemplo: efecto de una intervención sobre el colesterol HDL</a></li>
<li class="chapter" data-level="11.6.6" data-path="contrastes-con-dos-muestras.html"><a href="contrastes-con-dos-muestras.html#resumen-datos-independientes-frente-a-datos-apareados"><i class="fa fa-check"></i><b>11.6.6</b> Resumen: Datos independientes frente a datos apareados</a></li>
</ul></li>
<li class="chapter" data-level="11.7" data-path="contrastes-con-dos-muestras.html"><a href="contrastes-con-dos-muestras.html#comparaciones-de-2-proporciones-datos-independientes"><i class="fa fa-check"></i><b>11.7</b> Comparaciones de 2 proporciones (datos independientes)</a>
<ul>
<li class="chapter" data-level="11.7.1" data-path="contrastes-con-dos-muestras.html"><a href="contrastes-con-dos-muestras.html#premisas-e-hip%C3%B3tesis-1"><i class="fa fa-check"></i><b>11.7.1</b> Premisas e hipótesis</a></li>
<li class="chapter" data-level="11.7.2" data-path="contrastes-con-dos-muestras.html"><a href="contrastes-con-dos-muestras.html#estad%C3%ADstico-de-test-y-valores-cr%C3%ADticos-2"><i class="fa fa-check"></i><b>11.7.2</b> Estadístico de test y valores críticos</a></li>
<li class="chapter" data-level="11.7.3" data-path="contrastes-con-dos-muestras.html"><a href="contrastes-con-dos-muestras.html#condiciones-de-aplicaci%C3%B3n-del-test"><i class="fa fa-check"></i><b>11.7.3</b> Condiciones de aplicación del test</a></li>
<li class="chapter" data-level="11.7.4" data-path="contrastes-con-dos-muestras.html"><a href="contrastes-con-dos-muestras.html#intervalo-de-confianza-para-la-diferencia-de-proporciones-datos-independientes."><i class="fa fa-check"></i><b>11.7.4</b> Intervalo de confianza para la diferencia de proporciones (datos independientes).</a></li>
<li class="chapter" data-level="11.7.5" data-path="contrastes-con-dos-muestras.html"><a href="contrastes-con-dos-muestras.html#c%C3%A1lculo-del-tama%C3%B1o-de-muestra-en-el-contraste-de-proporciones-de-datos-independientes."><i class="fa fa-check"></i><b>11.7.5</b> Cálculo del tamaño de muestra en el contraste de proporciones de datos independientes.</a></li>
</ul></li>
<li class="chapter" data-level="11.8" data-path="contrastes-con-dos-muestras.html"><a href="contrastes-con-dos-muestras.html#comparaciones-de-dos-muestras-tabla-resumen"><i class="fa fa-check"></i><b>11.8</b> Comparaciones de dos muestras: Tabla resumen</a></li>
<li class="chapter" data-level="11.9" data-path="contrastes-con-dos-muestras.html"><a href="contrastes-con-dos-muestras.html#complementos-efecto-de-las-transformaciones-de-los-datos-en-el-test-t"><i class="fa fa-check"></i><b>11.9</b> Complementos: efecto de las transformaciones de los datos en el test t</a>
<ul>
<li class="chapter" data-level="11.9.1" data-path="contrastes-con-dos-muestras.html"><a href="contrastes-con-dos-muestras.html#efecto-del-cambi-de-posici%C3%B3n"><i class="fa fa-check"></i><b>11.9.1</b> Efecto del cambi de posición</a></li>
<li class="chapter" data-level="11.9.2" data-path="contrastes-con-dos-muestras.html"><a href="contrastes-con-dos-muestras.html#efecto-de-un-cambio-de-escala"><i class="fa fa-check"></i><b>11.9.2</b> Efecto de un cambio de escala</a></li>
</ul></li>
<li class="chapter" data-level="11.10" data-path="contrastes-con-dos-muestras.html"><a href="contrastes-con-dos-muestras.html#presentaci%C3%B3n-del-caso-1"><i class="fa fa-check"></i><b>11.10</b> Presentación del caso 1</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="las-pruebas-chi-cuadrado.html"><a href="las-pruebas-chi-cuadrado.html"><i class="fa fa-check"></i><b>12</b> Las pruebas “Chi-cuadrado”</a>
<ul>
<li class="chapter" data-level="12.1" data-path="las-pruebas-chi-cuadrado.html"><a href="las-pruebas-chi-cuadrado.html#introducci%C3%B3n-8"><i class="fa fa-check"></i><b>12.1</b> Introducción</a></li>
<li class="chapter" data-level="12.2" data-path="las-pruebas-chi-cuadrado.html"><a href="las-pruebas-chi-cuadrado.html#pruebas-chi2-de-bondad-de-ajuste"><i class="fa fa-check"></i><b>12.2</b> Pruebas <span class="math inline">\(\chi^2\)</span> de bondad de ajuste</a>
<ul>
<li class="chapter" data-level="12.2.1" data-path="las-pruebas-chi-cuadrado.html"><a href="las-pruebas-chi-cuadrado.html#hip%C3%B3tesis-simples"><i class="fa fa-check"></i><b>12.2.1</b> Hipótesis simples</a></li>
<li class="chapter" data-level="12.2.2" data-path="las-pruebas-chi-cuadrado.html"><a href="las-pruebas-chi-cuadrado.html#estad%C3%ADstico-de-pearson"><i class="fa fa-check"></i><b>12.2.2</b> Estadístico de Pearson</a></li>
<li class="chapter" data-level="12.2.3" data-path="las-pruebas-chi-cuadrado.html"><a href="las-pruebas-chi-cuadrado.html#observaciones-pr%C3%A1cticas"><i class="fa fa-check"></i><b>12.2.3</b> Observaciones prácticas</a></li>
<li class="chapter" data-level="12.2.4" data-path="las-pruebas-chi-cuadrado.html"><a href="las-pruebas-chi-cuadrado.html#ejemplo-1-bondad-de-ajuste"><i class="fa fa-check"></i><b>12.2.4</b> Ejemplo 1: Bondad de ajuste</a></li>
<li class="chapter" data-level="12.2.5" data-path="las-pruebas-chi-cuadrado.html"><a href="las-pruebas-chi-cuadrado.html#ejemplo-2-ajuste-de-modelo-gen%C3%A9tico"><i class="fa fa-check"></i><b>12.2.5</b> Ejemplo 2: Ajuste de modelo genético</a></li>
<li class="chapter" data-level="12.2.6" data-path="las-pruebas-chi-cuadrado.html"><a href="las-pruebas-chi-cuadrado.html#hip%C3%B3tesis-compuestas"><i class="fa fa-check"></i><b>12.2.6</b> Hipótesis compuestas</a></li>
<li class="chapter" data-level="12.2.7" data-path="las-pruebas-chi-cuadrado.html"><a href="las-pruebas-chi-cuadrado.html#ejemplo-3-ajuste-de-modelo-gen%C3%A9tico-con-probabilidades-desconocidas"><i class="fa fa-check"></i><b>12.2.7</b> Ejemplo 3: Ajuste de modelo genético con probabilidades desconocidas</a></li>
<li class="chapter" data-level="12.2.8" data-path="las-pruebas-chi-cuadrado.html"><a href="las-pruebas-chi-cuadrado.html#ejemplo-4-ajuste-a-una-distribuci%C3%B3n-normal"><i class="fa fa-check"></i><b>12.2.8</b> Ejemplo 4: Ajuste a una distribución normal</a></li>
</ul></li>
<li class="chapter" data-level="12.3" data-path="las-pruebas-chi-cuadrado.html"><a href="las-pruebas-chi-cuadrado.html#pruebas-de-independencia-en-tablas-de-contingencia"><i class="fa fa-check"></i><b>12.3</b> Pruebas de independencia en tablas de contingencia</a>
<ul>
<li class="chapter" data-level="12.3.1" data-path="las-pruebas-chi-cuadrado.html"><a href="las-pruebas-chi-cuadrado.html#ejemplo-5-relaci%C3%B3n-entre-nivel-de-estudios-y-preferencias"><i class="fa fa-check"></i><b>12.3.1</b> Ejemplo 5: Relación entre nivel de estudios y preferencias</a></li>
</ul></li>
<li class="chapter" data-level="12.4" data-path="las-pruebas-chi-cuadrado.html"><a href="las-pruebas-chi-cuadrado.html#pruebas-de-homogeneidad"><i class="fa fa-check"></i><b>12.4</b> Pruebas de homogeneidad</a>
<ul>
<li class="chapter" data-level="12.4.1" data-path="las-pruebas-chi-cuadrado.html"><a href="las-pruebas-chi-cuadrado.html#ejemplo-6-homogeneidad-en-los-grupos-sangu%C3%ADneos"><i class="fa fa-check"></i><b>12.4.1</b> Ejemplo 6: Homogeneidad en los grupos sanguíneos</a></li>
</ul></li>
<li class="chapter" data-level="12.5" data-path="las-pruebas-chi-cuadrado.html"><a href="las-pruebas-chi-cuadrado.html#ejemplos-resueltos-en-r"><i class="fa fa-check"></i><b>12.5</b> Ejemplos resueltos en R</a>
<ul>
<li class="chapter" data-level="12.5.1" data-path="las-pruebas-chi-cuadrado.html"><a href="las-pruebas-chi-cuadrado.html#ejemplo-1-bondad-de-ajuste-1"><i class="fa fa-check"></i><b>12.5.1</b> Ejemplo 1: Bondad de ajuste</a></li>
<li class="chapter" data-level="12.5.2" data-path="las-pruebas-chi-cuadrado.html"><a href="las-pruebas-chi-cuadrado.html#ejemplo-2-ajuste-de-modelo-gen%C3%A9tico-1"><i class="fa fa-check"></i><b>12.5.2</b> Ejemplo 2: Ajuste de modelo genético</a></li>
<li class="chapter" data-level="12.5.3" data-path="las-pruebas-chi-cuadrado.html"><a href="las-pruebas-chi-cuadrado.html#ejemplo-3-ajuste-de-modelo-gen%C3%A9tico-con-probabilidades-desconocidas-1"><i class="fa fa-check"></i><b>12.5.3</b> Ejemplo 3: Ajuste de modelo genético con probabilidades desconocidas</a></li>
<li class="chapter" data-level="12.5.4" data-path="las-pruebas-chi-cuadrado.html"><a href="las-pruebas-chi-cuadrado.html#ejemplo-4-ajuste-a-una-distribuci%C3%B3n-normal-1"><i class="fa fa-check"></i><b>12.5.4</b> Ejemplo 4: Ajuste a una distribución normal</a></li>
<li class="chapter" data-level="12.5.5" data-path="las-pruebas-chi-cuadrado.html"><a href="las-pruebas-chi-cuadrado.html#ejemplo-5-relaci%C3%B3n-entre-nivel-de-estudios-y-preferencias-1"><i class="fa fa-check"></i><b>12.5.5</b> Ejemplo 5: Relación entre nivel de estudios y preferencias</a></li>
<li class="chapter" data-level="12.5.6" data-path="las-pruebas-chi-cuadrado.html"><a href="las-pruebas-chi-cuadrado.html#ejemplo-6-homogeneidad-en-los-grupos-sangu%C3%ADneos-1"><i class="fa fa-check"></i><b>12.5.6</b> Ejemplo 6: Homogeneidad en los grupos sanguíneos</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="13" data-path="estadística-no-paramétrica.html"><a href="estadística-no-paramétrica.html"><i class="fa fa-check"></i><b>13</b> Estadística no paramétrica</a>
<ul>
<li class="chapter" data-level="13.1" data-path="estadística-no-paramétrica.html"><a href="estadística-no-paramétrica.html#introducci%C3%B3n-9"><i class="fa fa-check"></i><b>13.1</b> Introducción</a></li>
<li class="chapter" data-level="13.2" data-path="estadística-no-paramétrica.html"><a href="estadística-no-paramétrica.html#test-de-los-signos"><i class="fa fa-check"></i><b>13.2</b> Test de los signos</a>
<ul>
<li class="chapter" data-level="13.2.1" data-path="estadística-no-paramétrica.html"><a href="estadística-no-paramétrica.html#test-para-la-mediana"><i class="fa fa-check"></i><b>13.2.1</b> Test para la mediana</a></li>
<li class="chapter" data-level="13.2.2" data-path="estadística-no-paramétrica.html"><a href="estadística-no-paramétrica.html#test-de-los-signos-para-datos-apareados"><i class="fa fa-check"></i><b>13.2.2</b> Test de los signos para datos apareados</a></li>
<li class="chapter" data-level="13.2.3" data-path="estadística-no-paramétrica.html"><a href="estadística-no-paramétrica.html#test-para-datos-binarios"><i class="fa fa-check"></i><b>13.2.3</b> Test para datos binarios</a></li>
</ul></li>
<li class="chapter" data-level="13.3" data-path="estadística-no-paramétrica.html"><a href="estadística-no-paramétrica.html#test-de-mcnemar"><i class="fa fa-check"></i><b>13.3</b> Test de McNemar</a></li>
<li class="chapter" data-level="13.4" data-path="estadística-no-paramétrica.html"><a href="estadística-no-paramétrica.html#test-de-los-rangos-con-signo-de-wilcoxon"><i class="fa fa-check"></i><b>13.4</b> Test de los rangos con signo de Wilcoxon</a>
<ul>
<li class="chapter" data-level="13.4.1" data-path="estadística-no-paramétrica.html"><a href="estadística-no-paramétrica.html#observaciones-3"><i class="fa fa-check"></i><b>13.4.1</b> Observaciones</a></li>
<li class="chapter" data-level="13.4.2" data-path="estadística-no-paramétrica.html"><a href="estadística-no-paramétrica.html#ejemplo-4-1"><i class="fa fa-check"></i><b>13.4.2</b> Ejemplo 4</a></li>
<li class="chapter" data-level="13.4.3" data-path="estadística-no-paramétrica.html"><a href="estadística-no-paramétrica.html#ejemplo-5-1"><i class="fa fa-check"></i><b>13.4.3</b> Ejemplo 5</a></li>
</ul></li>
<li class="chapter" data-level="13.5" data-path="estadística-no-paramétrica.html"><a href="estadística-no-paramétrica.html#el-test-u-de-mann-whitney"><i class="fa fa-check"></i><b>13.5</b> El test <span class="math inline">\(U\)</span> de Mann-Whitney</a>
<ul>
<li class="chapter" data-level="13.5.1" data-path="estadística-no-paramétrica.html"><a href="estadística-no-paramétrica.html#observaciones-4"><i class="fa fa-check"></i><b>13.5.1</b> Observaciones</a></li>
<li class="chapter" data-level="13.5.2" data-path="estadística-no-paramétrica.html"><a href="estadística-no-paramétrica.html#ejemplo-6-1"><i class="fa fa-check"></i><b>13.5.2</b> Ejemplo 6</a></li>
</ul></li>
<li class="chapter" data-level="13.6" data-path="estadística-no-paramétrica.html"><a href="estadística-no-paramétrica.html#comparaci%C3%B3n-de-medianas"><i class="fa fa-check"></i><b>13.6</b> Comparación de medianas</a>
<ul>
<li class="chapter" data-level="13.6.1" data-path="estadística-no-paramétrica.html"><a href="estadística-no-paramétrica.html#ejemplo-7-1"><i class="fa fa-check"></i><b>13.6.1</b> Ejemplo 7</a></li>
</ul></li>
<li class="chapter" data-level="13.7" data-path="estadística-no-paramétrica.html"><a href="estadística-no-paramétrica.html#test-de-kolmogorov-smirnov-para-la-homogeneidad"><i class="fa fa-check"></i><b>13.7</b> Test de Kolmogorov-Smirnov para la homogeneidad</a>
<ul>
<li class="chapter" data-level="13.7.1" data-path="estadística-no-paramétrica.html"><a href="estadística-no-paramétrica.html#ejemplo-8-1"><i class="fa fa-check"></i><b>13.7.1</b> Ejemplo 8</a></li>
</ul></li>
<li class="chapter" data-level="13.8" data-path="estadística-no-paramétrica.html"><a href="estadística-no-paramétrica.html#test-h-de-kruskal-wallis"><i class="fa fa-check"></i><b>13.8</b> Test <span class="math inline">\(H\)</span> de Kruskal-Wallis</a>
<ul>
<li class="chapter" data-level="13.8.1" data-path="estadística-no-paramétrica.html"><a href="estadística-no-paramétrica.html#observaciones-5"><i class="fa fa-check"></i><b>13.8.1</b> Observaciones</a></li>
<li class="chapter" data-level="13.8.2" data-path="estadística-no-paramétrica.html"><a href="estadística-no-paramétrica.html#ejemplo-9-1"><i class="fa fa-check"></i><b>13.8.2</b> Ejemplo 9</a></li>
</ul></li>
<li class="chapter" data-level="13.9" data-path="estadística-no-paramétrica.html"><a href="estadística-no-paramétrica.html#test-de-friedman"><i class="fa fa-check"></i><b>13.9</b> Test de Friedman</a>
<ul>
<li class="chapter" data-level="13.9.1" data-path="estadística-no-paramétrica.html"><a href="estadística-no-paramétrica.html#ejemplo-10"><i class="fa fa-check"></i><b>13.9.1</b> Ejemplo 10</a></li>
<li class="chapter" data-level="13.9.2" data-path="estadística-no-paramétrica.html"><a href="estadística-no-paramétrica.html#observaciones-6"><i class="fa fa-check"></i><b>13.9.2</b> Observaciones</a></li>
</ul></li>
<li class="chapter" data-level="13.10" data-path="estadística-no-paramétrica.html"><a href="estadística-no-paramétrica.html#coeficientes-de-correlaci%C3%B3n-no-param%C3%A9tricos"><i class="fa fa-check"></i><b>13.10</b> Coeficientes de correlación no paramétricos</a>
<ul>
<li class="chapter" data-level="13.10.1" data-path="estadística-no-paramétrica.html"><a href="estadística-no-paramétrica.html#coeficiente-tau-de-kendall"><i class="fa fa-check"></i><b>13.10.1</b> Coeficiente <span class="math inline">\(\tau\)</span> de Kendall</a></li>
<li class="chapter" data-level="13.10.2" data-path="estadística-no-paramétrica.html"><a href="estadística-no-paramétrica.html#coeficiente-de-correlaci%C3%B3n-por-rangos-de-spearman"><i class="fa fa-check"></i><b>13.10.2</b> Coeficiente de correlación por rangos de Spearman</a></li>
<li class="chapter" data-level="13.10.3" data-path="estadística-no-paramétrica.html"><a href="estadística-no-paramétrica.html#el-par%C3%A1metro-poblacional-asociado-a-los-coeficientes-de-correlaci%C3%B3n-no-param%C3%A9tricos"><i class="fa fa-check"></i><b>13.10.3</b> El parámetro poblacional asociado a los coeficientes de correlación no paramétricos</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="bibliografia.html"><a href="bibliografia.html"><i class="fa fa-check"></i>Bibliografia</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Fundamentos de Inferencia Estadistica</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="estadística-no-paramétrica" class="section level1 hasAnchor" number="13">
<h1><span class="header-section-number">Capítulo 13</span> Estadística no paramétrica<a href="estadística-no-paramétrica.html#estad%C3%ADstica-no-param%C3%A9trica" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="introducción-9" class="section level2 hasAnchor" number="13.1">
<h2><span class="header-section-number">13.1</span> Introducción<a href="estadística-no-paramétrica.html#introducci%C3%B3n-9" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>En este capítulo presentaremos de forma breve algunos tests no paramétricos para problemas de una, dos o varias muestras. El objetivo de estos tests es disponer de alternativas a las pruebas de hipótesis de comparación clásicas cuando no se conoce la forma de la distribución de los datos o la ley de las variables. En particular, serán alternativas a los tests sobre la media o comparación de medias cuando no se verifica la suposición de normalidad de los datos. Nos referimos a los tests basados en poblaciones normales como contrastes paramétricos, ya que se basan en comparar medias o parámetros de la ley normal. En contraposición, los que consideramos aquí y que denominaremos contrastes no paramétricos pueden comparar medianas, cuantiles o incluso toda la distribución en bloque. Observemos, pues, que “no paramétrico” no significa que estos tests no comparen algún parámetro como la mediana; más bien significa que no queremos hacer determinadas suposiciones sobre la función de distribución de las variables.</p>
<p>Desde un punto de vista práctico, que es también el que adoptan muchos programas informáticos de análisis estadístico, distinguiremos entre:</p>
<ul>
<li>Problemas de una muestra</li>
<li>Problemas de dos muestras con datos apareados</li>
<li>Problemas de dos muestras independientes</li>
<li>Problemas de <span class="math inline">\(k\)</span> muestras independientes.</li>
</ul>
<p>Esta distinción nos permite clasificar las técnicas que estudiamos y compararlas con las correspondientes pruebas paramétricas:</p>
<table>
<colgroup>
<col width="33%" />
<col width="33%" />
<col width="33%" />
</colgroup>
<thead>
<tr class="header">
<th align="left">Problema</th>
<th align="left">Test paramétrico</th>
<th align="left">Test no paramétrico</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Una muestra</td>
<td align="left">Test <span class="math inline">\(t\)</span> de una muestra</td>
<td align="left">Test de los signos <br> Test de los rangos con signo</td>
</tr>
<tr class="even">
<td align="left">Datos apareados</td>
<td align="left">Test <span class="math inline">\(t\)</span> de datos <br> apareados</td>
<td align="left">Test de los signos <br> Test de los rangos con signo</td>
</tr>
<tr class="odd">
<td align="left">Dos muestras ind.</td>
<td align="left">Test <span class="math inline">\(t\)</span> para dos <br> muestras ind. (con <br> test <span class="math inline">\(F\)</span> previo)</td>
<td align="left">Test <span class="math inline">\(U\)</span> de Mann-Whitney</td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(k\)</span> muestras ind.</td>
<td align="left">ANOVA de un factor</td>
<td align="left">Test de Kruskal-Wallis</td>
</tr>
</tbody>
</table>
<p>Además, algunos tests no paramétricos tienen otras utilidades, como los tests de aleatoriedad, los tests de rachas, etc.</p>
</div>
<div id="test-de-los-signos" class="section level2 hasAnchor" number="13.2">
<h2><span class="header-section-number">13.2</span> Test de los signos<a href="estadística-no-paramétrica.html#test-de-los-signos" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="test-para-la-mediana" class="section level3 hasAnchor" number="13.2.1">
<h3><span class="header-section-number">13.2.1</span> Test para la mediana<a href="estadística-no-paramétrica.html#test-para-la-mediana" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Sea <span class="math inline">\(X\)</span> una variable aleatoria con distribución continua <span class="math inline">\(F_{X}\)</span> desconocida y <span class="math inline">\(M=Q_{50}\)</span> su mediana o cuantil del <span class="math inline">\(50\%\)</span>, es decir, el valor tal que</p>
<p><span class="math display">\[
P(X \leq M)=0.5
\]</span></p>
<p>Supongamos que queremos contrastar las hipótesis</p>
<p><span class="math display">\[
\begin{aligned}
&amp; H_{0}: M=m_{0} \\
&amp; H_{1}: M \neq m_{0}
\end{aligned}
\]</span></p>
<p>Dada una muestra <span class="math inline">\(x_{1}, x_{2}, \ldots, x_{n}\)</span>, consideremos el “signo” de cada valor muestral por comparación con la mediana propuesta por la hipótesis <span class="math inline">\(H_{0}\)</span>, es decir,</p>
<p><span class="math display">\[
\operatorname{signe}\left(x_{i}\right)= \begin{cases}+ &amp; \text { si } x_{i}&gt;m_{0} \\ - &amp; \text { si } x_{i}&lt;m_{0}\end{cases}
\]</span></p>
<p>El estadístico <span class="math inline">\(B=\)</span> “Número de signos positivos” es:</p>
<p><span class="math display">\[
B\left(x_{1}, x_{2}, \ldots, x_{n}\right)=\sum_{i=1}^{n} I_{x_{i}&gt;m_{0}}
\]</span></p>
<p>donde</p>
<p><span class="math display">\[
I_{x_{i}&gt;m_{0}}=\left\{\begin{array}{cc}
1 &amp; \text { si } x_{i}&gt;m_{0}\left(\operatorname{signe}\left(x_{i}\right)=+\right) \\
0 &amp; \text { si } x_{i}&lt;m_{0}\left(\operatorname{signe}\left(x_{i}\right)=-\right)
\end{array}\right.
\]</span></p>
<p>Si la hipótesis nula es cierta, la distribución del estadístico <span class="math inline">\(B\)</span> será una binomial de parámetros <span class="math inline">\(n\)</span> y <span class="math inline">\(1/2\)</span>, y es razonable esperar que <span class="math inline">\(B\)</span> tome valores próximos a <span class="math inline">\(n/2\)</span>; mientras que cuando sea falsa es de esperar que tome valores en las colas de la distribución. Así pues, una región crítica para el test será aquella en la que el número de signos positivos sea demasiado alto o demasiado bajo como para ser coherente con la hipótesis nula, que implica que hay tantos positivos como negativos. Podemos tomar como región crítica:</p>
<p><span class="math display">\[
W=\left\{B(\mathbf{x}) \leq b_{\alpha / 2}\right\} \cup\left\{B(\mathbf{x}) \geq b_{1-\alpha / 2}\right\}
\]</span></p>
<p>donde <span class="math inline">\(b_{\alpha / 2}\)</span> y <span class="math inline">\(b_{1-\alpha / 2}\)</span> se determinan de forma que la probabilidad de las dos colas de una distribución <span class="math inline">\(B\left(n, \frac{1}{2}\right)\)</span> sea igual al nivel de significación <span class="math inline">\(\alpha\)</span> (o algo menor que <span class="math inline">\(\alpha\)</span>), es decir,</p>
<p><span class="math display">\[
\sum_{i=0}^{b_{\alpha / 2}}\binom{n}{i}\left(\frac{1}{2}\right)^{i}\left(\frac{1}{2}\right)^{n-i}+\sum_{i=b_{1-\alpha / 2}}^{n}\binom{n}{i}\left(\frac{1}{2}\right)^{i}\left(\frac{1}{2}\right)^{n-i} \leq \alpha
\]</span></p>
<div id="observaciones-2" class="section level4 hasAnchor" number="13.2.1.1">
<h4><span class="header-section-number">13.2.1.1</span> Observaciones<a href="estadística-no-paramétrica.html#observaciones-2" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li>Aunque con probabilidad teórica cero, porque la variable considerada tiene función de distribución continua, puede darse el caso <span class="math inline">\(x_{i}=m_{0}\)</span>, de signo indefinido. Si no es posible aumentar la precisión, se aconseja eliminar este valor muestral y descontarlo en consecuencia del tamaño de la muestra.</li>
<li>Si la hipótesis alternativa es <span class="math inline">\(M&lt;m_{0}\)</span> o bien <span class="math inline">\(M&gt;m_{0}\)</span>, la región crítica se adapta a esta hipótesis de forma razonable, es decir:</li>
</ul>
<p><span class="math display">\[
\begin{aligned}
&amp; H_{1}: M&lt;m_{0} \quad \Rightarrow \quad W=\left\{B(\mathbf{x}) \leq b_{\alpha}\right\} \\
&amp; H_{1}: M&gt;m_{0} \quad \Rightarrow \quad W=\left\{B(\mathbf{x}) \geq b_{1-\alpha / 2}\right\}
\end{aligned}
\]</span></p>
<ul>
<li>Algunos libros incluyen tablas de la distribución binomial que podemos usar para encontrar los valores críticos. Para <span class="math inline">\(n \geq 20\)</span> podemos aproximar la binomial por una normal.</li>
</ul>
</div>
<div id="ejemplo-1-1" class="section level4 hasAnchor" number="13.2.1.2">
<h4><span class="header-section-number">13.2.1.2</span> Ejemplo 1<a href="estadística-no-paramétrica.html#ejemplo-1-1" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>La siguiente tabla recoge una muestra de 40 notas en un examen. Contraste, con un nivel de significación 0.05, la hipótesis de que el valor central (mediana) de las notas es 66.</p>
<table>
<thead>
<tr class="header">
<th align="center">71</th>
<th align="center">67</th>
<th align="center">55</th>
<th align="center">64</th>
<th align="center">82</th>
<th align="center">66</th>
<th align="center">74</th>
<th align="center">58</th>
<th align="center">79</th>
<th align="center">61</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">78</td>
<td align="center">46</td>
<td align="center">84</td>
<td align="center">93</td>
<td align="center">72</td>
<td align="center">54</td>
<td align="center">78</td>
<td align="center">86</td>
<td align="center">48</td>
<td align="center">52</td>
</tr>
<tr class="even">
<td align="center">67</td>
<td align="center">95</td>
<td align="center">70</td>
<td align="center">43</td>
<td align="center">70</td>
<td align="center">73</td>
<td align="center">57</td>
<td align="center">64</td>
<td align="center">60</td>
<td align="center">83</td>
</tr>
<tr class="odd">
<td align="center">73</td>
<td align="center">40</td>
<td align="center">78</td>
<td align="center">70</td>
<td align="center">64</td>
<td align="center">86</td>
<td align="center">76</td>
<td align="center">62</td>
<td align="center">95</td>
<td align="center">66</td>
</tr>
<tr class="even">
<td align="center">+</td>
<td align="center">+</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">+</td>
<td align="center">0</td>
<td align="center">+</td>
<td align="center">-</td>
<td align="center">+</td>
<td align="center">-</td>
</tr>
<tr class="odd">
<td align="center">+</td>
<td align="center">-</td>
<td align="center">+</td>
<td align="center">+</td>
<td align="center">+</td>
<td align="center">-</td>
<td align="center">+</td>
<td align="center">+</td>
<td align="center">-</td>
<td align="center">-</td>
</tr>
<tr class="even">
<td align="center">+</td>
<td align="center">+</td>
<td align="center">+</td>
<td align="center">-</td>
<td align="center">+</td>
<td align="center">+</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">+</td>
</tr>
<tr class="odd">
<td align="center">+</td>
<td align="center">-</td>
<td align="center">+</td>
<td align="center">+</td>
<td align="center">-</td>
<td align="center">+</td>
<td align="center">+</td>
<td align="center">-</td>
<td align="center">+</td>
<td align="center">0</td>
</tr>
</tbody>
</table>
<p>Si restamos 66 de las notas observadas y retenemos solo los signos de las diferencias, se obtienen 23 signos +, 15 signos - y 2 ceros. Descartados los ceros, <span class="math inline">\(B=23\)</span> sobre un total de 38. Si hacemos un contraste bilateral con la aproximación normal, la región de aceptación es <span class="math inline">\(\{-1.96 \leq z \leq 1.96\}\)</span>.
Dado que</p>
<p><span class="math display">\[
z=\frac{(23-0.5)-38 \cdot 0.5}{\sqrt{38 \cdot 0.5 \cdot 0.5}}=1.14
\]</span></p>
<p>aceptamos la hipótesis de que la mediana es 66, al nivel 0.05.</p>
<p>Podemos hacer lo mismo con R:</p>
<p>Primero introducimos las notas y calculamos cuántos valores son superiores a 66, es decir, el número de signos positivos.</p>
<div class="sourceCode" id="cb148"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb148-1"><a href="estadística-no-paramétrica.html#cb148-1" tabindex="-1"></a>notes<span class="ot">&lt;-</span><span class="fu">c</span> (<span class="dv">71</span>,<span class="dv">67</span>,<span class="dv">55</span>,<span class="dv">64</span>,<span class="dv">82</span>,<span class="dv">66</span>,<span class="dv">74</span>,<span class="dv">58</span>,<span class="dv">79</span>,<span class="dv">61</span>,<span class="dv">78</span>,<span class="dv">46</span>,<span class="dv">84</span>,<span class="dv">93</span>,<span class="dv">72</span>,<span class="dv">54</span>,<span class="dv">78</span>,<span class="dv">86</span>,<span class="dv">48</span>,<span class="dv">52</span>,<span class="dv">67</span>,<span class="dv">95</span>,<span class="dv">70</span>,<span class="dv">43</span>,<span class="dv">70</span>,<span class="dv">73</span>,<span class="dv">57</span>,<span class="dv">64</span>,<span class="dv">60</span>,<span class="dv">83</span>,<span class="dv">73</span>,<span class="dv">40</span>,<span class="dv">78</span>,<span class="dv">70</span>,<span class="dv">64</span>,<span class="dv">86</span>,<span class="dv">76</span>,<span class="dv">62</span>,<span class="dv">95</span>,<span class="dv">66</span>)</span>
<span id="cb148-2"><a href="estadística-no-paramétrica.html#cb148-2" tabindex="-1"></a>notes[notes<span class="sc">&gt;</span><span class="dv">66</span>]</span></code></pre></div>
<pre><code>##  [1] 71 67 82 74 79 78 84 93 72 78 86 67 95 70 70 73 83 73 78 70 86 76 95</code></pre>
<div class="sourceCode" id="cb150"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb150-1"><a href="estadística-no-paramétrica.html#cb150-1" tabindex="-1"></a>B<span class="ot">&lt;-</span><span class="fu">length</span>(notes[notes<span class="sc">&gt;</span><span class="dv">66</span>])</span>
<span id="cb150-2"><a href="estadística-no-paramétrica.html#cb150-2" tabindex="-1"></a>B</span></code></pre></div>
<pre><code>## [1] 23</code></pre>
<p>Y finalmente calculamos la aproximación normal del estadístico y su p-valor. Aplicaremos una <em>corrección por continuidad</em>, habitual en esta aproximación, restando 0.5 al estadístico B</p>
<div class="sourceCode" id="cb152"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb152-1"><a href="estadística-no-paramétrica.html#cb152-1" tabindex="-1"></a>n<span class="ot">&lt;-</span><span class="fu">length</span>(notes)<span class="sc">-</span><span class="fu">length</span>(notes[notes<span class="sc">==</span><span class="dv">66</span>])</span>
<span id="cb152-2"><a href="estadística-no-paramétrica.html#cb152-2" tabindex="-1"></a>n</span></code></pre></div>
<pre><code>## [1] 38</code></pre>
<div class="sourceCode" id="cb154"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb154-1"><a href="estadística-no-paramétrica.html#cb154-1" tabindex="-1"></a>z<span class="ot">&lt;-</span>(B<span class="fl">-0.5</span><span class="sc">-</span>n<span class="sc">*</span><span class="fl">0.5</span>)<span class="sc">/</span><span class="fu">sqrt</span>(n<span class="sc">*</span><span class="fl">0.5</span><span class="sc">*</span><span class="fl">0.5</span>)</span>
<span id="cb154-2"><a href="estadística-no-paramétrica.html#cb154-2" tabindex="-1"></a><span class="fu">round</span>(z,<span class="dv">2</span>)</span></code></pre></div>
<pre><code>## [1] 1.14</code></pre>
<div class="sourceCode" id="cb156"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb156-1"><a href="estadística-no-paramétrica.html#cb156-1" tabindex="-1"></a><span class="dv">2</span><span class="sc">*</span>(<span class="dv">1</span><span class="sc">-</span><span class="fu">pnorm</span>(z))</span></code></pre></div>
<pre><code>## [1] 0.256145</code></pre>
<div class="sourceCode" id="cb158"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb158-1"><a href="estadística-no-paramétrica.html#cb158-1" tabindex="-1"></a><span class="dv">2</span><span class="sc">*</span>(<span class="dv">1</span><span class="sc">-</span><span class="fu">pnorm</span>(B<span class="fl">-0.5</span>,<span class="at">mean=</span>n<span class="sc">*</span><span class="fl">0.5</span>,<span class="at">sd=</span><span class="fu">sqrt</span>(n<span class="sc">*</span><span class="fl">0.5</span><span class="sc">*</span><span class="fl">0.5</span>)))</span></code></pre></div>
<pre><code>## [1] 0.256145</code></pre>
<p>El p-valor, superior al nivel de significación, hace que aceptemos la hipótesis nula <span class="math inline">\(H_{0}: M=66\)</span>.</p>
</div>
</div>
<div id="test-de-los-signos-para-datos-apareados" class="section level3 hasAnchor" number="13.2.2">
<h3><span class="header-section-number">13.2.2</span> Test de los signos para datos apareados<a href="estadística-no-paramétrica.html#test-de-los-signos-para-datos-apareados" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>El test de los signos puede servir también en el caso de datos apareados.
Consideremos una muestra de dos variables <span class="math inline">\(X, Y\)</span></p>
<p><span class="math display">\[
\left(x_{1}, y_{1}\right),\left(x_{2}, y_{2}\right), \ldots,\left(x_{n}, y_{n}\right)
\]</span></p>
<p>con <span class="math inline">\(n\)</span> observaciones en dos situaciones lo más homogéneas posible.
Supongamos que las distribuciones de las dos variables son similares, excepto quizá en un parámetro de localización como la mediana. Es decir, las dos situaciones consideradas solo pueden desplazar la distribución y no modifican su forma.</p>
<p>Ahora queremos contrastar la hipótesis de que no hay diferencia entre las dos situaciones: las diferencias observadas entre los valores <span class="math inline">\(x_{i}\)</span> y <span class="math inline">\(y_{i}\)</span> se deben al azar; es decir, las dos muestras <span class="math inline">\(x_{1}, \ldots, x_{n}\)</span> e <span class="math inline">\(y_{1}, \ldots, y_{n}\)</span> proceden de la misma población. Esto puede expresarse estadísticamente con la hipótesis <span class="math inline">\(H_{0}\)</span> de igualdad de las distribuciones de probabilidad, que con las suposiciones asumidas es equivalente a la igualdad de medianas.
Si la hipótesis <span class="math inline">\(H_{0}\)</span> es cierta, y la distribución de la variable diferencia <span class="math inline">\(D=X-Y\)</span> es simétrica respecto del origen, se verificará</p>
<p><span class="math display">\[
P(X&gt;Y)=P(X-Y&gt;0)=\frac{1}{2}
\]</span></p>
<p>Así pues, podemos aplicar el test de los signos a la variable diferencia <span class="math inline">\(D=X-Y\)</span>. En general, aunque no necesariamente siempre, se tomará como valor de <span class="math inline">\(m_{0}\)</span> el 0.</p>
<div id="ejemplo-2-1" class="section level4 hasAnchor" number="13.2.2.1">
<h4><span class="header-section-number">13.2.2.1</span> Ejemplo 2<a href="estadística-no-paramétrica.html#ejemplo-2-1" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Se quiere comparar el número de piezas defectuosas producidas por dos máquinas diferentes. Se observa la producción durante 10 días, con la misma producción diaria para ambas máquinas, aunque diferente cada día. Los resultados son:</p>
<table>
<thead>
<tr class="header">
<th align="left">Día</th>
<th align="center">1</th>
<th align="center">2</th>
<th align="center">3</th>
<th align="center">4</th>
<th align="center">5</th>
<th align="center">6</th>
<th align="center">7</th>
<th align="center">8</th>
<th align="center">9</th>
<th align="center">10</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Máquina 1</td>
<td align="center">46</td>
<td align="center">110</td>
<td align="center">70</td>
<td align="center">54</td>
<td align="center">60</td>
<td align="center">120</td>
<td align="center">82</td>
<td align="center">76</td>
<td align="center">37</td>
<td align="center">28</td>
</tr>
<tr class="even">
<td align="left">Máquina 2</td>
<td align="center">42</td>
<td align="center">87</td>
<td align="center">75</td>
<td align="center">50</td>
<td align="center">48</td>
<td align="center">108</td>
<td align="center">80</td>
<td align="center">67</td>
<td align="center">40</td>
<td align="center">25</td>
</tr>
</tbody>
</table>
<p>Con un nivel de significación <span class="math inline">\(\alpha=0.06\)</span>, ¿podemos aceptar que la primera máquina produce más piezas defectuosas?</p>
<p><strong>Solución</strong>:</p>
<p>El hecho de que la producción total diaria de ambas máquinas sea la misma permite considerar los datos como apareados. Que la producción diaria sea diferente cada día aconseja utilizar un test no paramétrico.
Observemos los signos de las diferencias</p>
<p><span class="math display">\[
\begin{array}{lcccccccccc}
\text { Día: } &amp; 1 &amp; 2 &amp; 3 &amp; 4 &amp; 5 &amp; 6 &amp; 7 &amp; 8 &amp; 9 &amp; 10 \\
\text { Signo: } &amp; + &amp; + &amp; - &amp; + &amp; + &amp; + &amp; + &amp; + &amp; - &amp; +
\end{array}
\]</span></p>
<p>De modo que <span class="math inline">\(B=8\)</span> sobre <span class="math inline">\(n=10\)</span>. En este contraste, la región crítica es unilateral y concretamente es <span class="math inline">\(W=\{8,9,10\}\)</span>, ya que</p>
<p><span class="math display">\[
P(B \geq 8)=\sum_{i=8}^{10}\binom{10}{i} 0.5^{10}=0.0547&lt;\alpha=0.06
\]</span></p>
<p>Dado que la frecuencia observada es 8 y pertenece a la región crítica, rechazamos la igualdad y podemos aceptar que la máquina 1 produce más piezas defectuosas.</p>
<p>Para hacerlo con R introducimos los datos en dos vectores de la misma longitud. Calculamos la diferencia y el número de valores positivos. A partir de este número calculamos la probabilidad de la cola derecha de una binomial, ya que la hipótesis alternativa así lo requiere.</p>
<div class="sourceCode" id="cb160"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb160-1"><a href="estadística-no-paramétrica.html#cb160-1" tabindex="-1"></a>maq1<span class="ot">&lt;-</span><span class="fu">c</span>(<span class="dv">46</span>,<span class="dv">110</span>,<span class="dv">70</span>,<span class="dv">54</span>,<span class="dv">60</span>,<span class="dv">120</span>,<span class="dv">82</span>,<span class="dv">76</span>,<span class="dv">37</span>,<span class="dv">28</span>)</span>
<span id="cb160-2"><a href="estadística-no-paramétrica.html#cb160-2" tabindex="-1"></a>maq2<span class="ot">&lt;-</span><span class="fu">c</span>(<span class="dv">42</span>,<span class="dv">87</span>,<span class="dv">75</span>,<span class="dv">50</span>,<span class="dv">48</span>,<span class="dv">108</span>,<span class="dv">80</span>,<span class="dv">67</span>,<span class="dv">40</span>,<span class="dv">25</span>)</span>
<span id="cb160-3"><a href="estadística-no-paramétrica.html#cb160-3" tabindex="-1"></a>dif<span class="ot">&lt;-</span>maq1<span class="sc">-</span>maq2</span>
<span id="cb160-4"><a href="estadística-no-paramétrica.html#cb160-4" tabindex="-1"></a>dif</span></code></pre></div>
<pre><code>##  [1]  4 23 -5  4 12 12  2  9 -3  3</code></pre>
<div class="sourceCode" id="cb162"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb162-1"><a href="estadística-no-paramétrica.html#cb162-1" tabindex="-1"></a>B<span class="ot">&lt;-</span><span class="fu">length</span>(dif[dif<span class="sc">&gt;</span><span class="dv">0</span>]);B</span></code></pre></div>
<pre><code>## [1] 8</code></pre>
<div class="sourceCode" id="cb164"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb164-1"><a href="estadística-no-paramétrica.html#cb164-1" tabindex="-1"></a><span class="fu">dbinom</span>(<span class="dv">8</span>,<span class="dv">10</span>,<span class="fl">0.5</span>)<span class="sc">+</span><span class="fu">dbinom</span>(<span class="dv">9</span>,<span class="dv">10</span>,<span class="fl">0.5</span>)<span class="sc">+</span><span class="fu">dbinom</span>(<span class="dv">10</span>,<span class="dv">10</span>,<span class="fl">0.5</span>)</span></code></pre></div>
<pre><code>## [1] 0.0546875</code></pre>
<div class="sourceCode" id="cb166"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb166-1"><a href="estadística-no-paramétrica.html#cb166-1" tabindex="-1"></a><span class="dv">1</span><span class="sc">-</span><span class="fu">pbinom</span>(B<span class="dv">-1</span>,<span class="dv">10</span>,<span class="fl">0.5</span>)</span></code></pre></div>
<pre><code>## [1] 0.0546875</code></pre>
<p>El p-valor es inferior al nivel de significación 0.06 y, por tanto, rechazamos la hipótesis nula y aceptamos que la máquina 1 produce más piezas defectuosas.</p>
</div>
</div>
<div id="test-para-datos-binarios" class="section level3 hasAnchor" number="13.2.3">
<h3><span class="header-section-number">13.2.3</span> Test para datos binarios<a href="estadística-no-paramétrica.html#test-para-datos-binarios" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>En el caso de una muestra de valores de una variable dicotómica, por ejemplo</p>
<p><span class="math display">\[
a, a, b, b, b, a, a, b, a, \ldots, b
\]</span></p>
<p>podemos aplicar el test de los signos para contrastar el equilibrio de las probabilidades de ambos valores.</p>
<div id="ejemplo-3-1" class="section level4 hasAnchor" number="13.2.3.1">
<h4><span class="header-section-number">13.2.3.1</span> Ejemplo 3<a href="estadística-no-paramétrica.html#ejemplo-3-1" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Ante un cambio en un servicio público se realiza una encuesta a 300 personas, a las cuales se les pregunta si el servicio ha mejorado o empeorado, sin posibilidad de ser indiferente. Ha resultado que 197 personas han dicho que el servicio ha mejorado y queremos contrastar este hecho con un nivel de significación del 0.01.</p>
<p>Bajo la hipótesis nula de equilibrio, el número <span class="math inline">\(B\)</span> de personas que afirman que el servicio ha mejorado sigue una distribución binomial <span class="math inline">\(B(300,0.5)\)</span>, de forma que</p>
<p><span class="math display">\[
z=\frac{(197-0.5)-150}{\sqrt{300 \cdot 0.5 \cdot 0.5}}=5.37
\]</span></p>
<p>La región crítica de una cola es <span class="math inline">\(W=\{z&gt;2.33\}\)</span> para <span class="math inline">\(\alpha=0.01\)</span>, de manera que aceptamos la opinión de que el servicio ha mejorado.</p>
</div>
</div>
</div>
<div id="test-de-mcnemar" class="section level2 hasAnchor" number="13.3">
<h2><span class="header-section-number">13.3</span> Test de McNemar<a href="estadística-no-paramétrica.html#test-de-mcnemar" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Es una variante del test de los signos. Supongamos que un conjunto de individuos se clasifica en dos categorías opuestas, que podemos indicar con los signos <span class="math inline">\(+\mathrm{i}-\)</span>.
Después de algún estímulo, es posible que algunos individuos cambien de categoría, de forma que se obtiene la tabla de frecuencias</p>
<table>
<thead>
<tr class="header">
<th align="center"></th>
<th align="center"></th>
<th align="center">Después</th>
<th align="center"></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center"></td>
<td align="center"></td>
<td align="center">-</td>
<td align="center">+</td>
</tr>
<tr class="even">
<td align="center">Antes</td>
<td align="center">+</td>
<td align="center"><span class="math inline">\(a\)</span></td>
<td align="center"><span class="math inline">\(b\)</span></td>
</tr>
<tr class="odd">
<td align="center"></td>
<td align="center">-</td>
<td align="center"><span class="math inline">\(c\)</span></td>
<td align="center"><span class="math inline">\(d\)</span></td>
</tr>
</tbody>
</table>
<p>Solo <span class="math inline">\(a+d\)</span> individuos han cambiado. Bajo la hipótesis nula de que las proporciones no cambian, las probabilidades son</p>
<p><span class="math display">\[
P(+\rightarrow-)=P(-\rightarrow+)=1 / 2
\]</span></p>
<p>de manera que la frecuencia esperada en estos dos casos es <span class="math inline">\((a+d) / 2\)</span>. Podemos aplicar el test ji-cuadrado</p>
<p><span class="math display">\[
\chi^{2}=\frac{(a-(a+d) / 2)^{2}}{(a+d) / 2}+\frac{(d-(a+d) / 2)^{2}}{(a+d) / 2}=\frac{(a-d)^{2}}{a+d} \quad \text { con } 1 \text { g.l. }
\]</span></p>
<p>Rechazaremos la hipótesis de equilibrio si <span class="math inline">\(\chi&gt;\chi_{\alpha}^{2}\)</span>, donde <span class="math inline">\(\alpha\)</span> es el nivel de significación. Si las frecuencias son pequeñas es conveniente utilizar la corrección de Yates</p>
<p><span class="math display">\[
\chi=\frac{(|a-d|-1)^{2}}{a+d}
\]</span></p>
</div>
<div id="test-de-los-rangos-con-signo-de-wilcoxon" class="section level2 hasAnchor" number="13.4">
<h2><span class="header-section-number">13.4</span> Test de los rangos con signo de Wilcoxon<a href="estadística-no-paramétrica.html#test-de-los-rangos-con-signo-de-wilcoxon" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Visto como una extensión del test de los signos anterior, la idea de este test es utilizar, además del signo, la magnitud de las diferencias.</p>
<p>El <strong>rango</strong> de una observación es <em>la posición que ocupa en la muestra ordenada</em>. Por ejemplo, si consideramos la muestra</p>
<p><span class="math display">\[
x_{1}=3 \quad x_{2}=0 \quad x_{3}=5 \quad x_{4}=1.9
\]</span>
la muestra ordenada es</p>
<p><span class="math display">\[
x_{(1)}=0 \quad x_{(2)}=1.9 \quad x_{(3)}=3 \quad x_{(4)}=5
\]</span></p>
<p>de modo que los rangos son:</p>
<p><span class="math display">\[
r(0)=1 \quad r(1.9)=2 \quad r(3)=3 \quad r(5)=4
\]</span></p>
<p>Una parte importante de la estadística no paramétrica ha surgido de la sustitución de los valores cuantitativos de las muestras por sus rangos y de la obtención de estadísticos de contraste análogos a los utilizados con datos cuantitativos.</p>
<p>Podemos encontrar así, por ejemplo, un test equivalente al test <span class="math inline">\(t\)</span> de Student pero basado en rangos, o un coeficiente de correlación, denominado de Spearman, con la misma fórmula que el de Pearson pero utilizando rangos.</p>
<p>Ahora nos centramos en la comparación de medianas, basada en rangos, y no en los coeficientes de correlación.</p>
<p>Supongamos que la hipótesis nula es la misma que en el test de la mediana, es decir:</p>
<p><span class="math display">\[
\begin{aligned}
&amp; H_{0}: M=m_{0} \\
&amp; H_{1}: M \neq m_{0}
\end{aligned}
\]</span></p>
<p>donde <span class="math inline">\(M\)</span> representa la mediana de una variable o, con frecuencia, de la diferencia entre dos variables apareadas.</p>
<p>Wilcoxon propuso considerar los estadísticos:<br />
<span class="math inline">\(T^{+}=\)</span> Suma de los rangos de las observaciones con signo +<br />
<span class="math inline">\(T^{-}=\)</span> Suma de los rangos de las observaciones con signo -</p>
<p><span class="math display">\[
T^{+}=\sum_{i=1}^{n} r\left(\left|x_{i}-m_{0}\right|\right) I_{x_{i}&gt;m_{0}}
\]</span></p>
<p>Si <span class="math inline">\(H_{0}\)</span> es cierta, entonces es de esperar que <span class="math inline">\(T^{+}=T^{-}\)</span>.
El estadístico <span class="math inline">\(T^{+}\)</span> se conoce con el nombre de estadístico de Wilcoxon y está tabulado, de forma que pueden encontrarse valores <span class="math inline">\(t_{\alpha / 2}\)</span> y <span class="math inline">\(t_{1-\alpha / 2}\)</span> tales que</p>
<p><span class="math display">\[
P\left[T^{+}&lt;t_{\alpha / 2} \mid H_{0}\right]+P\left[T^{+}&gt;t_{1-\alpha / 2} \mid H_{0}\right] \leq \alpha
\]</span></p>
<p>y definir la región crítica como</p>
<p><span class="math display">\[
W=\left\{T^{+}&lt;t_{\alpha / 2}\right\} \cup\left\{T^{+}&gt;t_{1-\alpha / 2}\right\}
\]</span></p>
<div id="observaciones-3" class="section level3 hasAnchor" number="13.4.1">
<h3><span class="header-section-number">13.4.1</span> Observaciones<a href="estadística-no-paramétrica.html#observaciones-3" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li>Para valores grandes de <span class="math inline">\(n\)</span> puede utilizarse el hecho de que, bajo <span class="math inline">\(H_{0}\)</span>, el estadístico <span class="math inline">\(T^{+}\)</span> es asintóticamente normal:</li>
</ul>
<p><span class="math display">\[
T^{+} \sim A N\left(\mu_{T^{+}}, \sigma_{T^{+}}\right), \quad \mu_{T^{+}}=\frac{n(n+1)}{4}, \quad \sigma_{T^{+}}^{2}=\frac{n(n+1)(2 n+1)}{24}
\]</span></p>
<p>y, por tanto, para muestras grandes podemos basarnos en el estadístico</p>
<p><span class="math display">\[
Z=\frac{T^{+}-n(n+1) / 4}{\sqrt{n(n+1)(2 n+1) / 24}} \sim N(0,1)
\]</span></p>
<ul>
<li>Una alternativa al estadístico de contraste anterior consiste en considerar el estadístico</li>
</ul>
<p><span class="math display">\[
T=\min \left(T^{+}, T^{-}\right) .
\]</span></p>
<p>Si <span class="math inline">\(H_{0}\)</span> es cierta, entonces <span class="math inline">\(T^{+}=T^{-}\)</span>. Si no lo es, se tendrá <span class="math inline">\(T^{+}&gt;T^{-}\)</span> o bien <span class="math inline">\(T^{+}&lt;T^{-}\)</span>, de modo que el mínimo será un valor “pequeño”. El test basado en este estadístico rechazará la hipótesis nula si <span class="math inline">\(T\)</span> es menor que <span class="math inline">\(T_{\alpha}\)</span>, donde este valor crítico se obtiene a partir de una tabla diferente de la tabla de valores críticos para <span class="math inline">\(T^{+}\)</span>.</p>
</div>
<div id="ejemplo-4-1" class="section level3 hasAnchor" number="13.4.2">
<h3><span class="header-section-number">13.4.2</span> Ejemplo 4<a href="estadística-no-paramétrica.html#ejemplo-4-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Dado que en el ejemplo 1 las notas son numéricas, podemos utilizar el test de los rangos con signo para contrastar <span class="math inline">\(H_{0}: M=66\)</span> frente a <span class="math inline">\(H_{1}: M \neq 66\)</span>, con un nivel de significación del 0.05.</p>
<p><strong>Solución</strong>:</p>
<p>Para calcular el estadístico <span class="math inline">\(T^{+}\)</span> debemos asignar los rangos correspondientes a los valores positivos de las diferencias entre las observaciones y el valor 66 propuesto en la hipótesis nula.</p>
<p>En este ejemplo utilizaremos la función <code>wilcox.test</code> con el vector <code>notes</code> del ejemplo:</p>
<div class="sourceCode" id="cb168"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb168-1"><a href="estadística-no-paramétrica.html#cb168-1" tabindex="-1"></a><span class="fu">wilcox.test</span>(notes,<span class="at">mu=</span><span class="dv">66</span>,<span class="at">alternative=</span><span class="st">&quot;two.sided&quot;</span>,<span class="at">exact=</span>F)</span></code></pre></div>
<pre><code>## 
##  Wilcoxon signed rank test with continuity correction
## 
## data:  notes
## V = 465, p-value = 0.1726
## alternative hypothesis: true location is not equal to 66</code></pre>
<p>O bien, mediante una aproximación normal directa:</p>
<div class="sourceCode" id="cb170"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb170-1"><a href="estadística-no-paramétrica.html#cb170-1" tabindex="-1"></a>z<span class="ot">&lt;-</span>(<span class="dv">465</span><span class="fl">-0.5</span><span class="dv">-38</span><span class="sc">*</span><span class="dv">39</span><span class="sc">/</span><span class="dv">4</span>)<span class="sc">/</span><span class="fu">sqrt</span>(<span class="dv">38</span><span class="sc">*</span><span class="dv">39</span><span class="sc">*</span><span class="dv">77</span><span class="sc">/</span><span class="dv">24</span>);z</span></code></pre></div>
<pre><code>## [1] 1.363214</code></pre>
<div class="sourceCode" id="cb172"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb172-1"><a href="estadística-no-paramétrica.html#cb172-1" tabindex="-1"></a><span class="fu">round</span>(<span class="dv">2</span><span class="sc">*</span>(<span class="dv">1</span><span class="sc">-</span><span class="fu">pnorm</span>(z)),<span class="dv">2</span>)</span></code></pre></div>
<pre><code>## [1] 0.17</code></pre>
<p>Esta función calcula el estadístico <span class="math inline">\(T^{+}=465\)</span> y su p-valor con corrección por continuidad.</p>
<p>En este problema existen, además de dos ceros, un número considerable de empates o <em>ligaduras</em> (del inglés “ties”), de modo que la función <code>wilcox.test</code> no puede calcular el p-valor exacto y por ello se le ha indicado <code>exact=F</code>.</p>
<p>El estadístico <span class="math inline">\(z\)</span> que hemos calculado de forma directa y su p-valor, sin tener en cuenta las ligaduras, son bastante similares a los que calcula el algoritmo. Si no se indica nada sobre este parámetro, aparecen dos mensajes de advertencia relativos a este hecho.</p>
<div class="sourceCode" id="cb174"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb174-1"><a href="estadística-no-paramétrica.html#cb174-1" tabindex="-1"></a><span class="fu">wilcox.test</span>(notes,<span class="at">mu=</span><span class="dv">66</span>,<span class="at">alternative=</span><span class="st">&quot;two.sided&quot;</span>)</span></code></pre></div>
<pre><code>## 
##  Wilcoxon signed rank test with continuity correction
## 
## data:  notes
## V = 465, p-value = 0.1726
## alternative hypothesis: true location is not equal to 66</code></pre>
</div>
<div id="ejemplo-5-1" class="section level3 hasAnchor" number="13.4.3">
<h3><span class="header-section-number">13.4.3</span> Ejemplo 5<a href="estadística-no-paramétrica.html#ejemplo-5-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Dado que en el ejemplo 2 las observaciones son numéricas y apareadas, podemos utilizar los valores de las diferencias con el test de los rangos con signo para contrastar si existen diferencias entre las dos máquinas.</p>
<p><strong>Solución</strong>:</p>
<p>En este ejemplo se utiliza también la función <code>wilcox.test</code> con los dos vectores de datos del ejemplo y la opción <code>paired=TRUE</code>.</p>
<div class="sourceCode" id="cb176"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb176-1"><a href="estadística-no-paramétrica.html#cb176-1" tabindex="-1"></a><span class="fu">wilcox.test</span>(maq1,maq2,<span class="at">mu=</span><span class="dv">0</span>,<span class="at">paired=</span>T,<span class="at">alternative=</span><span class="st">&quot;greater&quot;</span>,<span class="at">exact=</span>F)</span></code></pre></div>
<pre><code>## 
##  Wilcoxon signed rank test with continuity correction
## 
## data:  maq1 and maq2
## V = 46.5, p-value = 0.02942
## alternative hypothesis: true location shift is greater than 0</code></pre>
<p>Observamos que en este caso hemos utilizado la opción <code>paired=T</code> para indicar que los datos son apareados. También hemos identificado correctamente la alternativa con <code>alternative="greater"</code>. Además, como en el ejemplo anterior, las ligaduras no permiten calcular el p-valor exacto. El p-valor aproximado 0.029 indica el rechazo de la hipótesis nula.</p>
</div>
</div>
<div id="el-test-u-de-mann-whitney" class="section level2 hasAnchor" number="13.5">
<h2><span class="header-section-number">13.5</span> El test <span class="math inline">\(U\)</span> de Mann-Whitney<a href="estadística-no-paramétrica.html#el-test-u-de-mann-whitney" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Este test permite comparar dos poblaciones con muestras independientes:
<span class="math display">\[
\left(x_{1}, \ldots, x_{n_{1}}\right),\left(y_{1}, \ldots, y_{n_{2}}\right)
\]</span></p>
<p>procedentes de dos poblaciones <span class="math inline">\(X, Y\)</span> con funciones de distribución <span class="math inline">\(F_{X}, F_{Y}\)</span> respectivamente. Queremos contrastar la hipótesis <span class="math inline">\(H_{0}: F_{X}=F_{Y}\)</span> frente a alguna de las alternativas</p>
<p><span class="math display">\[
H_{1}: F_{X} \neq F_{Y} \quad H_{1}: F_{X}&lt;F_{Y} \quad H_{1}: F_{X}&gt;F_{Y}
\]</span></p>
<p>Si la hipótesis nula es cierta, entonces <span class="math inline">\(P(X&lt;Y)=\frac{1}{2}\)</span>. Además, dado que existen <span class="math inline">\(n_{1} \cdot n_{2}\)</span> pares posibles, el número de pares de observaciones <span class="math inline">\(\left(x_{i}, y_{j}\right)\)</span> que se espera que verifiquen <span class="math inline">\(x_{i}&lt;y_{j}\)</span> estará alrededor de <span class="math inline">\(\frac{n_{1} \cdot n_{2}}{2}\)</span>.
Un estadístico de contraste razonable para decidir si aceptamos o rechazamos la hipótesis nula es el número de pares que verifican <span class="math inline">\(x_{i}&lt;y_{j}\)</span>, que definimos como:</p>
<p><span class="math display">\[
U=\sum_{i=1}^{n_{1}} \sum_{j=1}^{n_{2}} I_{x_{i}&lt;y_{j}}
\]</span></p>
<p>Una desviación significativa de <span class="math inline">\(U\)</span> respecto al valor esperado <span class="math inline">\(\frac{n_{1} \cdot n_{2}}{2}\)</span> conducirá al rechazo de la hipótesis nula. Para decidir si <span class="math inline">\(U\)</span> es significativo se consulta la tabla de Mann-Whitney-Wilcoxon, que permite decidir el rechazo de <span class="math inline">\(H_{0}\)</span> en función del nivel de significación elegido y de los tamaños muestrales <span class="math inline">\(n_{1}\)</span> y <span class="math inline">\(n_{2}\)</span>.</p>
<div id="observaciones-4" class="section level3 hasAnchor" number="13.5.1">
<h3><span class="header-section-number">13.5.1</span> Observaciones<a href="estadística-no-paramétrica.html#observaciones-4" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li>Un procedimiento alternativo para calcular <span class="math inline">\(U\)</span>, y a menudo más cómodo, consiste en formar la muestra conjunta reuniendo las dos muestras individuales y asignar los rangos <span class="math inline">\(1,2, \ldots, n_{1}+n_{2}\)</span> a cada uno de los valores de la muestra ordenada. Puede calcularse <span class="math inline">\(U\)</span> a partir de la relación:</li>
</ul>
<p><span class="math display">\[
U=W-\frac{n_{2}\left(n_{2}+1\right)}{2}
\]</span></p>
<p>donde <span class="math inline">\(W\)</span> es la suma de los rangos de las observaciones <span class="math inline">\(y_{j}\)</span></p>
<p><span class="math display">\[
W=\sum_{j=1}^{n_{2}} r\left(y_{j}\right)
\]</span></p>
<p>Este estadístico <span class="math inline">\(W\)</span> para comparar dos poblaciones fue propuesto por Wilcoxon pero, por la relación anterior, es equivalente al estadístico <span class="math inline">\(U\)</span> de Mann-Whitney.</p>
<ul>
<li>Si no existen ligaduras o empates, la relación entre el estadístico de Wilcoxon <span class="math inline">\(W\)</span> (suma de rangos correspondientes a las observaciones <span class="math inline">\(Y\)</span>) y el estadístico <span class="math inline">\(U\)</span> de Mann-Whitney (número de veces que <span class="math inline">\(x_{i}&lt;y_{j}\)</span> en la muestra conjunta ordenada) es</li>
</ul>
<p><span class="math display">\[
W=\frac{n_{2}\left(n_{2}+1\right)}{2}+U
\]</span></p>
<p>Si <span class="math inline">\(W^{\prime}\)</span> es la suma de los rangos correspondientes a las observaciones <span class="math inline">\(X\)</span>, entonces</p>
<p><span class="math display">\[
W+W^{\prime}=\frac{\left(n_{1}+n_{2}\right)\left(n_{1}+n_{2}+1\right)}{2}
\]</span></p>
<p>De modo que, si <span class="math inline">\(U^{\prime}\)</span> es el número de veces que <span class="math inline">\(y_{j}&lt;x_{i}\)</span>, se obtiene</p>
<p><span class="math display">\[
U+U^{\prime}=n_{1} n_{2} \quad W^{\prime}=\frac{n_{1}\left(n_{1}+1\right)}{2}+U^{\prime}
\]</span></p>
<ul>
<li>Para muestras “grandes” puede utilizarse el hecho de que, bajo <span class="math inline">\(H_{0}\)</span>, el estadístico <span class="math inline">\(U\)</span> es asintóticamente normal:</li>
</ul>
<p><span class="math display">\[
U \sim A N\left(\mu_{U}, \sigma_{U}\right), \quad \mu_{U}=\frac{n_{1} n_{2}}{2}, \quad \sigma_{U}^{2}=\frac{n_{1} n_{2}\left(n_{1}+n_{2}+1\right)}{12}
\]</span></p>
<p>y, por tanto, para <span class="math inline">\(n_{1}&gt;10\)</span> o <span class="math inline">\(n_{2}&gt;10\)</span> podemos basarnos en el estadístico de contraste</p>
<p><span class="math display">\[
Z=\frac{U-n_{1} n_{2} / 2}{\sqrt{n_{1} n_{2}\left(n_{1}+n_{2}+1\right) / 12}} \sim N(0,1)
\]</span></p>
</div>
<div id="ejemplo-6-1" class="section level3 hasAnchor" number="13.5.2">
<h3><span class="header-section-number">13.5.2</span> Ejemplo 6<a href="estadística-no-paramétrica.html#ejemplo-6-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Para comparar la resistencia en <span class="math inline">\(\mathrm{kg} / \mathrm{cm}^{2}\)</span> de un material suministrado por dos proveedores se midieron dos muestras de varios elementos:</p>
<p>Proveedor A 202, 229, 215, 220, 223, 233, 208, 228, 209<br />
Proveedor B 221, 207, 185, 203, 187, 190, 195, 204, 212</p>
<p>Con un nivel de significación del 0.05, indique si existen diferencias entre los materiales suministrados por los dos proveedores.</p>
<p><strong>Solución</strong>:</p>
<p>Ahora utilizaremos la función <code>wilcox.test</code> con los dos vectores de datos, teniendo en cuenta que son independientes, que es la opción por defecto.</p>
<div class="sourceCode" id="cb178"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb178-1"><a href="estadística-no-paramétrica.html#cb178-1" tabindex="-1"></a>pro.A<span class="ot">&lt;-</span><span class="fu">c</span>(<span class="dv">202</span>,<span class="dv">229</span>,<span class="dv">215</span>,<span class="dv">220</span>,<span class="dv">223</span>,<span class="dv">233</span>,<span class="dv">208</span>,<span class="dv">228</span>,<span class="dv">209</span>)</span>
<span id="cb178-2"><a href="estadística-no-paramétrica.html#cb178-2" tabindex="-1"></a>pro.B<span class="ot">&lt;-</span><span class="fu">c</span>(<span class="dv">221</span>,<span class="dv">207</span>,<span class="dv">185</span>,<span class="dv">203</span>,<span class="dv">187</span>,<span class="dv">190</span>,<span class="dv">195</span>,<span class="dv">204</span>,<span class="dv">212</span>)</span>
<span id="cb178-3"><a href="estadística-no-paramétrica.html#cb178-3" tabindex="-1"></a><span class="fu">wilcox.test</span>(pro.A,pro.B,<span class="at">alternative=</span><span class="st">&quot;two.sided&quot;</span>)</span></code></pre></div>
<pre><code>## 
##  Wilcoxon rank sum exact test
## 
## data:  pro.A and pro.B
## W = 70, p-value = 0.007775
## alternative hypothesis: true location shift is not equal to 0</code></pre>
<p>No debemos dejarnos confundir por la notación.</p>
<p>El estadístico calculado es el que hemos denominado <span class="math inline">\(U^{\prime}=70&gt;n_{1} n_{2} / 2=40.5\)</span>. En cualquier caso, el p-valor es muy explícito e implica el rechazo de la hipótesis nula de equivalencia.</p>
</div>
</div>
<div id="comparación-de-medianas" class="section level2 hasAnchor" number="13.6">
<h2><span class="header-section-number">13.6</span> Comparación de medianas<a href="estadística-no-paramétrica.html#comparaci%C3%B3n-de-medianas" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Consideremos una situación en la que se desea comparar dos poblaciones continuas con distribuciones de igual forma y tratar de detectar desplazamientos entre ambas distribuciones.
Sean <span class="math inline">\(x_{1}, \ldots, x_{n_{1}} \mathrm{i} y_{1}, \ldots, y_{n_{2}}\)</span> dos muestras aleatorias correspondientes a cada población e independientes entre sí. Si se ordenan conjuntamente ambas muestras en orden creciente y se considera la mediana <span class="math inline">\(M\)</span> de la muestra combinada, podemos calcular el estadístico</p>
<p><span class="math display">\[
T=\sum_{i=1}^{n_{1}} I_{x_{i}&lt;M}
\]</span></p>
<p>que sirve para contrastar la hipótesis <span class="math inline">\(H_{0}: M_{X}=M_{Y}\)</span>.
Si ambas poblaciones tienen la misma distribución, es de esperar que <span class="math inline">\(T\)</span> sea próximo a <span class="math inline">\(n_{1} / 2\)</span>. En cambio, si <span class="math inline">\(T\)</span> resulta mucho mayor que <span class="math inline">\(n_{1} / 2\)</span>, es razonable suponer que la mediana <span class="math inline">\(M_{X}\)</span> de la primera población es inferior a la de la segunda <span class="math inline">\(M_{Y}\)</span>; mientras que si <span class="math inline">\(T\)</span> es mucho menor que <span class="math inline">\(n_{1} / 2\)</span>, ello parece indicar que <span class="math inline">\(M_{X}\)</span> es superior a <span class="math inline">\(M_{Y}\)</span>. Las regiones críticas son:</p>
<table>
<colgroup>
<col width="50%" />
<col width="50%" />
</colgroup>
<thead>
<tr class="header">
<th align="left">Alternativa</th>
<th align="left">Región crítica</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><span class="math inline">\(M_{X}&lt;M_{Y}\)</span></td>
<td align="left"><span class="math inline">\(\{T \geq k\}\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(M_{X}&gt;M_{Y}\)</span></td>
<td align="left"><span class="math inline">\(\left\{T \leq k^{\prime}\right\}\)</span></td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(M_{X} \neq M_{Y}\)</span></td>
<td align="left"><span class="math inline">\(\left\{T \leq k_{1}\right\} \cup\left\{T \geq k_{2}\right\}\)</span></td>
</tr>
</tbody>
</table>
<p>Si la distribución de ambas poblaciones es la misma, la distribución de <span class="math inline">\(T\)</span> puede obtenerse con facilidad. Dado que las <span class="math inline">\(n_{1}+n_{2}\)</span> observaciones son independientes
e idénticamente distribuidas, las <span class="math inline">\(\binom{n_{1}+n_{2}}{n_{1}}\)</span> formas de asignar <span class="math inline">\(n_{1}\)</span> a la primera muestra (y las restantes <span class="math inline">\(n_{2}\)</span> a la segunda) son equiprobables. Si <span class="math inline">\(p\)</span> es la parte entera de <span class="math inline">\(\left(n_{1}+n_{2}\right) / 2\)</span>, existen <span class="math inline">\(p\)</span> de las <span class="math inline">\(n_{1}+n_{2}\)</span> observaciones inferiores a <span class="math inline">\(M\)</span> y se tendrá <span class="math inline">\(T=t\)</span> en todas aquellas asignaciones en las que resulten <span class="math inline">\(t\)</span> observaciones de la primera muestra de entre las <span class="math inline">\(p\)</span> primeras y <span class="math inline">\(n_{1}-t\)</span> entre las <span class="math inline">\(n_{1}+n_{2}-p\)</span> últimas. Así</p>
<p><span class="math display">\[
P(T=t)=\frac{\binom{p}{t}\binom{n_{1}+n_{2}-p}{n_{1}-t}}{\binom{n_{1}+n_{2}}{n_{1}}}
\]</span></p>
<p>donde <span class="math inline">\(t\)</span> puede variar entre <span class="math inline">\(\max \left\{0, p-n_{2}\right\}\)</span> y <span class="math inline">\(\min \left\{n_{1}, p\right\}\)</span>. Se trata, por tanto, de una distribución hipergeométrica que puede aproximarse, si <span class="math inline">\(n_{1}\)</span> y <span class="math inline">\(n_{2}\)</span> son grandes, por una <span class="math inline">\(N\left(n_{1} / 2, \sqrt{n_{1} n_{2} / 4\left(n_{1}+n_{2}\right)}\right)\)</span>.</p>
<div id="ejemplo-7-1" class="section level3 hasAnchor" number="13.6.1">
<h3><span class="header-section-number">13.6.1</span> Ejemplo 7<a href="estadística-no-paramétrica.html#ejemplo-7-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Con los datos del ejemplo anterior, calcule el estadístico <span class="math inline">\(\chi^{2}\)</span> y compare las medianas de las dos muestras.</p>
<p><strong>Solución</strong>:</p>
<p>Como ya sabemos, la mediana común de las dos muestras es <span class="math inline">\(M=208.5\)</span>. Entonces la tabla para el test de homogeneidad es</p>
<table>
<thead>
<tr class="header">
<th align="left"></th>
<th align="center">Pro. A</th>
<th align="center">Pro. B</th>
<th align="center"></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">valores inferiores</td>
<td align="center">2</td>
<td align="center">7</td>
<td align="center">9</td>
</tr>
<tr class="even">
<td align="left">valores superiores</td>
<td align="center">7</td>
<td align="center">2</td>
<td align="center">9</td>
</tr>
<tr class="odd">
<td align="left">Total</td>
<td align="center">9</td>
<td align="center">9</td>
<td align="center"></td>
</tr>
</tbody>
</table>
<p>Así, debemos calcular el estadístico ji-cuadrado con la corrección de Yates</p>
<p><span class="math display">\[
\chi^{2}=\frac{(|2 \cdot 2-7 \cdot 7|-18 / 2)^{2}}{9 \cdot 9 \cdot 9 \cdot 9} 18=3.556
\]</span></p>
<p>Con un grado de libertad y para un nivel de significación del 0.05 , la región crítica comienza en <span class="math inline">\(\chi_{0.05}^{2}=3.841\)</span>, de modo que podemos aceptar la hipótesis nula.</p>
<p>Con tamaños muestrales grandes, el test ji-cuadrado es preferible si no se tiene constancia de que la forma de ambas distribuciones sea la misma, ya que el test <span class="math inline">\(T\)</span> anterior tiende a aceptar la homogeneidad si <span class="math inline">\(M_{X}=M_{Y}\)</span> aunque la forma de las distribuciones sea diferente.
Por la misma razón, es preferible el test de Kolmogorov-Smirnov que se explica en la sección siguiente.</p>
<p>Observemos el cálculo de la mediana conjunta con <code>median(c(pro.A,pro.B))</code>.</p>
<div class="sourceCode" id="cb180"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb180-1"><a href="estadística-no-paramétrica.html#cb180-1" tabindex="-1"></a>T<span class="ot">&lt;-</span><span class="fu">length</span>(pro.A[pro.A<span class="sc">&lt;</span><span class="fu">median</span>(<span class="fu">c</span>(pro.A,pro.B))])</span>
<span id="cb180-2"><a href="estadística-no-paramétrica.html#cb180-2" tabindex="-1"></a>T</span></code></pre></div>
<pre><code>## [1] 2</code></pre>
<div class="sourceCode" id="cb182"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb182-1"><a href="estadística-no-paramétrica.html#cb182-1" tabindex="-1"></a><span class="fu">phyper</span>(T,<span class="dv">9</span>,<span class="dv">9</span>,<span class="dv">9</span>)</span></code></pre></div>
<pre><code>## [1] 0.02834225</code></pre>
<div class="sourceCode" id="cb184"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb184-1"><a href="estadística-no-paramétrica.html#cb184-1" tabindex="-1"></a><span class="dv">1</span><span class="sc">-</span><span class="fu">phyper</span>(<span class="dv">6</span>,<span class="dv">9</span>,<span class="dv">9</span>,<span class="dv">9</span>)</span></code></pre></div>
<pre><code>## [1] 0.02834225</code></pre>
<p>La distribución hipergeométrica permite encontrar los límites de la región crítica.</p>
<p>Si queremos hacerlo mediante el test ji-cuadrado, en primer lugar debemos introducir las frecuencias de la tabla.</p>
<div class="sourceCode" id="cb186"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb186-1"><a href="estadística-no-paramétrica.html#cb186-1" tabindex="-1"></a>numero<span class="ot">&lt;-</span><span class="fu">cbind</span>(<span class="fu">expand.grid</span>(<span class="at">M=</span><span class="fu">c</span>(<span class="st">&quot;inferior a M&quot;</span>,<span class="st">&quot;superior a M&quot;</span>),</span>
<span id="cb186-2"><a href="estadística-no-paramétrica.html#cb186-2" tabindex="-1"></a><span class="at">grup=</span><span class="fu">c</span>(<span class="st">&quot;A&quot;</span>,<span class="st">&quot;B&quot;</span>)))</span>
<span id="cb186-3"><a href="estadística-no-paramétrica.html#cb186-3" tabindex="-1"></a>fr<span class="ot">&lt;-</span><span class="fu">c</span>(<span class="dv">2</span>,<span class="dv">7</span>,<span class="dv">7</span>,<span class="dv">2</span>)</span>
<span id="cb186-4"><a href="estadística-no-paramétrica.html#cb186-4" tabindex="-1"></a><span class="fu">attach</span>(numero)</span>
<span id="cb186-5"><a href="estadística-no-paramétrica.html#cb186-5" tabindex="-1"></a>taula<span class="ot">&lt;-</span><span class="fu">table</span>(M,grup)<span class="sc">*</span>fr</span>
<span id="cb186-6"><a href="estadística-no-paramétrica.html#cb186-6" tabindex="-1"></a>taula</span></code></pre></div>
<pre><code>##               grup
## M              A B
##   inferior a M 2 7
##   superior a M 7 2</code></pre>
<p>Y con esta tabla calculamos el test de homogeneidad.</p>
<div class="sourceCode" id="cb188"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb188-1"><a href="estadística-no-paramétrica.html#cb188-1" tabindex="-1"></a><span class="fu">chisq.test</span>(taula)</span></code></pre></div>
<pre><code>## 
##  Pearson&#39;s Chi-squared test with Yates&#39; continuity correction
## 
## data:  taula
## X-squared = 3.5556, df = 1, p-value = 0.05935</code></pre>
<p>El resultado es la aceptación de la igualdad de medianas. La discrepancia con el ejemplo anterior es posible debido al reducido número de observaciones.</p>
</div>
</div>
<div id="test-de-kolmogorov-smirnov-para-la-homogeneidad" class="section level2 hasAnchor" number="13.7">
<h2><span class="header-section-number">13.7</span> Test de Kolmogorov-Smirnov para la homogeneidad<a href="estadística-no-paramétrica.html#test-de-kolmogorov-smirnov-para-la-homogeneidad" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Cuando disponemos de dos muestras independientes <span class="math inline">\(x_{1}, x_{2}, \ldots, x_{n_{1}}\)</span> e <span class="math inline">\(y_{1}, y_{2}, \ldots, y_{n_{2}}\)</span> procedentes de dos poblaciones con distribuciones desconocidas <span class="math inline">\(F_{X}\)</span> y <span class="math inline">\(F_{Y}\)</span> respectivamente, y queremos contrastar su coincidencia, es decir, la hipótesis <span class="math inline">\(H_{0}: F_{X}=F_{Y}\)</span>, podemos comparar las distribuciones empíricas asociadas a cada muestra. Esto es posible si conocemos los valores exactos de las observaciones y, en este aspecto, esta comparación es preferible al test ji-cuadrado de homogeneidad, que utiliza frecuencias y necesita muchos datos de cada población.</p>
<p>Las distribuciones empíricas son:</p>
<p><span class="math display">\[
F_{n_{1}}(z)=\frac{1}{n_{1}} \sum_{i=1}^{n_{1}} I_{x_{i}&lt;z} \quad
G_{n_{2}}(z)=\frac{1}{n_{2}} \sum_{i=1}^{n_{2}} I_{y_{i}&lt;z}
\]</span></p>
<p>y el estadístico de Kolmogorov-Smirnov es</p>
<p><span class="math display">\[
\Delta_{n_{1}, n_{2}}=\sup _{z \in \mathbb{R}}\left|F_{n_{1}}(z)-G_{n_{2}}(z)\right|
\]</span></p>
<p>Si la hipótesis <span class="math inline">\(H_{0}\)</span> es cierta, las dos distribuciones empíricas deben estar muy próximas y la medida global de discrepancia <span class="math inline">\(\Delta_{n_{1}, n_{2}}\)</span> será pequeña. Por el contrario, cuando <span class="math inline">\(F_{X} \neq F_{Y}\)</span>, el valor de <span class="math inline">\(\Delta_{n_{1}, n_{2}}\)</span> será mayor, de modo que la región crítica que debemos considerar es de la forma</p>
<p><span class="math display">\[
\left\{\Delta_{n_{1}, n_{2}}&gt;a\right\}
\]</span></p>
<p>El test se basa en el Teorema de Smirnov, que afirma lo siguiente:</p>
<p><em>Si las distribuciones continuas de las dos poblaciones coinciden <span class="math inline">\(F_{X}=F_{Y}\)</span> y <span class="math inline">\(n_{1} \rightarrow \infty, n_{2} \rightarrow \infty\)</span>, entonces para cada <span class="math inline">\(\lambda\)</span></em></p>
<p><span class="math display">\[
P\left(\sqrt{\frac{n_{1} n_{2}}{n_{1}+n_{2}}} \Delta_{n_{1}, n_{2}} \leq \lambda\right)
\rightarrow
Q(\lambda)=\sum_{i=-\infty}^{\infty}(-1)^{i} e^{-2 i^{2} \lambda^{2}}
\]</span></p>
<p><em>donde <span class="math inline">\(Q(\lambda)\)</span> es la distribución asintótica de Kolmogorov-Smirnov.</em></p>
<div id="ejemplo-8-1" class="section level3 hasAnchor" number="13.7.1">
<h3><span class="header-section-number">13.7.1</span> Ejemplo 8<a href="estadística-no-paramétrica.html#ejemplo-8-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Con los datos del ejemplo 6, calcule el estadístico de Kolmogorov-Smirnov y compare las distribuciones de las dos muestras.</p>
<p><strong>Solución</strong>:</p>
<p>Los datos han sido introducidos en los vectores <code>pro.A</code> y <code>pro.B</code>, y el test se calcula mediante la función <code>ks.test</code>.</p>
<div class="sourceCode" id="cb190"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb190-1"><a href="estadística-no-paramétrica.html#cb190-1" tabindex="-1"></a><span class="fu">ks.test</span>(pro.A,pro.B,<span class="at">alternative=</span><span class="st">&quot;two.sided&quot;</span>)</span></code></pre></div>
<pre><code>## 
##  Exact two-sample Kolmogorov-Smirnov test
## 
## data:  pro.A and pro.B
## D = 0.66667, p-value = 0.03357
## alternative hypothesis: two-sided</code></pre>
<p>En este caso, el p-valor indica el rechazo de la hipótesis nula.</p>
</div>
</div>
<div id="test-h-de-kruskal-wallis" class="section level2 hasAnchor" number="13.8">
<h2><span class="header-section-number">13.8</span> Test <span class="math inline">\(H\)</span> de Kruskal-Wallis<a href="estadística-no-paramétrica.html#test-h-de-kruskal-wallis" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>El test <span class="math inline">\(U\)</span> es un test no paramétrico para decidir si dos muestras independientes proceden o no de la misma población. El test <span class="math inline">\(H\)</span> de Kruskal-Wallis es una generalización para <span class="math inline">\(k\)</span> muestras tomadas de <span class="math inline">\(k\)</span> poblaciones. Así pues, <em>es una versión no paramétrica de un ANOVA de un factor</em>.</p>
<p>Consideremos <span class="math inline">\(k\)</span> muestras de tamaños <span class="math inline">\(n_{1}, n_{2}, \ldots, n_{k}\)</span> recogidas en las <span class="math inline">\(k\)</span> poblaciones, tales que <span class="math inline">\(n_{1}+n_{2}+\cdots+n_{k}=n\)</span>. Supongamos que ordenamos todas las observaciones de forma conjunta y calculamos las sumas de rangos para las <span class="math inline">\(k\)</span> muestras <span class="math inline">\(R_{1}, R_{2}, \ldots, R_{k}\)</span>, respectivamente. Si definimos el estadístico</p>
<p><span class="math display">\[
H=\left(\frac{12}{n(n+1)} \sum_{i=1}^{k} \frac{R_{i}^{2}}{n_{i}}\right)-3(n+1)
\]</span></p>
<p>se demuestra que, si existe homogeneidad entre las distribuciones de los <span class="math inline">\(k\)</span> grupos, su distribución muestral está muy próxima a una ji-cuadrado con <span class="math inline">\(k-1\)</span> grados de libertad cuando los tamaños muestrales <span class="math inline">\(n_{i}\)</span> son grandes.</p>
<p>Así, exigiremos siempre que <span class="math inline">\(n_{1}, n_{2}, \ldots, n_{k}\)</span> sean todos ellos superiores a 5. Para valores pequeños es necesario consultar tablas especiales.</p>
<div id="observaciones-5" class="section level3 hasAnchor" number="13.8.1">
<h3><span class="header-section-number">13.8.1</span> Observaciones<a href="estadística-no-paramétrica.html#observaciones-5" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li>El estadístico de Kruskal-Wallis puede escribirse en la forma</li>
</ul>
<p><span class="math display">\[
H=\frac{12}{n(n+1)} \sum_{i=1}^{k} n_{i}\left(R_{\bullet i}-R_{\bullet \bullet}\right)^{2}
\]</span></p>
<p>donde <span class="math inline">\(R_{\bullet i}=R_{i} / n_{i}\)</span> y <span class="math inline">\(R_{\bullet \bullet}=(n+1) / 2\)</span>. De esta forma, el test basado en <span class="math inline">\(H\)</span> se asemeja mucho al test <span class="math inline">\(F\)</span> en un diseño de un factor con réplicas.</p>
<ul>
<li>Si existen observaciones repetidas, el estadístico <span class="math inline">\(H\)</span> se corrige mediante un factor, de forma que el nuevo estadístico es</li>
</ul>
<p><span class="math display">\[
H^{\prime}=\frac{H}{1-\frac{\sum_{j=1}^{r}\left(t_{j}^{3}-t_{j}\right)}{n^{3}-n}}
\]</span></p>
<p>donde <span class="math inline">\(t_{j}\)</span> es el número de observaciones repetidas para un rango dado en la muestra combinada y <span class="math inline">\(r\)</span> es el número de repeticiones. Esta corrección tiene poco efecto sobre el valor de <span class="math inline">\(H\)</span>, incluso en presencia de muchas observaciones repetidas.</p>
</div>
<div id="ejemplo-9-1" class="section level3 hasAnchor" number="13.8.2">
<h3><span class="header-section-number">13.8.2</span> Ejemplo 9<a href="estadística-no-paramétrica.html#ejemplo-9-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Se desea comparar el peso en gramos de un producto envasado por tres fabricantes, con muestras de tamaño 6 en los tres casos.</p>
<table>
<thead>
<tr class="header">
<th align="left">Fabr. <span class="math inline">\(A\)</span></th>
<th align="left">251</th>
<th align="left">250</th>
<th align="left">249</th>
<th align="left">255</th>
<th align="left">258</th>
<th align="left">258</th>
<th align="left"></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Fabr. <span class="math inline">\(B\)</span></td>
<td align="left">247</td>
<td align="left">246</td>
<td align="left">250</td>
<td align="left">241</td>
<td align="left">240</td>
<td align="left">242</td>
<td align="left"></td>
</tr>
<tr class="even">
<td align="left">Fabr. <span class="math inline">\(C\)</span></td>
<td align="left">228</td>
<td align="left">236</td>
<td align="left">240</td>
<td align="left">225</td>
<td align="left">236</td>
<td align="left">230</td>
<td align="left"></td>
</tr>
</tbody>
</table>
<p>Estudie si existen diferencias entre los tres fabricantes utilizando el test de Kruskal-Wallis.</p>
<p><strong>Solución</strong>:</p>
<p>Para realizar el test de Kruskal-Wallis utilizamos la función <code>kruskal.test</code>, que proporciona el estadístico <span class="math inline">\(H\)</span> o, si es necesario, como en este caso, el estadístico corregido <span class="math inline">\(H^{\prime}\)</span>.</p>
<div class="sourceCode" id="cb192"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb192-1"><a href="estadística-no-paramétrica.html#cb192-1" tabindex="-1"></a>pes<span class="ot">&lt;-</span><span class="fu">c</span>(<span class="dv">251</span>,<span class="dv">250</span>,<span class="dv">249</span>,<span class="dv">255</span>,<span class="dv">258</span>,<span class="dv">258</span>,<span class="dv">247</span>,<span class="dv">246</span>,<span class="dv">250</span>,<span class="dv">241</span>,<span class="dv">240</span>,<span class="dv">242</span>,</span>
<span id="cb192-2"><a href="estadística-no-paramétrica.html#cb192-2" tabindex="-1"></a><span class="dv">228</span>,<span class="dv">236</span>,<span class="dv">240</span>,<span class="dv">225</span>,<span class="dv">236</span>,<span class="dv">230</span>)</span>
<span id="cb192-3"><a href="estadística-no-paramétrica.html#cb192-3" tabindex="-1"></a>fabr<span class="ot">&lt;-</span><span class="fu">c</span>(<span class="fu">rep</span>(<span class="dv">1</span>,<span class="dv">6</span>),<span class="fu">rep</span>(<span class="dv">2</span>,<span class="dv">6</span>),<span class="fu">rep</span>(<span class="dv">3</span>,<span class="dv">6</span>))</span>
<span id="cb192-4"><a href="estadística-no-paramétrica.html#cb192-4" tabindex="-1"></a><span class="fu">kruskal.test</span>(pes,fabr)</span></code></pre></div>
<pre><code>## 
##  Kruskal-Wallis rank sum test
## 
## data:  pes and fabr
## Kruskal-Wallis chi-squared = 14.396, df = 2, p-value = 0.0007482</code></pre>
<p>El p-valor es suficientemente significativo como para rechazar la hipótesis nula.</p>
</div>
</div>
<div id="test-de-friedman" class="section level2 hasAnchor" number="13.9">
<h2><span class="header-section-number">13.9</span> Test de Friedman<a href="estadística-no-paramétrica.html#test-de-friedman" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Este test está pensado para comprobar si existen diferencias significativas entre <span class="math inline">\(k\)</span> tratamientos o condiciones experimentales aplicados a <span class="math inline">\(n\)</span> individuos.</p>
<table style="width:100%;">
<colgroup>
<col width="14%" />
<col width="14%" />
<col width="14%" />
<col width="14%" />
<col width="14%" />
<col width="14%" />
<col width="14%" />
</colgroup>
<thead>
<tr class="header">
<th align="center"></th>
<th align="center">Tratamiento</th>
<th align="center"></th>
<th align="center"></th>
<th align="center"></th>
<th align="center"></th>
<th align="center"></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">Individuo</td>
<td align="center">1</td>
<td align="center">2</td>
<td align="center"><span class="math inline">\(\ldots\)</span></td>
<td align="center"><span class="math inline">\(j\)</span></td>
<td align="center"><span class="math inline">\(\ldots\)</span></td>
<td align="center"><span class="math inline">\(k\)</span></td>
</tr>
<tr class="even">
<td align="center">1</td>
<td align="center"><span class="math inline">\(x_{11}\)</span></td>
<td align="center"><span class="math inline">\(x_{12}\)</span></td>
<td align="center"><span class="math inline">\(\ldots\)</span></td>
<td align="center"><span class="math inline">\(x_{1 j}\)</span></td>
<td align="center"><span class="math inline">\(\ldots\)</span></td>
<td align="center"><span class="math inline">\(x_{1 k}\)</span></td>
</tr>
<tr class="odd">
<td align="center">2</td>
<td align="center"><span class="math inline">\(x_{21}\)</span></td>
<td align="center"><span class="math inline">\(x_{22}\)</span></td>
<td align="center"><span class="math inline">\(\ldots\)</span></td>
<td align="center"><span class="math inline">\(x_{2 j}\)</span></td>
<td align="center"><span class="math inline">\(\ldots\)</span></td>
<td align="center"><span class="math inline">\(x_{2 k}\)</span></td>
</tr>
<tr class="even">
<td align="center"><span class="math inline">\(\vdots\)</span></td>
<td align="center"><span class="math inline">\(\vdots\)</span></td>
<td align="center"><span class="math inline">\(\vdots\)</span></td>
<td align="center"></td>
<td align="center"><span class="math inline">\(\vdots\)</span></td>
<td align="center"></td>
<td align="center"><span class="math inline">\(\vdots\)</span></td>
</tr>
<tr class="odd">
<td align="center"><span class="math inline">\(n\)</span></td>
<td align="center"><span class="math inline">\(x_{n 1}\)</span></td>
<td align="center"><span class="math inline">\(x_{n 2}\)</span></td>
<td align="center"><span class="math inline">\(\ldots\)</span></td>
<td align="center"><span class="math inline">\(x_{n j}\)</span></td>
<td align="center"><span class="math inline">\(\ldots\)</span></td>
<td align="center"><span class="math inline">\(x_{n k}\)</span></td>
</tr>
</tbody>
</table>
<p>Los individuos deben escogerse al azar y de forma independiente, de modo que las filas son independientes entre sí. Sin embargo, como los individuos son los mismos, las columnas son dependientes.</p>
<p>El test de Friedman sirve para comprobar si existen diferencias entre los <span class="math inline">\(k\)</span> tratamientos (efecto columna), en presencia de efectos individuales (efecto fila). <em>Es una versión no paramétrica del diseño de dos factores sin interacción</em>.</p>
<p>La hipótesis nula es la igualdad de respuesta o de efecto de los diferentes tratamientos, mientras que la alternativa es que existen, al menos, dos tratamientos con respuesta diferente.</p>
<p>Para calcular el estadístico no paramétrico, para cada fila por separado se asignan los rangos correspondientes a los valores observados. Una vez convertida la tabla original en rangos, se calculan las sumas de rangos <span class="math inline">\(R_{j}\)</span> para cada columna o tratamiento <span class="math inline">\(j=1, \ldots, k\)</span>. El estadístico es</p>
<p><span class="math display">\[
S=\frac{12}{n k(k+1)} \sum_{j=1}^{k} R_{j}^{2}-3 n(k+1)
\]</span></p>
<p>La distribución aproximada de <span class="math inline">\(S\)</span> para valores grandes de <span class="math inline">\(n\)</span> es una ji-cuadrado con <span class="math inline">\(k-1\)</span> grados de libertad. Para valores muy pequeños de <span class="math inline">\(n\)</span> (<span class="math inline">\(n&lt;10\)</span>) es necesario consultar tablas especiales. La región crítica es de la forma <span class="math inline">\(\{S \geq c\}\)</span>.</p>
<p>Cuando existen ligaduras en una fila, deben promediarse los rangos de los valores repetidos y calcular el estadístico de Friedman modificado mediante un factor de corrección</p>
<p><span class="math display">\[
S^{\prime}=\frac{12 \sum_{j=1}^{k} R_{j}^{2}-3 n^{2} k(k+1)^{2}}{n k(k+1)-\frac{1}{k-1} \sum_{i=1}^{n}\left\{\sum_{j=1}^{g_{i}} t_{i j}^{3}-k\right\}}
\]</span></p>
<p>donde <span class="math inline">\(g_{i}\)</span> es el número de grupos de observaciones ligadas en la fila <span class="math inline">\(i\)</span> y <span class="math inline">\(t_{i j}\)</span> es el número de observaciones ligadas en el grupo <span class="math inline">\(j\)</span> de la fila <span class="math inline">\(i\)</span>. Cuando no hay ligaduras se considera, por convenio, que <span class="math inline">\(g_{i}=k\)</span> y <span class="math inline">\(t_{i j}=1\)</span>, y entonces el término de corrección para el individuo <span class="math inline">\(i\)</span> es <span class="math inline">\(k-k=0\)</span>. Si esto ocurre en todas las filas, entonces <span class="math inline">\(S^{\prime}=S\)</span>.</p>
<div id="ejemplo-10" class="section level3 hasAnchor" number="13.9.1">
<h3><span class="header-section-number">13.9.1</span> Ejemplo 10<a href="estadística-no-paramétrica.html#ejemplo-10" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Se ha consultado a un grupo de 12 personas para que opinen sobre cinco marcas de champú. En concreto, sus clasificaciones se recogen en la siguiente tabla.</p>
<table>
<thead>
<tr class="header">
<th align="center"></th>
<th align="center">Champú</th>
<th align="center"></th>
<th align="center"></th>
<th align="center"></th>
<th align="center"></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">Ind.</td>
<td align="center"><span class="math inline">\(A\)</span></td>
<td align="center"><span class="math inline">\(B\)</span></td>
<td align="center"><span class="math inline">\(C\)</span></td>
<td align="center"><span class="math inline">\(D\)</span></td>
<td align="center"><span class="math inline">\(E\)</span></td>
</tr>
<tr class="even">
<td align="center">1</td>
<td align="center">5</td>
<td align="center">3</td>
<td align="center">2</td>
<td align="center">4</td>
<td align="center">1</td>
</tr>
<tr class="odd">
<td align="center">2</td>
<td align="center">4</td>
<td align="center">3</td>
<td align="center">5</td>
<td align="center">2</td>
<td align="center">1</td>
</tr>
<tr class="even">
<td align="center">3</td>
<td align="center">3</td>
<td align="center">5</td>
<td align="center">4</td>
<td align="center">2</td>
<td align="center">1</td>
</tr>
<tr class="odd">
<td align="center">4</td>
<td align="center">4</td>
<td align="center">5</td>
<td align="center">1</td>
<td align="center">2</td>
<td align="center">3</td>
</tr>
<tr class="even">
<td align="center">5</td>
<td align="center">3</td>
<td align="center">4</td>
<td align="center">5</td>
<td align="center">1</td>
<td align="center">2</td>
</tr>
<tr class="odd">
<td align="center">6</td>
<td align="center">5</td>
<td align="center">3</td>
<td align="center">4</td>
<td align="center">2</td>
<td align="center">1</td>
</tr>
<tr class="even">
<td align="center">7</td>
<td align="center">2</td>
<td align="center">5</td>
<td align="center">4</td>
<td align="center">3</td>
<td align="center">1</td>
</tr>
<tr class="odd">
<td align="center">8</td>
<td align="center">3</td>
<td align="center">5</td>
<td align="center">4</td>
<td align="center">1</td>
<td align="center">2</td>
</tr>
<tr class="even">
<td align="center">9</td>
<td align="center">3</td>
<td align="center">4</td>
<td align="center">5</td>
<td align="center">2</td>
<td align="center">1</td>
</tr>
<tr class="odd">
<td align="center">10</td>
<td align="center">4</td>
<td align="center">5</td>
<td align="center">3</td>
<td align="center">1</td>
<td align="center">2</td>
</tr>
<tr class="even">
<td align="center">11</td>
<td align="center">5</td>
<td align="center">3</td>
<td align="center">2</td>
<td align="center">4</td>
<td align="center">1</td>
</tr>
<tr class="odd">
<td align="center">12</td>
<td align="center">5</td>
<td align="center">4</td>
<td align="center">3</td>
<td align="center">2</td>
<td align="center">1</td>
</tr>
</tbody>
</table>
<p>Determine si existen diferencias significativas entre las puntuaciones otorgadas a los champús.</p>
<p><strong>Solución</strong>:</p>
<p>El test de Friedman se aplica mediante la función <code>friedman.test</code>.</p>
<div class="sourceCode" id="cb194"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb194-1"><a href="estadística-no-paramétrica.html#cb194-1" tabindex="-1"></a>nota<span class="ot">&lt;-</span><span class="fu">c</span>(<span class="dv">5</span>,<span class="dv">3</span>,<span class="dv">2</span>,<span class="dv">4</span>,<span class="dv">1</span>,<span class="dv">4</span>,<span class="dv">3</span>,<span class="dv">5</span>,<span class="dv">2</span>,<span class="dv">1</span>,<span class="dv">3</span>,<span class="dv">5</span>,<span class="dv">4</span>,<span class="dv">2</span>,<span class="dv">1</span>,</span>
<span id="cb194-2"><a href="estadística-no-paramétrica.html#cb194-2" tabindex="-1"></a><span class="dv">4</span>,<span class="dv">5</span>,<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>,<span class="dv">3</span>,<span class="dv">4</span>,<span class="dv">5</span>,<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">5</span>,<span class="dv">3</span>,<span class="dv">4</span>,<span class="dv">2</span>,<span class="dv">1</span>,</span>
<span id="cb194-3"><a href="estadística-no-paramétrica.html#cb194-3" tabindex="-1"></a><span class="dv">2</span>,<span class="dv">5</span>,<span class="dv">4</span>,<span class="dv">3</span>,<span class="dv">1</span>,<span class="dv">3</span>,<span class="dv">5</span>,<span class="dv">4</span>,<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>,<span class="dv">4</span>,<span class="dv">5</span>,<span class="dv">2</span>,<span class="dv">1</span>,</span>
<span id="cb194-4"><a href="estadística-no-paramétrica.html#cb194-4" tabindex="-1"></a><span class="dv">4</span>,<span class="dv">5</span>,<span class="dv">3</span>,<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">5</span>,<span class="dv">3</span>,<span class="dv">2</span>,<span class="dv">4</span>,<span class="dv">1</span>,<span class="dv">5</span>,<span class="dv">4</span>,<span class="dv">3</span>,<span class="dv">2</span>,<span class="dv">1</span>)</span>
<span id="cb194-5"><a href="estadística-no-paramétrica.html#cb194-5" tabindex="-1"></a>individu<span class="ot">&lt;-</span><span class="fu">c</span>(<span class="fu">rep</span>(<span class="dv">1</span>,<span class="dv">5</span>),<span class="fu">rep</span>(<span class="dv">2</span>,<span class="dv">5</span>),<span class="fu">rep</span>(<span class="dv">3</span>,<span class="dv">5</span>),</span>
<span id="cb194-6"><a href="estadística-no-paramétrica.html#cb194-6" tabindex="-1"></a><span class="fu">rep</span>(<span class="dv">4</span>,<span class="dv">5</span>),<span class="fu">rep</span>(<span class="dv">5</span>,<span class="dv">5</span>),<span class="fu">rep</span>(<span class="dv">6</span>,<span class="dv">5</span>),</span>
<span id="cb194-7"><a href="estadística-no-paramétrica.html#cb194-7" tabindex="-1"></a><span class="fu">rep</span>(<span class="dv">7</span>,<span class="dv">5</span>),<span class="fu">rep</span>(<span class="dv">8</span>,<span class="dv">5</span>),<span class="fu">rep</span>(<span class="dv">9</span>,<span class="dv">5</span>),</span>
<span id="cb194-8"><a href="estadística-no-paramétrica.html#cb194-8" tabindex="-1"></a><span class="fu">rep</span>(<span class="dv">10</span>,<span class="dv">5</span>),<span class="fu">rep</span>(<span class="dv">11</span>,<span class="dv">5</span>),<span class="fu">rep</span>(<span class="dv">12</span>,<span class="dv">5</span>))</span>
<span id="cb194-9"><a href="estadística-no-paramétrica.html#cb194-9" tabindex="-1"></a>xampu<span class="ot">&lt;-</span><span class="fu">c</span>(<span class="fu">rep</span>(<span class="fu">seq</span>(<span class="dv">1</span>,<span class="dv">5</span>,<span class="dv">1</span>),<span class="dv">12</span>))</span>
<span id="cb194-10"><a href="estadística-no-paramétrica.html#cb194-10" tabindex="-1"></a><span class="fu">friedman.test</span>(nota,xampu,individu)</span></code></pre></div>
<pre><code>## 
##  Friedman rank sum test
## 
## data:  nota, xampu and individu
## Friedman chi-squared = 25.533, df = 4, p-value = 3.929e-05</code></pre>
<p>El p-valor indica claramente la significación de las diferencias.</p>
</div>
<div id="observaciones-6" class="section level3 hasAnchor" number="13.9.2">
<h3><span class="header-section-number">13.9.2</span> Observaciones<a href="estadística-no-paramétrica.html#observaciones-6" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li>Puede verse que</li>
</ul>
<p><span class="math display">\[
S=\frac{12 n}{k(k+1)} \sum_{j=1}^{k}\left(R_{\bullet j}-R_{\bullet \bullet}\right)^{2}
\]</span></p>
<p>donde <span class="math inline">\(R_{\bullet j}=R_{j} / n\)</span> y <span class="math inline">\(R_{\bullet \bullet}=(k+1) / 2\)</span>. Esto pone de manifiesto la relación de este test con el test <span class="math inline">\(F\)</span> para detectar el efecto columna en el diseño de dos factores sin interacción.</p>
</div>
</div>
<div id="coeficientes-de-correlación-no-paramétricos" class="section level2 hasAnchor" number="13.10">
<h2><span class="header-section-number">13.10</span> Coeficientes de correlación no paramétricos<a href="estadística-no-paramétrica.html#coeficientes-de-correlaci%C3%B3n-no-param%C3%A9tricos" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>En esta sección presentamos dos coeficientes no paramétricos que permiten medir la dependencia estocástica de dos muestras apareadas en poblaciones continuas.</p>
<p>También estamos interesados en los contrastes de independencia que pueden formularse a partir de estos coeficientes.</p>
<div id="coeficiente-tau-de-kendall" class="section level3 hasAnchor" number="13.10.1">
<h3><span class="header-section-number">13.10.1</span> Coeficiente <span class="math inline">\(\tau\)</span> de Kendall<a href="estadística-no-paramétrica.html#coeficiente-tau-de-kendall" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Consideremos una muestra aleatoria simple <span class="math inline">\((x_{1}, y_{1}), \ldots, (x_{n}, y_{n})\)</span> procedente de una distribución bidimensional. Sabemos que la frecuencia relativa de los pares tales que <span class="math inline">\((x_{i}-x_{j})(y_{i}-y_{j})&gt;0\)</span> es un estimador del parámetro</p>
<p><span class="math display">\[
\pi_{+}=P\left\{(X-X^{\prime})(Y-Y^{\prime})&gt;0\right\}
\]</span></p>
<p>donde <span class="math inline">\((X, Y)\)</span> y <span class="math inline">\((X^{\prime}, Y^{\prime})\)</span> son independientes y tienen la misma distribución conjunta poblacional.
La continuidad de las distribuciones implica que</p>
<p><span class="math display">\[
P\left\{(X-X^{\prime})(Y-Y^{\prime})=0\right\}=0
\]</span></p>
<p>de modo que</p>
<p><span class="math display">\[
\pi_{-}=P\left\{(X-X^{\prime})(Y-Y^{\prime})&lt;0\right\}=1-\pi_{+}
\]</span></p>
<p>Entonces,</p>
<p><span class="math display">\[
\tau=\pi_{+}-\pi_{-}=2\pi_{+}-1
\]</span></p>
<p>es el llamado <strong>coeficiente de asociación de Kendall</strong>, y mide, en cierto modo, la dependencia entre las variables. De hecho, si <span class="math inline">\(X\)</span> e <span class="math inline">\(Y\)</span> son independientes,</p>
<p><span class="math display">\[
\begin{aligned}
\pi_{+} &amp; =P(X&lt;X^{\prime})P(Y&lt;Y^{\prime})+P(X&gt;X^{\prime})P(Y&gt;Y^{\prime}) \\
&amp; =P(X&gt;X^{\prime})P(Y&lt;Y^{\prime})+P(X&lt;X^{\prime})P(Y&gt;Y^{\prime})=\pi_{-}
\end{aligned}
\]</span></p>
<p>de forma que <span class="math inline">\(\tau=0\)</span>. El recíproco no es cierto: puede ocurrir que <span class="math inline">\(\tau=0\)</span> sin que necesariamente las variables <span class="math inline">\(X\)</span> e <span class="math inline">\(Y\)</span> sean independientes.</p>
<p>Si <span class="math inline">\(P\)</span> y <span class="math inline">\(N\)</span> representan el número de pares tales que <span class="math inline">\((x_{i}-x_{j})(y_{i}-y_{j})&gt;0\)</span> y <span class="math inline">\((x_{i}-x_{j})(y_{i}-y_{j})&lt;0\)</span>, respectivamente, entre las <span class="math inline">\(\binom{n}{2}\)</span> posibles, el estimador natural de <span class="math inline">\(\tau\)</span> es</p>
<p><span class="math display">\[
T=\frac{P}{\binom{n}{2}}-\frac{N}{\binom{n}{2}}=\frac{2}{n(n-1)}(P-N)
\]</span></p>
<p>Además, dado que <span class="math inline">\(P+N=\binom{n}{2}\)</span>, se obtiene</p>
<p><span class="math display">\[
T=\frac{4P}{n(n-1)}-1
\]</span></p>
<p>Para una muestra concreta, <span class="math inline">\(P\)</span> se calcula fácilmente ordenando la muestra según la primera componente: es el número de pares con <span class="math inline">\(i&lt;j\)</span> tales que <span class="math inline">\(y_{i}&lt;y_{j}\)</span>.</p>
<p>El estadístico <span class="math inline">\(T\)</span> toma valores entre <span class="math inline">\(-1\)</span> y <span class="math inline">\(1\)</span>, y un valor alejado de cero indica que <span class="math inline">\(\tau \neq 0\)</span> y, por tanto, que las variables <span class="math inline">\(X\)</span> e <span class="math inline">\(Y\)</span> no son independientes.</p>
<p>La distribución exacta de <span class="math inline">\(T\)</span> puede calcularse para valores moderados de <span class="math inline">\(n\)</span>. Para <span class="math inline">\(n \leq 10\)</span> existen tablas de valores críticos tales que <span class="math inline">\(P(|T|&gt;k_{\alpha}) \leq \alpha\)</span>. Para <span class="math inline">\(n&gt;10\)</span> puede considerarse la aproximación</p>
<p><span class="math display">\[
T \sim N\left(0, \sigma_K^2\right),
\]</span>
dond <span class="math inline">\(\displaystyle{\sigma_K^2 =\frac{2(2n+5)}{9n(n-1)}}\)</span> es la varianza asintótica del estadístico <span class="math inline">\(T\)</span>.</p>
<div id="ejemplo-14" class="section level5 hasAnchor" number="13.10.1.0.1">
<h5><span class="header-section-number">13.10.1.0.1</span> Ejemplo 14<a href="estadística-no-paramétrica.html#ejemplo-14" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>La longitud y la anchura de una muestra de 11 hojas de una determinada planta son:</p>
<table>
<colgroup>
<col width="8%" />
<col width="8%" />
<col width="8%" />
<col width="8%" />
<col width="8%" />
<col width="8%" />
<col width="8%" />
<col width="8%" />
<col width="8%" />
<col width="8%" />
<col width="8%" />
<col width="8%" />
</colgroup>
<thead>
<tr class="header">
<th align="left">Hoja</th>
<th align="right">1</th>
<th align="right">2</th>
<th align="right">3</th>
<th align="right">4</th>
<th align="right">5</th>
<th align="right">6</th>
<th align="right">7</th>
<th align="right">8</th>
<th align="right">9</th>
<th align="right">10</th>
<th align="right">11</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Long.</td>
<td align="right">6.60</td>
<td align="right">7.11</td>
<td align="right">9.80</td>
<td align="right">6.62</td>
<td align="right">7.10</td>
<td align="right">6.83</td>
<td align="right">6.54</td>
<td align="right">7.14</td>
<td align="right">7.13</td>
<td align="right">12.52</td>
<td align="right">10.41</td>
</tr>
<tr class="even">
<td align="left">Anch.</td>
<td align="right">4.24</td>
<td align="right">5.41</td>
<td align="right">5.26</td>
<td align="right">5.53</td>
<td align="right">3.25</td>
<td align="right">4.22</td>
<td align="right">3.98</td>
<td align="right">3.29</td>
<td align="right">3.43</td>
<td align="right">5.57</td>
<td align="right">6.01</td>
</tr>
</tbody>
</table>
<p>Calcule el coeficiente de correlación de Kendall y estudie si es significativo.</p>
<p><strong>Solución</strong>:</p>
<p>Para el cálculo de los coeficientes de correlación se utiliza la función <code>cor.test</code> con el parámetro <code>method</code> y el valor <code>"pearson"</code>, <code>"kendall"</code> o <code>"spearman"</code>, según si deseamos el coeficiente clásico de Pearson, la <span class="math inline">\(\tau\)</span> de Kendall o el coeficiente por rangos de Spearman.</p>
<p>Observemos, no obstante, que estrictamente esta función no sirve para calcular los coeficientes, sino para contrastar la hipótesis de que el coeficiente correspondiente es cero. Sin embargo, como el primer paso que realiza la función es el cálculo del coeficiente, se obtiene este como un efecto colateral.</p>
<p>Otra cuestión <strong>muy importante</strong>, y que muchos usuarios suelen olvidar, es que si estos tests resultan significativos, únicamente indican que la correlación, del tipo que sea, no es cero; en ningún caso garantizan que sea suficientemente alta como para hablar de una “buena correlación”.</p>
<p>Volviendo a los datos del ejemplo:</p>
<div class="sourceCode" id="cb196"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb196-1"><a href="estadística-no-paramétrica.html#cb196-1" tabindex="-1"></a>long<span class="ot">&lt;-</span><span class="fu">c</span>(<span class="fl">6.60</span>,<span class="fl">7.11</span>,<span class="fl">9.80</span>,<span class="fl">6.62</span>,<span class="fl">7.10</span>,<span class="fl">6.83</span>,<span class="fl">6.54</span>,<span class="fl">7.14</span>,<span class="fl">7.13</span>,<span class="fl">12.52</span>,<span class="fl">10.41</span>)</span>
<span id="cb196-2"><a href="estadística-no-paramétrica.html#cb196-2" tabindex="-1"></a>ampl<span class="ot">&lt;-</span><span class="fu">c</span>(<span class="fl">4.24</span>,<span class="fl">5.41</span>,<span class="fl">5.26</span>,<span class="fl">5.53</span>,<span class="fl">3.25</span>,<span class="fl">4.22</span>,<span class="fl">3.98</span>,<span class="fl">3.29</span>,<span class="fl">3.43</span>,<span class="fl">5.57</span>,<span class="fl">6.01</span>)</span>
<span id="cb196-3"><a href="estadística-no-paramétrica.html#cb196-3" tabindex="-1"></a><span class="fu">cor.test</span>(long, ampl, <span class="at">method=</span><span class="st">&#39;kendall&#39;</span>)</span></code></pre></div>
<pre><code>## 
##  Kendall&#39;s rank correlation tau
## 
## data:  long and ampl
## T = 34, p-value = 0.3587
## alternative hypothesis: true tau is not equal to 0
## sample estimates:
##       tau 
## 0.2363636</code></pre>
<p>Observemos que el número de pares positivos es 34, estadístico que hemos denominado <span class="math inline">\(P\)</span>.
El procedimiento calcula el p-valor exacto para <span class="math inline">\(n&lt;50\)</span>. En este caso, el p-valor indica que el estadístico no es significativo.</p>
</div>
</div>
<div id="coeficiente-de-correlación-por-rangos-de-spearman" class="section level3 hasAnchor" number="13.10.2">
<h3><span class="header-section-number">13.10.2</span> Coeficiente de correlación por rangos de Spearman<a href="estadística-no-paramétrica.html#coeficiente-de-correlaci%C3%B3n-por-rangos-de-spearman" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>El coeficiente de correlación de Spearman es el coeficiente de correlación ordinario (coeficiente de Pearson) aplicado a los rangos de las observaciones.</p>
<p>El resultado es</p>
<p><span class="math display">\[
r_{S}=1-\frac{6 \sum_{i=1}^{n} d_{i}^{2}}{n(n^{2}-1)}
\]</span></p>
<p>donde <span class="math inline">\(d_{i}=r(x_{i})-r(y_{i})\)</span>, <span class="math inline">\(i=1, \ldots, n\)</span>, son las diferencias entre los rangos.</p>
<p>Este coeficiente se utiliza cuando las variables están medidas en una escala ordinal y el orden muestral es la información más relevante.</p>
<div id="cálculo-del-coeficiente" class="section level4 hasAnchor" number="13.10.2.1">
<h4><span class="header-section-number">13.10.2.1</span> Cálculo del coeficiente<a href="estadística-no-paramétrica.html#c%C3%A1lculo-del-coeficiente" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>El coeficiente de correlación <span class="math inline">\(r_{S}\)</span> de Spearman se obtiene sustituyendo los valores <span class="math inline">\((x_{i},y_{i})\)</span> por sus rangos <span class="math inline">\((a_{i},b_{i})\)</span>, donde <span class="math inline">\(a_{i}=r(x_{i})\)</span> y <span class="math inline">\(b_{i}=r(y_{i})\)</span>.</p>
<p>Así,</p>
<p><span class="math display">\[
r_{S}=\frac{\sum_{i=1}^{n}(a_{i}-\bar a)(b_{i}-\bar b)}{\sqrt{\sum_{i=1}^{n}(a_{i}-\bar a)^{2}\sum_{i=1}^{n}(b_{i}-\bar b)^{2}}}
\]</span></p>
<p>donde <span class="math inline">\(\bar a=\bar b=(n+1)/2\)</span> y</p>
<p><span class="math display">\[
\sum_{i=1}^{n}(a_{i}-\bar a)^{2}=\sum_{i=1}^{n}(b_{i}-\bar b)^{2}=\frac{n(n^{2}-1)}{12}
\]</span></p>
<p>de modo que</p>
<p><span class="math display">\[
r_{S}=\frac{12}{n(n^{2}-1)}\sum_{i=1}^{n}(a_{i}-\bar a)(b_{i}-\bar b)
\]</span></p>
<p>Por otro lado,</p>
<p><span class="math display">\[
\sum_{i=1}^{n} d_{i}^{2}=\frac{n(n^{2}-1)}{6}(1-r_{S})
\]</span></p>
<p>lo que conduce a la expresión inicial de <span class="math inline">\(r_{S}\)</span>.</p>
<p>Bajo la hipótesis de independencia, puede obtenerse la distribución muestral de <span class="math inline">\(r_{S}\)</span> y, por tanto, los puntos críticos del contraste para <span class="math inline">\(n\leq10\)</span>. Para <span class="math inline">\(n&gt;10\)</span>, <span class="math inline">\(r_{S}\)</span> es aproximadamente <span class="math inline">\(N(0,1/\sqrt{n-1})\)</span>.</p>
<div id="ejemplo-11" class="section level5 hasAnchor" number="13.10.2.1.1">
<h5><span class="header-section-number">13.10.2.1.1</span> Ejemplo<a href="estadística-no-paramétrica.html#ejemplo-11" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>Calcule el coeficiente de correlación por rangos de Spearman para los datos del ejemplo 14 y estudie si es significativo.</p>
<p><strong>Solución</strong>:</p>
<div class="sourceCode" id="cb198"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb198-1"><a href="estadística-no-paramétrica.html#cb198-1" tabindex="-1"></a><span class="fu">cor.test</span>(long,ampl,<span class="at">method=</span><span class="st">&#39;spearman&#39;</span>)</span></code></pre></div>
<pre><code>## 
##  Spearman&#39;s rank correlation rho
## 
## data:  long and ampl
## S = 140, p-value = 0.2732
## alternative hypothesis: true rho is not equal to 0
## sample estimates:
##       rho 
## 0.3636364</code></pre>
</div>
</div>
</div>
<div id="el-parámetro-poblacional-asociado-a-los-coeficientes-de-correlación-no-paramétricos" class="section level3 hasAnchor" number="13.10.3">
<h3><span class="header-section-number">13.10.3</span> El parámetro poblacional asociado a los coeficientes de correlación no paramétricos<a href="estadística-no-paramétrica.html#el-par%C3%A1metro-poblacional-asociado-a-los-coeficientes-de-correlaci%C3%B3n-no-param%C3%A9tricos" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Si la distribución conjunta de <span class="math inline">\((X,Y)\)</span> es <span class="math inline">\(F(x,y)\)</span> y las distribuciones marginales son <span class="math inline">\(F_X(x)\)</span> y <span class="math inline">\(F_Y(y)\)</span>, entonces <span class="math inline">\(\rho_S\)</span> es el coeficiente de correlación ordinario entre las variables <span class="math inline">\(V_1=F_X(X)\)</span> y <span class="math inline">\(V_2=F_Y(Y)\)</span>, ambas con distribución uniforme.</p>
<p>En consecuencia, puede demostrarse que</p>
<p><span class="math display">\[
\begin{aligned}
\rho_S &amp;=12\iint_{\mathbb R^2}(F(x,y)-F_X(x)F_Y(y))\,dF_X(x)dF_Y(y)\\
&amp;=12\iint_{\mathbb R^2}F(x,y)\,dF_X(x)dF_Y(y)-3
\end{aligned}
\]</span></p>
<p>La versión probabilística de la correlación <span class="math inline">\(\tau\)</span> de Kendall es</p>
<p><span class="math display">\[
\tau=4\int_{\mathbb R^2}(F(x,y)-F_X(x)F_Y(y))\,dF(x,y)
\]</span></p>
<p>y se verifica la relación</p>
<p><span class="math display">\[
-1\leq 3\tau-2\rho_S\leq1
\]</span></p>
<p>Observemos que <span class="math inline">\(\rho_S=\tau=0\)</span> si <span class="math inline">\(F(x,y)=F_X(x)F_Y(y)\)</span>, es decir, si <span class="math inline">\(X\)</span> e <span class="math inline">\(Y\)</span> son estocásticamente independientes.</p>
<p>Para contrastar la hipótesis nula <span class="math inline">\(H_0:\rho_S=0\)</span>, puede calcularse el estadístico</p>
<p><span class="math display">\[
t=\sqrt{n-2}\frac{r_S}{\sqrt{1-r_S^2}}
\]</span></p>
<p>que tiene aproximadamente una distribución <span class="math inline">\(t\)</span> de Student con <span class="math inline">\(n-2\)</span> grados de libertad, siempre que <span class="math inline">\(n\geq10\)</span>.</p>

<div style="page-break-after: always;"></div>
</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="las-pruebas-chi-cuadrado.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="bibliografia.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
  "sharing": {
    "github": false,
    "facebook": true,
    "twitter": true,
    "linkedin": false,
    "weibo": false,
    "instapaper": false,
    "vk": false,
    "whatsapp": false,
    "all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
  },
  "fontsettings": {
    "theme": "white",
    "family": "sans",
    "size": 2
  },
  "edit": {
    "link": null,
    "text": null
  },
  "history": {
    "link": null,
    "text": null
  },
  "view": {
    "link": null,
    "text": null
  },
  "download": ["FundamentosInferenciaEstadistica.pdf"],
  "search": {
    "engine": "fuse",
    "options": null
  },
  "toc": {
    "collapse": "section",
    "scroll_highlight": true
  }
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
